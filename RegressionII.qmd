# Regression II

Quantifying relationships between variables is a critical skill in business analytics. The regression line, representing the best linear fit between two variables, enables businesses to make predictions, identify trends, and optimize strategies using data-driven insights. Understanding how to calculate and interpret a regression line provides a foundation for analyzing key business relationships, such as the effect of advertising on sales or customer satisfaction on revenue. Below, we delve into generating and analyzing a regression line.

## The Regression Line

The regression line is calculated to minimize the average distance (or errors) between the line and the observed data points. It is defined by two key components: a **slope** ($\beta$) and an **intercept** ($\alpha$). Mathematically, the regression line is expressed as $\hat{y_i}=\hat{\alpha}+\hat{\beta}x_i$, where $\hat{y_i}$ are the predicted values of $y$ given the $x$'s.

The **slope** determines the steepness of the line. The estimate quantifies how much a unit increase in $x$ changes $y$. The estimate is given by $\hat{\beta}= \frac {s_{xy}}{s_{x}^2}$.

The **intercept** determines where the line crosses the $y$ axis. It returns the value of $y$ when $x$ is zero. The estimate is given by $\hat{\alpha}=\bar{y}-\hat{\beta}\bar{x}$.

**Example:** Let’s examine a data set on Price and Advertisement. Below is the data used to calculate the regression line, followed by its interpretation.

```{r echo=FALSE, out.width="50%", out.height="50%", fig.align='center'}
knitr::include_graphics("images/reg1.png")
```

The data shows a clear relationship between advertisement and price: when advertisement spending is high, price also tends to be high. This suggests a direct relationship between the two variables. In a previous chapter, we learned to quantify such relationships using covariance and the correlation coefficient.

In this section, we aim to answer two key questions:

1.  Prediction: What is the predicted price if we have a budget of 6 for advertisement?
2.  Effectiveness: How much can we increase the price for every additional dollar spent on advertisement?

The regression line allows us to answer these questions by quantifying the relationship between the two variables.

Let’s start by calculating the slope of the regression line, as this will help us answer the effectiveness question. The slope measures how much the price increases for every additional dollar spent on advertisement. It is calculated using the formula: $\hat{\beta}= \frac {s_{xy}}{s_{x}^2}$. Given that the covariance is 3.67 and the variance of x is 1.67, the slope of the regression line is $\hat{\beta}= \frac{3.67}{1.67}=2.2$. This tells us that for every additional dollar spent on advertisement, the price increases by 2.2. Thus, we have answered the effectiveness question.

Next, we calculate the intercept to complete the regression line and answer the prediction question. The intercept represents the predicted price when advertisement spending is zero. It is calculated as: $\hat{\alpha}=\bar{y}-\hat{\beta}\bar{x}$. Since the mean of advertisement is 2.5 and the mean of price is 7, the intercept is $\hat{\alpha}=7-2.2(2.5)=1.5$. This means that if we do not advertise, the predicted price is zero.

The regression line can be completed by using the intercept and slope that we have estimated above. In particular, the regression line is $\hat{y_i}=1.5+2.2x_i$. With this equation we can now establish that if we had a budget of 6 for advertisement, our predicted price would be $\hat{y_i}=1.5+2.2(6)=14.7$. This answers our prediction question.

In conclusion regression line has allowed us to answer both questions: 1. The effectiveness of advertisement: For every dollar spent, the price increases by 2.2. 2. The predicted price: With an advertisement budget of 6, the predicted price is 14.7.

This demonstrates the value of regression analysis in quantifying relationships and making informed predictions.

## Measures of Goodness of Fit

When analyzing the effectiveness of a regression model, it is crucial to assess how well the model fits the data. This is where measures of goodness of fit come into play. Below we explore some measures.

### Coefficient of Determination {.unnumbered}
The **coefficient of determination** or $R^2$ is the percent of the variation in $y$ that is explained by changes in $x$. The higher the $R^2$ the better the explanatory power of the model. The $R^2$ is always between \[0,1\]. To calculate use $R^2=SSR/SST$. Below you can find how to calculate each component of the $R^2$.

-   $SSR$ (Sum of Squares due to Regression) is the part of the variation in $y$ explained by the model. Mathematically, $SSR=\sum{(\hat{y_i}-\bar{y})^2}$.

-   $SSE$ (Sum of Squares due to Error) is the part of the variation in $y$ that is unexplained by the model. Mathematically, $SSE=\sum{(y_i-\hat{y_i})^2}$.

-   $SST$ (Sum of Squares Total) is the total variation of $y$ with respect to the mean. Mathematically, $SST=\sum{(y_i-\bar{y})^2}$.

-   Note that $SST=SSR+SSE$.


**Example:** Let's consider data on the Weight (y) and Exercise (x) of a particular person.

```{r echo=FALSE, out.width="50%", out.height="50%", fig.align='center'}
knitr::include_graphics("images/rs1.png")
```

Now, if we were to predict the weight of this person, we could think about just using the mean value (i.e., 166.4). If this were our guess, then the mistakes (errors) we would have made with this prediction is quantified by the $SST=25.2$. Below you can see these mistakes visually.

```{r echo=FALSE, fig.align='center'}
knitr::include_graphics("images/rs2.png")
```
The measure adds up all of the squared error made by using the mean as a prediction. Hence, $SST= (-1.4)^2 + (3.6)^2 + (1.6)^2 + (-2.4)^2 + (-1.4)^2= 25.2$.

The idea behind using regression, is that another variable that is related to weight can help us make a better prediction. So let's consider exercise. Generally speaking, if you exercise you burn calories and so your weight should be lower. The $SSE$ quantifies the mistakes made by a prediction that involves using the regression line. To understand this let's start by looking at the regression line.

```{r echo=FALSE, fig.align='center'}
knitr::include_graphics("images/rs3.png")
```

The image highlights how the regression line can reduce the errors of our prediction. For the two points highlighted in the graph, the distance to the regression line is shorter than the distance to the mean. Hence, the inclusion of exercise in our analysis has improved our prediction of the weight. Formally, $SSE=7.57$.

-   The **adjusted** $R^2$ recognizes that the $R^2$ is a non-decreasing function of the number of explanatory variables in the model. This metric penalizes a model with more explanatory variables relative to a simpler model. It is calculated by $1-(1-R^2) \frac {n-1}{n-k-1}$, where $k$ is the number of explanatory variables used in the model and $n$ is the sample size.

-   The **Residual Standard Error** estimates the average dispersion of the data points around the regression line. It is calculated by $s_e =\sqrt{\frac{SSE}{n-k-1}}$.

### Useful R Functions {.unnumbered}

The `lm()` function to estimates the linear regression model.

The `predict()` function uses the linear model object to predict values. New data is entered as a data frame.

The `coef()` function returns the model's coefficients.

The `summary()` function returns the model's coefficients, and goodness of fit measures.

## Exercises

The following exercises will help you get practice on Regression Line estimation and interpretation. In particular, the exercises work on:

-   Estimating the slope and intercept.

-   Calculating measures of goodness of fit.

-   Prediction using the regression line.

Answers are provided below. Try not to peak until you have a formulated your own answer and double checked your work for any mistakes.

### Exercise 1 {.unnumbered}

For the following exercises, make your calculations by hand and verify results using R functions when possible.

1.  Consider the data below. Calculate the deviations from the mean for each variable and use the results to estimate the regression line. Use R to verify your result. On average by how much does *y* increase per unit increase of *x*?

| **x** | 20  | 21  | 15  | 18  | 25  |
|:-----:|:---:|:---:|:---:|:---:|:---:|
| **y** | 17  | 19  | 12  | 13  | 22  |

<details>

<summary>Answer</summary>

*The regression lines is* $\hat{y}=-4.93+1.09x$. For each unit increase in x, y increases on average $1.09$.

*Start by generating the deviations from the mean for each variable. For x the deviations are:*

```{r}
x<-c(20,21,15,18,25)
(devx<-x-mean(x))
```

*Next, find the deviations for y:*

```{r}
y<-c(17,19,12,13,22)
(devy<-y-mean(y))
```

*For the slope we need to find the deviation squared of the x's. This can easily be done in R:*

```{r}
(devx2<-devx^2)
```

*The slope is calculated by* $\frac{\sum_{i=i}^{n} (x_{i}-\bar{x})(y_{i}-\bar{y})}{\sum_{i=i}^{n} (x_{i}-\bar{x})^2}$. In R we can just find the ratio between the summations of (devx)(devy) and devx2.

```{r}
(slope<-sum(devx*devy)/sum(devx2))
```

*The intercept is given by* $\bar{y}-\beta(\bar{x})$. In R we find that the intercept is equal to:

```{r}
(intercept<-mean(y)-slope*mean(x))
```

*Our results can be easily verified by using the `lm()` and `coef()` functions in R.*

```{r}
fitEx1<-lm(y~x)
coef(fitEx1)
```

</details>

2.  Calculate *SST*, *SSR*, and *SSE*. Confirm your results in R. What is the $R^2$? What is the Standard Error estimate? Is the regression line a good fit for the data?

<details>

<summary>Answer</summary>

*SST is* $69.2$, SSR is $64.82$ and SSE is $4.38$ (note that $SSR+SSE=SST$). The $R^2$ is just $\frac{SSR}{SST}=0.94$ and the Standard Error estimate is $1.21$. They both indicate a great fit of the regression line to the data.

*Let's start by calculating the SST. This is just* $\sum{(y_{i}-\bar{y})^2}$.

```{r}
(SST<-sum((y-mean(y))^2))
```

*Next, we can calculate SSR. This is calculated by the following formula* $\sum{(\hat{y_{i}}-\bar{y})^2}$. To obtain the predicted values in R, we can use the output of the `lm()` function. Recall our fitEx1 object created in Exercise 1. It has fitted.values included:

```{r}
(SSR<-sum((fitEx1$fitted.values-mean(y))^2))
```

*The ratio of SSR to SST is the* $R^2$:

```{r}
(R2<-SSR/SST)
```

*Finally, let's calculate SSE* $\sum{(y_{i}-\hat{y_{i}})^2}$:

```{r}
(SSE<-sum((y-fitEx1$fitted.values)^2))
```

*With the SSE we can calculate the Standard Error estimate:*

```{r}
sqrt(SSE/3)
```

*We can confirm these results using the `summary()` function.*

```{r}
summary(fitEx1)
```

</details>

3.  Assume that *x* is observed to be *32*, what is your prediction of *y*? How confident are you in this prediction?

<details>

<summary>Answer</summary>

*If* $x=32$ then $\hat{y}=29.87$. The regression is a good fit, so we can feel good about our prediction. However, we would be concerned about the sample size of the data.

*In R we can obtain a prediction by using the `predict()` function. This function requires a data frame as an input for new data.*

```{r}
predict(fitEx1, newdata = data.frame(x=c(32)))
```

</details>

### Exercise 2 {.unnumbered}

You will need the **Education** data set to answer this question. You can find the data set at https://jagelves.github.io/Data/Education.csv . The data shows the years of education (*Education*), and annual salary in thousands (*Salary*) for a sample of $100$ people.

1.  Estimate the regression line using R. By how much does an extra year of education increase the annual salary on average? What is the salary of someone without any education?

<details>

<summary>Answer</summary>

*An extra year of education increases the annual salary about* $5,300$ dollars (slope). A person that has no education would be expected to earn $17,2582$ dollars (intercept).

*Start by loading the data in R:*

```{r message=FALSE}
library(tidyverse)
Education<-read_csv("https://jagelves.github.io/Data/Education.csv")
```

*Next, let's use the `lm()` function to estimate the regression line and obtain the coefficients:*

```{r}
fitEducation<-lm(Salary~Education, data = Education)
coefficients(fitEducation)
```

</details>

2.  Confirm that the regression line is a good fit for the data. What is the estimated salary of a person with $16$ years of education?

<details>

<summary>Answer</summary>

*The* $R^2$ is $0.668$ and the standard error is $21$. The line is a moderately good fit. If someone has $16$ years of experience, the regression line would predict a salary of $102,000$ dollars.

*Let's get the* $R^2$ and the Standard Error estimate by using the `summary()` function and fitEx1 object.

```{r}
summary(fitEducation)
```

*Lastly, let's use the regression line to predict the salary for someone who has* $16$ years of education.

```{r}
predict(fitEducation, newdata = data.frame(Education=c(16)))
```

</details>

### Exercise 3 {.unnumbered}

You will need the **FoodSpend** data set to answer this question. You can find this data set at https://jagelves.github.io/Data/FoodSpend.csv .

1.  Omit any NA's that the data has. Create a dummy variable that is equal to $1$ if an individual owns a home and $0$ if the individual doesn't. Find the mean of your dummy variable. What proportion of the sample owns a home?

<details>

<summary>Answer</summary>

*Approximately,* $36$% of the sample owns a home.

*Start by loading the data into R and removing all NA's:*

```{r}
Spend<-read_csv("https://jagelves.github.io/Data/FoodSpend.csv")
Spend<-na.omit(Spend)
```

*To create a dummy variable for OwnHome we can use the `ifelse()` function:*

```{r}
Spend$dummyOH<-ifelse(Spend$OwnHome=="Yes",1,0)
```

*The average of the dummy variable is given by:*

```{r}
mean(Spend$dummyOH)
```

</details>

2.  Run a regression with *Food* being the dependent variable and your dummy variable as the independent variable. What is the interpretation of the intercept and slope?

<details>

<summary>Answer</summary>

*The intercept is the average food expenditure of individuals without homes (*$6417$). The slope, is the difference in food expenditures between individuals that do have homes minus those who don't. We then conclude that individuals that do have a home spend about $-2516$ less on food than those who don't have homes.

*To run the regression use the `lm()` function:*

```{r}
lm(Food~dummyOH,data=Spend)
```

</details>

3.  Now run a regression with *Food* being the independent variable and your dummy variable as the dependent variable. What is the interpretation of the intercept and slope? Hint: you might want to plot the scatter diagram and the regression line.

<details>

<summary>Answer</summary>

*The scatter plot shows that most of the points for home owners are below* $6000$. For non-home owners they are mainly above $6000$. The line can be used to predict the likelihood of owning a home given someones food expenditure. The intercept is above one, but still it gives us the indication that it is likely that low food expenditures are highly predictive of owning a home. The slope tells us how that likelihood changes as the food expenditures increase by 1. In general, the likelihood of owning a home decreases as the food expenditure increases.

*Run the `lm()` function once again:*

```{r}
fitFood<-lm(dummyOH~Food,data=Spend)
coefficients(fitFood)
```

*For the scatter plot use the following code:*

```{r}
library(ggthemes)
Spend %>% ggplot() + 
  geom_point(aes(y=dummyOH,x=Food), 
             col="black", pch=21, bg="grey") +
  geom_smooth(aes(y=dummyOH,x=Food), method="lm",
              formula=y~x, se=F) + 
  theme_clean()
```

</details>

### Exercise 4 {.unnumbered}

You will need the **Population** data set to answer this question. You can find this data set at https://jagelves.github.io/Data/Population.csv .

1.  Run a regression of *Population* on *Year*. How well does the regression line fit the data?

<details>

<summary>Answer</summary>

*If we follow the* $R^2=0.81$ the model fits the data very well.

*Let's load the data from the web:*

```{r}
Population<-read_csv("https://jagelves.github.io/Data/Population.csv")
```

*Now let's filter the data so that we can focus on the population for Japan.*

```{r message=FALSE}
Japan<-filter(Population,Country.Name=="Japan")
```

*Next, we can run the regression of Population against the Year. Let's also run the `summary()` function to obtain the fit and the coefficients.*

```{r}
fit<-lm(Population~Year,data=Japan)
summary(fit)
```

</details>

2.  Create a prediction for Japan's population in 2030. What is your prediction?

<details>

<summary>Answer</summary>

*The prediction for* $2030$ is about $140$ million people.

*Let's use the `predict()` function:*

```{r}
predict(fit,newdata=data.frame(Year=c(2030)))
```

</details>

3.  Create a scatter diagram and include the regression line. How confident are you of your prediction after looking at the diagram?

<details>

<summary>Answer</summary>

*After looking at the scatter plot, it seems unlikely that the population in Japan will hit* $140$ million. Population has been decreasing in Japan!

*Use the `plot()` and `abline()` functions to create the figure.*

```{r}
Japan %>% ggplot() +
  geom_point(aes(y=Population,x=Year), 
             col="black", pch=21, bg="grey") +
  geom_smooth(aes(y=Population,x=Year), 
              formula=y~x, method="lm", se=F) +
  theme_clean()
```

</details>
