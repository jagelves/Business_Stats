[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Business Statistics",
    "section": "",
    "text": "Introduction\n“Whatever you would make habitual, practice it; and if you would not make a thing habitual, do not practice it, but accustom yourself to something else.” – Epictetus\nThis course companion aims to build your mastery of statistics and its applications in R through consistent practice. Each chapter starts with key concepts to steer your learning, followed by problems designed to solidify these ideas through hands-on work. For extra R support, see Grolemund (2014). I thank my business statistics students and Haipei Liu for their valuable comments and reviews—all remaining errors are mine. Take your time, enjoy the journey, and make practice a habit!",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#why-r",
    "href": "index.html#why-r",
    "title": "Business Statistics",
    "section": "Why R?",
    "text": "Why R?\nWe will be using R to apply the lessons we learn in BUAD 231. R is a language and environment for statistical computing and graphics. There are several advantages to using the R software for statistical analysis and data science. Some of the main benefits include:\n\nR is a powerful and flexible programming language that allows users to manipulate and analyze data in many different ways.\nR has a large and active community of users, who have developed a wide range of packages and tools for data analysis and visualization.\nR is free and open-source, which makes it accessible to anyone who wants to use it.\nR is widely used in academia and industry, which means that there are many resources and tutorials available to help users learn how to use it.\nR is well-suited for working with large and complex datasets, and it can handle data from many different sources.\nR can be easily integrated with other tools and software, such as databases, visualization tools, and machine learning algorithms.\n\nOverall, R is a powerful and versatile tool for data analysis and data science, and it offers many benefits to users who want to work with data.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#installing-r.",
    "href": "index.html#installing-r.",
    "title": "Business Statistics",
    "section": "Installing R.",
    "text": "Installing R.\nTo install R, visit the R webpage at https://www.r-project.org/. Once in the website, click on the CRAN hyperlink.\n\n\n\n\n\n\n\n\n\nHere you can select the CRAN mirror. Scroll down until you see USA. You are free to choose any mirror you like, I recommend using the Duke University mirror.\n\n\n\n\n\n\n\n\n\nOnce you click on the hyperlink, you will be prompted to choose the download for your operating system. Depending on your operating system, choose either a Windows or Macintosh download.\n\n\n\n\n\n\n\n\n\nFollow all prompts and complete installation.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#installing-rstudio",
    "href": "index.html#installing-rstudio",
    "title": "Business Statistics",
    "section": "Installing RStudio",
    "text": "Installing RStudio\nVisit the Posit website at https://posit.co. Once on the website, hover to the top of the screen and select “Open Source” from the drop down menus.\n\n\n\n\n\n\n\n\n\nNext, choose “R Studio IDE”.\n\n\n\n\n\n\n\n\n\nScroll down until you see the products. You want to download “RStudio Desktop” and make sure it is the free version.\n\n\n\n\n\n\n\n\n\nFinally, select “Download RStudio” and follow the instructions for installation.\n\n\n\n\n\n\n\n\n\nIt is important to note that RStudio will not work if R is not installed. You can think of R as the engine and RStudio as the interface.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#posit-cloud",
    "href": "index.html#posit-cloud",
    "title": "Business Statistics",
    "section": "Posit Cloud",
    "text": "Posit Cloud\nIf you do not wish to install R, you can always use the cloud version. To do this, visit https://posit.cloud/. On the main page click on the “Get Started” button.\n\n\n\n\n\n\n\n\n\nChoose the “Cloud Free” option and log in using your Google credentials (if you have a Google account) or sign up if you want to create a new account.\n\n\n\n\nGrolemund, Garret. 2014. “Hands-on Programming with r.” https://jjallaire.github.io/hopr/.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "DescriptiveI.html",
    "href": "DescriptiveI.html",
    "title": "1  Descriptive Stats I",
    "section": "",
    "text": "1.1 Data and Types of Data\nUnderstanding the nature and classification of data is crucial for effective analysis and decision-making. Data are the building blocks of insights, providing a foundation for businesses, researchers, and policymakers to make informed choices. How we collect and categorize data significantly impacts how it is analyzed and interpreted. Below we highlight key types of data and their unique characteristics to help you get started with data analysis.\nLet’s start by defining data and the types of data structures that you might encounter when conducting analysis. Data are facts and figures collected, analyzed and summarized for presentation and interpretation. Data can be classified as:\nExample: Consider a retail store analyzing its sales performance. If the store collects data on the total revenue generated by each location on Black Friday, it is cross-sectional data. On the other hand, if the store tracks weekly sales for the past year to observe trends, it is time series data. Structured data, like sales figures stored in spreadsheets, allows for easy comparison and analysis. Meanwhile, customer feedback gathered from social media posts and video reviews represents unstructured data, requiring advanced tools to extract meaningful insights.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Descriptive Stats I</span>"
    ]
  },
  {
    "objectID": "DescriptiveI.html#data-and-types-of-data",
    "href": "DescriptiveI.html#data-and-types-of-data",
    "title": "1  Descriptive Stats I",
    "section": "",
    "text": "Cross Sectional Data refers to data collected at the same (or approximately the same) point in time. Ex: NFL standings in 1980 or Country GDP in 2015.\nTime Series Data refers to data collected over several time periods. Ex: U.S. inflation rate from 2000-2010 or Tesla deliveries from 2016-2022.\nStructured Data resides in a predefined row-column format (tidy). Ex: spreadsheet data.\nUnstructured Data do not conform to a pre-defined row-column format. Ex: Text, video, and other multimedia.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Descriptive Stats I</span>"
    ]
  },
  {
    "objectID": "DescriptiveI.html#data-sets",
    "href": "DescriptiveI.html#data-sets",
    "title": "1  Descriptive Stats I",
    "section": "1.2 Data Sets",
    "text": "1.2 Data Sets\nA data set contains all data collected for a particular study. Understanding the structure the data set aids in communicating findings to stakeholders clearly and effectively. Data sets are composed of:\n\nElements are the entities on which data are collected. Ex: Football teams, countries, and individuals.\nVariables are a set of characteristics collected for each element. Ex: Goals scored, GDP, weight.\nObservations are the set of measurements obtained for a particular element. Ex: Salah, 20 (goals), 15 (assists). US, 2.3 (inflation), 4.5% (federal interest rate).\n\n\n\n\nElements\nVariable 1\nVariable 2\n\n\n\n\nElement 1\n#\n#\n\n\nElement 2\n#\n#\n\n\nElement 3\n#\n#\n\n\n…\n…\n…\n\n\n\nExample: Consider the dataset on electric vehicles (EV’s) displayed below:\n\n\n\n\n\n\n\n\n\nIn this data set, each row represents an electric vehicle model, making the elements the specific EV models rather than the manufacturers. The variables collected for each model include:\n\nMake: The manufacturer of the EV.\nModel: The specific name of the EV model.\nRange_km: Driving range in kilometers on a full charge.\nTopSpeed_kmh: Maximum speed in km/h.\nPrice_pounds: Price in pounds (£).\nCharge_kmh: Charging speed in kilometers per hour.\n\nAn example observation is “Tesla Model 3,” with the following data: Make: Tesla, Model: Model 3, Range_km: 415, TopSpeed_kmh: 201, Price_pounds: 39,990, Charge_kmh: 690.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Descriptive Stats I</span>"
    ]
  },
  {
    "objectID": "DescriptiveI.html#scales-of-measurement",
    "href": "DescriptiveI.html#scales-of-measurement",
    "title": "1  Descriptive Stats I",
    "section": "1.3 Scales of Measurement",
    "text": "1.3 Scales of Measurement\nUnderstanding scales of measurement is crucial for analyzing and interpreting data effectively in business. By distinguishing between categorical (e.g., marital status, satisfaction ratings) and numerical data (e.g., profits, prices), you’ll know what methods to use for analysis. Knowing whether data is nominal, ordinal, interval, or ratio ensures your analysis and conclusions are accurate and relevant.\nThe scales of measurements determine the amount and type of information contained in each variable. In general, variables can be classified as categorical or numerical.\n\nCategorical (qualitative) data includes labels or names to identify an attribute of each element. Categorical data can be nominal or ordinal.\n\nWith nominal data, the order of the categories is arbitrary. Ex: Marital Status, Race/Ethnicity, or NFL division.\nWith ordinal data, the order or rank of the categories is meaningful. Ex: Rating, Difficulty Level, or Spice Level.\n\nNumerical (quantitative) include numerical values that indicate how many (discrete) or how much (continuous). The data can be either interval or ratio.\n\nWith interval data, the distance between values is expressed in terms of a fixed unit of measure. The zero value is arbitrary and does not represent the absence of the characteristic. Ratios are not meaningful. Ex: Temperature or Dates.\nWith ratio data, the ratio between values is meaningful. The zero value is not arbitrary and represents the absence of the characteristic. Ex: Prices, Profits, Wins.\n\n\nExample: Let’s keep using the EV example. Consider the new data set below:\n\n\n\n\n\n\n\n\n\nThe variables can be classified as follows:\n\nCar (Categorical - Nominal), consists of names of cars, which are labels used to identify each row. The order of these names does not matter, making it nominal data.\nBrand (Categorical - Nominal) represents the manufacturer of the car (e.g., Ford, Audi). These are labels with no inherent order, making it nominal data.\nRange (Numerical - Ratio), refers to the car’s driving range in miles. It is numerical and ratio because it has a meaningful zero (a car with zero range cannot move), and ratios are meaningful (e.g., a car with 250 miles range has double the range of one with 125 miles).\nRating (Categorical - Ordinal) represents a rank or score (e.g., 4, 3, 2). The order matters, as higher ratings indicate better performance. However, the intervals between ratings are not consistent, so it is ordinal data.\nYear (Numerical - Interval) represents a point in time. While numerical, it is interval data because the zero point is arbitrary (e.g., year 0 does not indicate the “absence” of time), and ratios are not meaningful (e.g., 2020 is not “twice as late” as 1010).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Descriptive Stats I</span>"
    ]
  },
  {
    "objectID": "DescriptiveI.html#useful-base-r-functions",
    "href": "DescriptiveI.html#useful-base-r-functions",
    "title": "1  Descriptive Stats I",
    "section": "1.4 Useful Base R Functions",
    "text": "1.4 Useful Base R Functions\nUnderstanding and using Base R functions is essential for efficiently managing and analyzing data. Functions like na.omit() help clean datasets by removing rows with missing values, ensuring your analyses are accurate and complete. nrow() and ncol() quickly provide insights into the size of your dataset, while is.na() allows you to identify and address missing data. The summary() function is a powerful way to generate descriptive statistics and assess the overall structure of your data at a glance. Additionally, coercion functions like as.integer(), as.factor(), and as.double() enable you to convert variables to appropriate data types, ensuring compatibility with different analysis methods.\n\nThe na.omit() function removes any observations that have a missing value (NA). The resulting data frame has only complete cases. Input: A data frame (tibble) or vector.\nThe nrow() and ncol() functions return the number of rows and columns respectively from a data frame. Input: A data frame (tibble).\nThe is.na() function returns a vector of True and False that specify if an entry is missing (NA) or not. Input: A data frame (tibble) or vector.\nThe summary() function returns a collection of descriptive statistics from a data frame (or vector). The function also returns whether there are any missing values (NA) in a variable. Input: A data frame (tibble) or vector.\nThe as.integer(), as.factor(), as.double(), are functions used to coerce your data into a different scale of measurement. Input: A vector or column of a data frame (tibble).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Descriptive Stats I</span>"
    ]
  },
  {
    "objectID": "DescriptiveI.html#useful-dplyr-functions",
    "href": "DescriptiveI.html#useful-dplyr-functions",
    "title": "1  Descriptive Stats I",
    "section": "1.5 Useful dplyr Functions",
    "text": "1.5 Useful dplyr Functions\nThe dplyr package has a collection of functions that are useful for data manipulation and transformation. If you are interested in this package you can refer to Wickham (2017). To install, run the following command in the console install.packages(\"dplyr\").\n\nThe arrange() function allows you to sort data frames in ascending order. Pair with the desc() function to sort the data in descending order.\nThe filter() function allows you to subset the rows of your data based on a condition.\nThe select() function allows you to select a subset of variables from your data frame.\nThe mutate() function allows you to create a new variable.\nThe group_by() function allows you to group your data frame by categories present in a given variable.\nThe summarise() function allows you to summarise your data, based on groupings generated by the goup_by() function.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Descriptive Stats I</span>"
    ]
  },
  {
    "objectID": "DescriptiveI.html#exercises",
    "href": "DescriptiveI.html#exercises",
    "title": "1  Descriptive Stats I",
    "section": "1.4 Exercises",
    "text": "1.4 Exercises\nThe following exercises will help you test your knowledge on the Scales of Measurement. They will also allow you to practice some basic data “wrangling” in R. In these exercises you will:\n\nIdentify numerical and categorical data.\nClassify data according to their scale of measurement.\nSort and filter data in R.\nHandle missing values (NA’s) in R.\n\nAnswers are provided below. Try not to peak until you have a formulated your own answer and double checked your work for any mistakes.\n\nExercise 1\nA bookstore has compiled data set on their current inventory. A portion of the data is shown below:\n\n\n\nTitle\nPrice\nYear Published\nRating\n\n\n\n\nFrankenstein\n5.49\n1818\n4.2\n\n\nDracula\n7.60\n1897\n4.0\n\n\n…\n…\n…\n…\n\n\nSleepy Hollow\n6.95\n1820\n3.8\n\n\n\n\nWhich of the above variables are categorical and which are numerical?\n\n\n\nAnswer\n\n\nThe “Title” variable represents the names of books. Therefore, this is a categorical variable.\n“Price” represents the cost of each book in a numeric format, making it a numerical variable.\n“Year Published” indicates the publication year of each book. It is numerical.\nIf “Rating” represents a numerical score based on a continuous scale (e.g., average user ratings on a platform like Goodreads), it is numerical because arithmetic operations like averaging or comparing differences are meaningful. If “Rating” represents predefined categories (e.g., “Excellent,” “Good,” “Fair,” “Poor”) or is interpreted as ranks without meaningful differences between values, it would be categorical.\n\n\n\nWhat is the measurement scale of each of the above variable?\n\n\n\nAnswer\n\n\nThe measurement scale is nominal for “Title” since these are labels used to identify each book and do not have a numerical meaning or order.\nIf “Rating” represents a score (e.g., 4.2, 4.0) given to each book, it is numerical and could be considered interval data because the scale represents a meaningful difference, but it may not have an absolute zero or meaningful ratios (e.g., a book rated 4.0 is not “twice as good” as one rated 2.0).\n“Price” is a measurable quantity with a meaningful zero (e.g., a book priced at $0 means it is free), making it ratio data.\n“Year” is interval data because the zero point is arbitrary (year 0 does not represent the absence of time) and differences between years are meaningful (e.g., 1897 - 1818 = 79 years).\n\n\n\n\nExercise 2\nA car company tracks the number of deliveries every quarter. A portion of the data is shown below:\n\n\n\nYear\nQuarter\nDeliveries\n\n\n\n\n2016\n1\n14800\n\n\n2016\n2\n14400\n\n\n…\n…\n…\n\n\n2022\n3\n343840\n\n\n\n\nWhat is the measurement scale of the Year variable? What are the strengths and weaknesses of this type of measurement scale?\n\n\n\nAnswer\n\nThe variable Year is measured on the interval scale because the observations can be ranked, categorized and measured when using this kind of scale. However, there is no true zero point so we cannot calculate meaningful ratios between years.\n\n\nWhat is the measurement scale for the Quarter variable? What is the weakness of this type of measurement scale?\n\n\n\nAnswer\n\nThe variable Quarter is measured on the ordinal scale, even though it contains numbers. It is the least sophisticated level of measurement because if we are presented with nominal data, all we can do is categorize or group the data.\n\n\nWhat is the measurement scale for the Deliveries variable? What are the strengths of this type of measurement scale?\n\n\n\nAnswer\n\nThe variable Deliveries is measured on the ratio scale. It is the strongest level of measurement because it allows us to categorize and rank the data as well as find meaningful differences between observations. Also, with a true zero point, we can interpret the ratios between observations.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Descriptive Stats I</span>"
    ]
  },
  {
    "objectID": "DescriptiveII.html",
    "href": "DescriptiveII.html",
    "title": "2  Descriptive Stats II",
    "section": "",
    "text": "2.1 Frequency Distributions (Categorical)\nUnderstanding and visualizing data distributions is a fundamental step in data analysis. It provides critical insights into the underlying characteristics of the data, which directly impact decision-making, model performance, and interpretation. Below, we introduce tabular and visual techniques to describe your data.\nA frequency distribution is perhaps the most valuable tool for summarizing categorical data. It illustrates with a table the number of items within distinct, non-overlapping categories. An alternative known as the relative frequency quantifies the proportion of items in each category relative to the total number of observations. You can calculate it by taking the frequency of a particular class (\\(f_{i}\\)), and dividing it by the number of observations \\(n\\). Relative frequency helps contextualize the data by highlighting the significance of each category compared to the whole.\nExample: Consider data on students’ answers to the question, what is your favorite food? You can see the data below:\nSimply observing raw data can make identifying the most and least popular items challenging. A frequency distribution organizes this information into a clear table, showcasing the popularity of each item. The frequency distribution of the table is displayed below:\nFood\nFrequency\nRelative\n\n\n\n\nChicken\n5\n0.20\n\n\nPasta\n4\n0.16\n\n\nPizza\n6\n0.24\n\n\nSushi\n10\n0.40\nEach food item is tallied up, and the result is shown in the frequency column. Alternately, we can show the tally as a proportion of the total (i.e., 25). For example, five students liked chicken; out of the 25 students surveyed, this represents 0.2 or 20%. This calculation is shown for each food item in the relative frequency column.\nBelow, you can see the bar graph showing the frequency distribution of the food items data. Note that the visualization is constructed by showing each food item as a bar with a height equal to the frequency.\nIn sum, the bar plot illustrates the frequency distribution of categorical data. It includes the classes in the horizontal axis and frequencies or relative frequencies in the vertical axis and has gaps between each bar.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Stats II</span>"
    ]
  },
  {
    "objectID": "DescriptiveII.html#freqdistnum",
    "href": "DescriptiveII.html#freqdistnum",
    "title": "2  Descriptive Stats II",
    "section": "2.2 Frequency Distributions (Numerical)",
    "text": "2.2 Frequency Distributions (Numerical)\nWhen working with numerical data, building a frequency distributions requires additional steps compared to categorical data. The challenge lies in the absence of predefined categories or classes. To construct a frequency distribution for numerical data, it is essential to determine the number, width, and limits of the classes. Here are the steps to create a frequency distribution when data is numerical:\n1. Determine the Number of Classes: The number of classes can be estimated using the \\(2^k\\) rule, where \\(k\\) is the smallest integer such that \\(2^k\\) exceeds the total number of observations by the least amount. This ensures the chosen number of classes provides a reasonable level of granularity for summarizing the data.\nEx: If a data set has 50 observations, we would choose six classes since \\(2^6=64\\) is greater than \\(50\\) by the least amount.\n2. Calculate the Width of Each Class: The width of a class is determined using the formula:\n\\[\nWidth = \\frac{Max-Min}{Number\\ of\\ Classes}\n\\]\nEx: If the data set has 50 observations and the minimum value 20 and the maximum is 78, then the width of each class is \\(58/6 \\approx 9.7\\). Hence, we can round up and use a class width of 10. It is important to note to always round up, as this ensures that all data points are included in an class.\n3. Establish Class Limits: The class limits define the range of values in each class. These limits should be chosen such that each data point belongs to only one class.\nEx: Consider a data set of 50 observations where each class has a width of 10. Set the class limits of the first class to [20,30). Note that the square bracket indicates that the point should be included in the class, whereas ) indicates that the point should no be included in the class. The six classes would be [20,30), [30,40), [40,50), [50,60), [60,70), and [70,80). By choosing these classes, each point belongs to only one class.\nExample: Let’s look at a snapshot of the Dow Jones Industrial 30 stock prices. Below you can see the data:\n\n\n\n\n\n\n\n\n\nLet’s follow the steps to build the frequency distribution.\n1. Determine the Number of Classes: Here we choose five classes since \\(2^5=32\\) is greater than \\(30\\) by the least amount.\n2. Calculate the Width of Each Class: The smallest values in the data set is \\(23\\) and the maximum is \\(501\\). This gives us a range of \\(478\\). Now we can just take the range and divide by five to get \\(95.6\\). To make things simple we can round to \\(100\\) and use a class width of \\(100\\).\n3. Establish Class Limits: Since we have rounded up we can be flexible with our class limits. The following class limits are suggested [20,120), [120,220), [220,320), [320,420), and [420,520). Note that each class has a width of \\(100\\), and that each data point belongs to one single class.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Stats II</span>"
    ]
  },
  {
    "objectID": "DescriptiveII.html#frequency-distributions-in-r-categorical",
    "href": "DescriptiveII.html#frequency-distributions-in-r-categorical",
    "title": "2  Descriptive Stats II",
    "section": "2.3 Frequency Distributions in R (Categorical)",
    "text": "2.3 Frequency Distributions in R (Categorical)\nThe process of constructing frequency distributions in R is straightforward. We will be mainly using the table() function. Let’s start by saving the data in a vector:\n\nfood&lt;-c(\"Pizza\",\"Sushi\",\"Sushi\",\"Chicken\",\n        \"Chicken\",\"Pasta\",\"Pasta\",\"Pasta\",\n        \"Sushi\",\"Pasta\",\"Chicken\",\"Pizza\",\n        \"Chicken\",\"Sushi\",\"Pizza\",\"Sushi\",\n        \"Sushi\",\"Sushi\",\"Sushi\",\"Pizza\",\n        \"Pizza\",\"Chicken\",\"Sushi\",\"Pizza\",\n        \"Sushi\")\n\nHere we define a vector called food by assigning (&lt;-) the combination (c) of all the food items. To generate the frequency distribution we simply pass the food vector into the table() command.\n\ntable(food)\n\nfood\nChicken   Pasta   Pizza   Sushi \n      5       4       6      10 \n\n\nAs you can see this tallies all the instances for each item. If instead we wanted to obtain the relative frequency we can use the prop.table() function. This function requires the frequency distribution created by the table() function. Hence, we can first create the frequency distribution, save it into an object, and then generate the relative frequency. The code is below:\n\nfreq&lt;-table(food)\nprop.table(freq)\n\nfood\nChicken   Pasta   Pizza   Sushi \n   0.20    0.16    0.24    0.40 \n\n\nAs a last modification. If you want percent frequencies, you can multiply the prop.table() function by 100, as shown below:\n\nprop.table(freq)*100\n\nfood\nChicken   Pasta   Pizza   Sushi \n     20      16      24      40",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Stats II</span>"
    ]
  },
  {
    "objectID": "DescriptiveII.html#bar-plot-in-r",
    "href": "DescriptiveII.html#bar-plot-in-r",
    "title": "2  Descriptive Stats II",
    "section": "2.4 Bar Plot in R",
    "text": "2.4 Bar Plot in R\nTo create the bar plot we will be using the geom_bar() function within the tidyverse package. We start by loading the package:\n\nlibrary(tidyverse)\n\nNow R will identify the functions ggplot() and geom_bar() from the ggplot library. To construct the plot we will first call on ggplot() and then specify the type of graph we want by calling on geom_bar(). In the aes() function we will specify which variable (or vector) we want to plot.\n\nggplot() + geom_bar(aes(food))\n\n\n\n\n\n\n\n\nWe can enhance the visualization by adding a title and changing the theme. The labs() function allows us to change the titles and the ggthemes package allows us to choose from a variety of themes.\n\nggplot() + geom_bar(aes(food), col=\"black\", alpha=0.5, bg=\"blue\") +\n  labs(title=\"Favorite Food Items\",\n       subtitle=\"Class of 25 Students\",\n       x=\"\", y=\"Frequency\") + \n  theme_clean()\n\n\n\n\n\n\n\n\nA few arguments are worth explaining in the code above. The arguments in the geom_bar() function change the background color (bg), the transparency of the color (alpha), and the color of the outline for the bars (col). Title and subtitles are added within the labs() function. To omit labels we can just open and close quotations. Hence, x=\"\" omits the x label.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Stats II</span>"
    ]
  },
  {
    "objectID": "DescriptiveII.html#frequency-distribution-in-r-numerical",
    "href": "DescriptiveII.html#frequency-distribution-in-r-numerical",
    "title": "2  Descriptive Stats II",
    "section": "2.5 Frequency Distribution in R (Numerical)",
    "text": "2.5 Frequency Distribution in R (Numerical)\nTo construct the frequency distribution in R we will be first generating the classes and then using the table() function as we did in the categorical case. Let’s first get the data into R:\n\ndow&lt;-c(277,174,202,383,358,188,293,156,\n       212,42,149,410,303,165,203,103,\n       22,59,287,121,312,52,53,158,500,\n       96,95,43,188,200)\n\nTo generate the bins we will use the example and procedure found in 2.2. That is we will be using five classes, of width 100. Below is the code to do this:\n\nintervals&lt;-cut_width(dow, boundary=20, width=100)\n(dowfreq&lt;-table(intervals))\n\nintervals\n [20,120] (120,220] (220,320] (320,420] (420,520] \n        9        12         5         3         1 \n\n\nThe process involves two steps. First we place each observation in the dow, into the bins by using the cut_width() function. This involves specifying where we want to start our first bin (boundary=20) and the width of each bin (width 100). The function creates the bins and places each observation in the respective bin. The last step is to tally the results with the table() function.\nTo obtain the cumulative distribution, we can use the cumsum() function. Below we just wrap the frequency distribution (freq) into the cumsum() function.\n\ncumsum(dowfreq)\n\n [20,120] (120,220] (220,320] (320,420] (420,520] \n        9        21        26        29        30",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Stats II</span>"
    ]
  },
  {
    "objectID": "DescriptiveII.html#histograms-in-r",
    "href": "DescriptiveII.html#histograms-in-r",
    "title": "2  Descriptive Stats II",
    "section": "2.6 Histograms in R",
    "text": "2.6 Histograms in R\nTo generate the histogram in R we will use once again the tidyverse package. This time we will use the geom_histogram() function. Below is the code to generate the histogram for the dow data:\n\nggplot() + \n  geom_histogram(aes(dow), bg=\"blue\", alpha=0.5, col=\"black\",\n                 bins=5,\n                 binwidth = 100,\n                 boundary=20) +\n  labs(title=\"Dow Stock Prices\",\n       y=\"Frequency\", x=\"\") + \n  theme_clean()\n\n\n\n\n\n\n\n\nWithin the geom_histogram() command we can set the classes (or bins) for the histogram. The bins argument specifies the number of bins, bin.width specifies the bin width, and the boundary specifies where should the histogram starts. This histogram allows us to observe quickly that most stocks in the dow are priced between 20 and 220 dollars.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Stats II</span>"
    ]
  },
  {
    "objectID": "DescriptiveII.html#exercises",
    "href": "DescriptiveII.html#exercises",
    "title": "2  Descriptive Stats II",
    "section": "2.7 Exercises",
    "text": "2.7 Exercises\nThe following exercises will help you practice summarizing data with tables and simple graphs. In particular, the exercises work on:\n\nDeveloping frequency distributions for both categorical and numerical data.\nConstructing bar charts, histograms, and line charts.\nCreating contingency tables.\n\nAnswers are provided below. Try not to peak until you have a formulated your own answer and double checked your work for any mistakes.\n\nExercise 1\nInstall the ISLR2 package in R. You will need the BrainCancer data set to answer this question.\n\nConstruct a frequency and relative frequency table of the Diagnosis variable. What was the most common diagnosis? What percentage of the sample had this diagnosis?\n\n\n\nAnswer\n\nThe most common diagnosis is Meningioma, a slow-growing tumor that forms from the membranous layers surrounding the brain and spinal cord. The diagnosis represents about \\(48.28\\)% of the sample.\nStart by loading the ISLR2 package. To construct the frequency distribution table, use the table() function.\n\nlibrary(ISLR2)\ntable(BrainCancer$diagnosis)\n\n\nMeningioma  LG glioma  HG glioma      Other \n        42          9         22         14 \n\n\nThe relative frequency distribution can be easily retrieved by saving the frequency table in an object and then using the prop.table() function.\n\nfreq&lt;-table(BrainCancer$diagnosis)\nprop.table(freq)\n\n\nMeningioma  LG glioma  HG glioma      Other \n 0.4827586  0.1034483  0.2528736  0.1609195 \n\n\n\n\nConstruct a bar chart. Summarize the findings.\n\n\n\nAnswer\n\nThe majority of diagnosis are Meningioma. Low grade glioma is the least common of diagnosis. High grade glioma and other diagnosis have about the same frequency.\nTo construct the bar chart use the geom_bar() function from tidyverse.\n\nlibrary(tidyverse)\nlibrary(ggthemes)\nggplot(data=BrainCancer) + \n  geom_bar(aes(diagnosis), alpha=0.5, col=\"black\") + \n  theme_clean()\n\n\n\n\n\n\n\n\n\n\nConstruct a contingency table that shows the Diagnosis along with the Status. Which diagnosis had the highest number of non-survivals (0)? What was the survival rate of this diagnosis?\n\n\n\nAnswer\n\n\\(33\\) people did not survive Meningioma. The survival rate of Meningioma is only \\(21.43\\)%.\nUse the table() function one more time to create the contingency table for the two variables.\n\n(freq2&lt;-table(BrainCancer$status,BrainCancer$diagnosis))\n\n   \n    Meningioma LG glioma HG glioma Other\n  0         33         5         5     9\n  1          9         4        17     5\n\n\nTo get the survival rates, we can use the prop.table() function once again.\n\nprop.table(freq2,margin = 2)\n\n   \n    Meningioma LG glioma HG glioma     Other\n  0  0.7857143 0.5555556 0.2272727 0.6428571\n  1  0.2142857 0.4444444 0.7727273 0.3571429\n\n\n\n\n\nExercise 2\nYou will need the airquality data set (in base R) to answer this question.\n\nConstruct a frequency distribution for Temp. Use five classes with widths of \\(50&lt;x\\le60\\); \\(60&lt;x\\le70\\); etc. Which interval had the highest frequency? How many times was the temperature between \\(50\\) and \\(60\\) degrees?\n\n\n\nAnswer\n\nThe highest frequency is in the \\(80 &lt; x ≤ 90\\) bin. \\(8\\) temperatures were between \\(50 &lt; x ≤ 60\\) degrees.\nWe can create the intervals using the cut_width() function:\n\nintervals &lt;- cut_width(airquality$Temp, width = 10,\n                       boundary=50)\n\nThe frequency distribution can be obtained by using the table() function on the interval.cut object created above.\n\ntable(intervals)\n\nintervals\n [50,60]  (60,70]  (70,80]  (80,90] (90,100] \n       8       25       52       54       14 \n\n\n\n\nConstruct a relative frequency, cumulative frequency and the relative cumulative frequency distributions. What proportion of the time was Temp between \\(50\\) and \\(60\\) degrees? How many times was the Temp \\(70\\) degrees or less? What proportion of the time was Temp more than \\(70\\) degrees?\n\n\n\nAnswer\n\nThe temperature was \\(5.22\\)% of the time between \\(50\\) and \\(60\\); The temperature was \\(70\\) or less \\(33\\) times; The temperature was above \\(70\\), \\(78.43\\)% of the time.\nTo get the relative frequency table, start by saving the proportion table into an object.Then you can use the prop.table() function.\n\nfreq&lt;-table(intervals) \nprop.table(freq)\n\nintervals\n   [50,60]    (60,70]    (70,80]    (80,90]   (90,100] \n0.05228758 0.16339869 0.33986928 0.35294118 0.09150327 \n\n\nFor the cumulative distribution you can use the cumsum() function on the frequency distribution.\n\ncumulfreq&lt;-cumsum(freq)\ncumulfreq\n\n [50,60]  (60,70]  (70,80]  (80,90] (90,100] \n       8       33       85      139      153 \n\n\nLastly, for the relative cumulative distribution table, you can use the cumsum() function on the relative frequency table.\n\ncumsum(prop.table(freq))\n\n   [50,60]    (60,70]    (70,80]    (80,90]   (90,100] \n0.05228758 0.21568627 0.55555556 0.90849673 1.00000000 \n\n\n\n\nConstruct the histogram. Is the distribution symmetric? If not, is it skewed to the left or right?\n\n\n\nAnswer\n\nThe distribution is not perfectly symmetric. It is skewed slightly to the left (see histogram.)\nUse the geom_histogram() function to create the histogram.\n\nggplot() + \n  geom_histogram(aes(airquality$Temp), col=\"black\", \n                 bg=\"darkgreen\",alpha=0.2,\n                 bins=5,\n                 binwidth = 10,\n                 boundary=0) + theme_clean() +\n  labs(x=\"Temperature\", y=\"Frequency\")\n\n\n\n\n\n\n\n\n\n\n\nExercise 3\nYou will need the Portfolio data set from the ISLR2 package to answer this question.\n\nConstruct a line chart that shows the returns over time for each portfolio (X and Y) by using two lines each with a unique color. Assume the data is for the period \\(1901\\) to \\(2000\\). Include also a legend that matches colors to portfolios.\n\n\n\nAnswer\n\nFrom \\(1901\\) through \\(2000\\), both portfolios have behaved very similarly. Returns are between \\(-3\\)% and \\(3\\)%, there is no trend, and positive (negative) returns for X seem to match with positive (negative) returns of Y.\nYou can use the geom_line() function to create a line for each portfolio.\n\nggplot() +\n  geom_line(aes(y=Portfolio$Y,x=seq(1901,2000)), col=\"blue\") +\n  geom_line(aes(y=Portfolio$X,x=seq(1901,2000)), col=\"grey\") +\n  theme_clean() +\n  labs(x=\"Year\", y=\"Return\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Stats II</span>"
    ]
  },
  {
    "objectID": "DescriptiveIII.html",
    "href": "DescriptiveIII.html",
    "title": "3  Descriptive Statistics III",
    "section": "",
    "text": "3.1 The Mean\nUnderstanding where the “center” of a dataset lies is a fundamental step when interpreting and analyzing data. Measures of central location provide different ways to identify the “typical” value in a dataset, each with its unique strengths and limitations.\nThe mean is the average value for a numerical variable. It is a widely understood and straightforward measure to calculate. It incorporates all data points, providing a comprehensive representation of the data set. However, its reliance on every value also makes it sensitive to outliers or skewed distributions, which may cause it to not accurately reflect the true center of the data.\nThe sample statistic is estimated by:\n\\[\n\\bar{x}=\\frac{\\sum x_{i}}{n}\n\\]\nwhere \\(x_i\\) is observation \\(i\\), and \\(n\\) is the number of observations in the sample. The formula instructs you to add up all of the values in a variable and then divide by the sample size. If you wish to estimate the population parameter you can use:\n\\[\n\\mu=\\frac{\\sum x_{i}}{N}\n\\]\nwhere \\(N\\) is the population size.\nEx 1: Consider the following sample of numbers \\(x=\\{1,4,2,1\\}\\). The mean would be equal to \\(\\bar{x}=\\frac{1+4+2+1}{4}\\) or \\(\\bar{x}=2\\).\nEx 2: Consider the following sample of numbers \\(x=\\{1,4,2,1,100\\}\\). The mean is \\(\\bar{x}=21.6\\). Although most of the numbers are in the range \\((1,4)\\), the \\(100\\) biases the mean to \\(21.6\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics III</span>"
    ]
  },
  {
    "objectID": "DescriptiveIII.html#the-median",
    "href": "DescriptiveIII.html#the-median",
    "title": "3  Descriptive Statistics III",
    "section": "3.2 The Median",
    "text": "3.2 The Median\nThe median is the value in the middle when data is organized in ascending order. When \\(n\\) is even, the median is the average between the two middle values. The median is resistant to outliers, making it an ideal measure for skewed data or data sets with irregular distributions. It is particularly useful for ordinal data, where precise ranking is important. However, the median does not utilize all data points, which can lead to less precise comparisons compared to other measures like the mean.\nEx 1: Consider the following numbers \\(x=\\{1,4,2,1\\}\\). We first sort the data to obtain \\(x_{sorted}=\\{1,1,2,4\\}\\). Since the number of observations is even (\\(n=4\\)), the median is the average of the two middle numbers \\((1,2)\\). \\(median_x=\\frac{1+2}{2}\\) or \\(1.5\\).\nEx 2: Consider the following numbers \\(x=\\{1,4,2,1,100\\}\\). We once again sort the number obtaining \\(x_{sorted}=\\{1,1,2,4,100\\}\\). Since \\(n=5\\) in this case we can identify the median as the third value in \\(x_{sorted}\\). \\(median_x=2\\). Note that the inclusion of \\(100\\) in the data did not change much the measure of central location.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics III</span>"
    ]
  },
  {
    "objectID": "DescriptiveIII.html#the-mode",
    "href": "DescriptiveIII.html#the-mode",
    "title": "3  Descriptive Statistics III",
    "section": "3.3 The Mode",
    "text": "3.3 The Mode\nThe mode is the value with highest frequency from a set of observations. This measure is particularly useful for categorical data as it helps determine popularity of values. It can be applied to both numerical and non-numerical data sets. The mode has its limitations; it may not exist in cases where all values occur with equal frequency, and there may be multiple modes, which can complicate interpretation. Additionally, since the mode focuses only on the most frequent value, it does not account for other data points, limiting its overall utility as a comprehensive measure.\nEx 1: Consider the following numbers \\(x=\\{1,4,2,1\\}\\). Since \\(1\\) is repeated twice and all other numbers just repeated once, \\(x_{mode}=1\\).\nEx 2: Consider the following numbers \\(x=\\{1,4,2,1,4\\}\\). Now \\(4\\) is also repeated twice. The variable has two modes \\(x_{mode}=\\{1,2\\}\\). \\(x\\) is said to be bimodal.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics III</span>"
    ]
  },
  {
    "objectID": "DescriptiveIII.html#the-weighted-mean",
    "href": "DescriptiveIII.html#the-weighted-mean",
    "title": "3  Descriptive Statistics III",
    "section": "3.4 The Weighted Mean",
    "text": "3.4 The Weighted Mean\nThe weighted mean is useful in scenarios where some data points are more significant than others, such as in financial portfolios, grade point averages, or survey results, as it accounts for variability in importance across observations. This measure requires additional information in the form of weights (\\(w_i\\)), which may not always be available or accurate. It is calculated the sum product of values (\\(x_i\\)) and weights (\\(w_i\\)) and then dividing by the sum of weights. Mathematically, the weighted average is:\n\\[\n\\bar{x}_w=\\frac{\\sum w_{i}x_{i}}{\\sum w_{i}}\n\\]\nEx: Consider three different stocks \\(S=\\{T, C, X\\}\\) with stock returns of \\(R=\\{2,4,10\\}\\). Each stock has a weight in the portfolio of \\(W=\\{0.3,0.2,0.5\\}\\). The average return of the portfolio is \\(\\bar{x}_{weighted}=\\frac{0.6+0.8+5}{1}\\) or \\(\\bar{x}_{weighted}=6.4\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics III</span>"
    ]
  },
  {
    "objectID": "DescriptiveIII.html#the-geometric-mean",
    "href": "DescriptiveIII.html#the-geometric-mean",
    "title": "3  Descriptive Statistics III",
    "section": "3.5 The Geometric Mean",
    "text": "3.5 The Geometric Mean\nThe geometric mean is a multiplicative average that is less sensitive to outliers relative to the arithmetic mean. It is useful when averaging growth rates or rates of return. It is calculated by:\n\\[\n\\bar{x}_g=\\sqrt[n]{(1+r_1)*(1+r_2)...(1+r_n)}-1\n\\]\nwhere \\(\\sqrt[n]{}\\) is the \\(n_{th}\\) root, and \\(r_i\\) are the returns or growth rates. When working with growth rates or rates of return, you add 1 to each rate because these metrics represent changes relative to a base value.\nWhen dealing with proportions, these already reflect a standalone quantity, not a relative change. As a consequence, there is no need to add 1. In this case the geometric mean simplifies to:\n\\[\n\\bar{x}_g=\\sqrt[n]{(x_1) \\times (x_2)...(x_n)}\n\\]\nEx: Consider the variable \\(x=\\{0.2,0.3,0.1,0.1\\}\\). The geometric mean is equal to \\(\\bar{x}_g=\\sqrt[4]{(1.2 \\times 1.3 \\times 1.1 \\times 1.1)}-1\\) or \\(\\bar{x_g}=0.17\\) if x represents growth rates. When x represents ratios from a whole, the geometric mean is \\(\\bar{x}_g=\\sqrt[4]{0.2 \\times 0.3 \\times 0.1 \\times 0.1)}\\) or \\(0.156\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics III</span>"
    ]
  },
  {
    "objectID": "DescriptiveIII.html#measures-of-central-location-in-r",
    "href": "DescriptiveIII.html#measures-of-central-location-in-r",
    "title": "3  Descriptive Statistics III",
    "section": "3.6 Measures of Central Location in R",
    "text": "3.6 Measures of Central Location in R\nBase R has a collection of functions that calculate measures of central location. Let’s consider the following data on approval ratings:\n\nlibrary(tidyverse)\n(poll&lt;-tibble(date=c(\"01/01/24\", \"02/01/24\", \n                    \"03/01/24\", \"04/01/24\"),\n             people=c(50,100,30,250),\n             approval=c(0.25,0.25,0.7,0.85)))\n\n# A tibble: 4 × 3\n  date     people approval\n  &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 01/01/24     50     0.25\n2 02/01/24    100     0.25\n3 03/01/24     30     0.7 \n4 04/01/24    250     0.85\n\n\nTo calculate the mean we can just pass a vector into the mean() function. Hence, the mean approval is:\n\nmean(poll$approval)\n\n[1] 0.5125\n\n\nTo calculate the mode we will use the table() function, as there is no mode function in base R.\n\ntable(poll$approval)\n\n\n0.25  0.7 0.85 \n   2    1    1 \n\n\nFor the median we will use the median() function.\n\nmedian(poll$approval)\n\n[1] 0.475\n\n\nThe weighted average can be calculated using the weighted.mean() function. Let the approval be the value and number of people surveyed the weight.\n\nweighted.mean(x=poll$approval,w = poll$people)\n\n[1] 0.6302326\n\n\nLastly, the geometric mean has no built in function in base R. However, we can easily calculate it with the command:\n\ngeometric_mean &lt;- prod(poll$approval)^(1/length(poll$approval))\n\nSince the approval rating is a percentage of the total people polled there is no need to add one to these numbers.\nThe summary() calculates a collection of summary statistics for a vector or data frame. Below we apply it to the entire data set:\n\nsummary(poll)\n\n     date               people         approval     \n Length:4           Min.   : 30.0   Min.   :0.2500  \n Class :character   1st Qu.: 45.0   1st Qu.:0.2500  \n Mode  :character   Median : 75.0   Median :0.4750  \n                    Mean   :107.5   Mean   :0.5125  \n                    3rd Qu.:137.5   3rd Qu.:0.7375  \n                    Max.   :250.0   Max.   :0.8500  \n\n\nBelow a list of the functions used:\n\nThe mean() function calculates the average for a vector of numbers.\nThe median() function calculates the median for a vector of numbers.\nThe table() function generates the frequency distribution so that we can identify the mode or modes.\nThe weighted.mean() function calculates the weighted mean for a vector of numbers, and corresponding vector of weights.\nThe lenght() function calculates the length of a vector.\nThe summary() function provides measures of location for a vector of numbers.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics III</span>"
    ]
  },
  {
    "objectID": "DescriptiveIII.html#exercises",
    "href": "DescriptiveIII.html#exercises",
    "title": "3  Descriptive Statistics III",
    "section": "3.7 Exercises",
    "text": "3.7 Exercises\nThe following exercises will help you practice the measures of central location. In particular, the exercises work on:\n\nCalculating the mean, median, and the mode.\nCalculating the weighted average.\nApplying the geometric mean for growth rates and returns.\n\nAnswers are provided below. Try not to peak until you have a formulated your own answer and double checked your work for any mistakes.\n\nExercise 1\nFor the following exercises, make your calculations by hand and verify results using R functions when possible.\n\nUse the following observations to calculate the mean, the median, and the mode.\n\n\n\n8\n10\n9\n12\n12\n\n\n\n\n\n\nAnswer\n\nTo find the mean we will use the following formula \\(( \\frac{1}{n} \\sum_{i=i}^{n} x_{i})\\). The summation of the values is \\(51\\) and the number of observations is \\(5\\). The mean is \\(51/5=10.2\\).\nThe median is found by locating the middle value when data is sorted in ascending order. The median in this example is \\(10\\).\nThe mode is the value with the highest frequency. In this example the mode is \\(12\\) since it is repeated twice and all other numbers appear only once.\nThe mean can be easily verified in R by using the mean() function:\n\nmean(c(8,10,9,12,12))\n\n[1] 10.2\n\n\nSimilarly, the median is easily verified by using the median() function:\n\nmedian(c(8,10,9,12,12))\n\n[1] 10\n\n\nWe can use the table() function to calculate frequencies and easily identify the mode.\n\ntable(c(8,10,9,12,12))\n\n\n 8  9 10 12 \n 1  1  1  2 \n\n\n\n\nUse following observations to calculate the mean, the median, and the mode.\n\n\n\n-4\n0\n-6\n1\n-3\n-4\n\n\n\n\n\n\nAnswer\n\nThe mean is \\(-2.67\\), the median is \\(-3.5\\), the mode is \\(-4\\).\nThe mean is verified in R:\n\nmean(c(-4,0,-6,1,-3,-4))\n\n[1] -2.666667\n\n\nThe median in R:\n\nmedian(c(-4,0,-6,1,-3,-4))\n\n[1] -3.5\n\n\nFinally, the mode in R:\n\ntable(c(-4,0,-6,1,-3,-4))\n\n\n-6 -4 -3  0  1 \n 1  2  1  1  1 \n\n\n\n\nUse the following observations, calculate the mean, the median, and the mode.\n\n\n\n20\n15\n25\n20\n10\n15\n25\n20\n15\n\n\n\n\n\n\nAnswer\n\nThe mean is \\(18.33\\), the median is \\(20\\), the data is bimodal with both \\(15\\) and \\(20\\) being modes.\nThe mean is verified in R:\n\nmean(c(20,15,25,20,10,15,25,20,15))\n\n[1] 18.33333\n\n\nThe median in R:\n\nmedian(c(20,15,25,20,10,15,25,20,15))\n\n[1] 20\n\n\nThe frequency distribution identifies the modes:\n\ntable(c(20,15,25,20,10,15,25,20,15))\n\n\n10 15 20 25 \n 1  3  3  2 \n\n\n\n\n\nExercise 2\nDownload the ISLR2 package. You will need the OJ data set to answer this question.\n\nFind the mean price for Country Hill (PriceCH) and Minute Maid (PriceMM).\n\n\n\nAnswer\n\nThe mean price for Country Hill is \\(1.87\\). The mean price for Minute Maid is \\(2.09\\).\nThe means can be easily found with the mean() function:\n\nlibrary(ISLR2)\nOJ=OJ\nmean(OJ$PriceCH)\n\n[1] 1.867421\n\nmean(OJ$PriceMM)\n\n[1] 2.085411\n\n\n\n\nFind the mean price of Country Hill (PriceCH) at each store (StoreID). Which store provides the better price?\n\n\n\nAnswer\n\nThe mean price at store 1 for Country Hill is \\(1.80\\). The juice is cheaper at store 1.\nThe means for each store can be found by using group_by() and summarise().The mean price at each store is:\n\nOJ %&gt;% group_by(StoreID) %&gt;% summarise(MeanCH=mean(PriceCH))\n\n# A tibble: 5 × 2\n  StoreID MeanCH\n    &lt;dbl&gt;  &lt;dbl&gt;\n1       1   1.80\n2       2   1.84\n3       3   1.93\n4       4   1.95\n5       7   1.84\n\n\n\n\nFind the median price paid by Country Hill (PriceCH) purchasers (Purchase) in all stores? Which store had the better median price?\n\n\n\nAnswer\n\nPurchasers of Country Hill at store 1 paid a median price of \\(1.76\\) for Country Hill juice. This once again was the lowest price.\nThe median price for Country Hill purchasers at each store is given by:\n\nOJ %&gt;% filter(Purchase==\"CH\") %&gt;% group_by(StoreID) %&gt;% summarise(MedianCH=median(PriceCH))\n\n# A tibble: 5 × 2\n  StoreID MedianCH\n    &lt;dbl&gt;    &lt;dbl&gt;\n1       1     1.76\n2       2     1.86\n3       3     1.99\n4       4     1.99\n5       7     1.86\n\n\n\n\n\nExercise 3\n\nOver the past year an investor bought TSLA. She made these purchases on three occasions at the prices shown in the table below. Calculate the average price per share.\n\n\n\n\nDate\nPrice Per Share\nNumber of Shares\n\n\n\n\nFebruary\n250.34\n80\n\n\nApril\n234.59\n120\n\n\nAug\n270.45\n50\n\n\n\n\n\nAnswer\n\nThe average price of sale is found by using the weighted average formula. \\(\\frac{\\sum w_{i}x_{i}}{\\sum w_{i}}\\) The weights (\\(w_{i}\\)) are given by the number of shares bought and the values (\\(x_{i}\\)) are the prices. The weighted average is \\(246.802\\).\nIn R you can create two vectors. One holds the share price and the other one the number of shares bought.\n\nPricePerShare&lt;-c(250.34,234.59,270.45)\nNumberOfShares&lt;-c(80,120,50)\n\nNext, can use the weighted.mean() function in R, with PricePerShare as the value and NumberOfShares as the weights. The weighted average is:\n\n(WeightedAverage&lt;-weighted.mean(PricePerShare,NumberOfShares))\n\n[1] 246.802\n\n\n\n\nWhat would have been the average price per share if the investor would have bought equal amounts of shares each month?\n\n\n\nAnswer\n\nThe average if equal shares were bought would be \\(251.7933\\).\nIn R you can use the mean() function on the PricePerShare vector.\n\n(Average&lt;-mean(PricePerShare))\n\n[1] 251.7933\n\n\n\n\n\nExercise 4\n\nConsider the following observations for the consumer price index (CPI). Calculate the inflation rate (Growth Rate of the CPI) for each period.\n\n\n\n1.0\n1.3\n1.6\n1.8\n2.1\n\n\n\n\n\n\nAnswer\n\nThe inflation rate is the percentage change in the CPI. The inflation rate for each period is shown in the table below:\n\n\n\n30%\n23.08%\n12.5%\n16.67%\n\n\n\nIn R create an object to store the values of the CPI:\n\nCPI&lt;-c(1,1.3,1.6,1.8,2.1)\n\nNext use the diff() function to find the difference between the end value and start value. Divide the result by a vector of starting value and multiply times 100.\n\n(Inflation&lt;-100*diff(CPI)/CPI[1:4])\n\n[1] 30.00000 23.07692 12.50000 16.66667\n\n\n\n\nWhat is the average growth rate for the inflation rate?\n\n\n\nAnswer\n\nThe average growth rate is \\(31.61%\\)\nWe can use the geometric mean formula with compounding. In R:\n\n(Growth&lt;-prod(1.3,1.2308,1.125,1.1667)^(1/4)-1)\n\n[1] 0.2038175\n\n\n\n\nSuppose that you want to invest $1000 dollars in a stock that is predicted to yield the following returns in the next four years. Calculate both the arithmetic mean and the geometric mean. Use the geometric mean to estimate how much money you would have by the end of year 4.\n\n\n\nYear\nAnnual Return\n\n\n\n\n1\n17.3\n\n\n2\n19.6\n\n\n3\n6.8\n\n\n4\n8.2\n\n\n\n\n\n\nAnswer\n\nAt the end of 4 years it is predicted that you would have \\(1621.17\\) dollars. Each year you would have gained \\(12.84\\)% on average.\nIn R include the annual rates in a vector:\n\ngrowth&lt;-c(0.173,0.196,0.068,0.082)\n\nThe arithmetic mean is:\n\n100*mean(growth)\n\n[1] 12.975\n\n\nThe geometric mean is:\n\n(geom&lt;-((prod(1+growth))^(1/length(growth))-1)*100)\n\n[1] 12.8384\n\n\nAt the end of the four years we would have:\n\n1000*(1+geom/100)^4\n\n[1] 1621.167",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics III</span>"
    ]
  },
  {
    "objectID": "DescriptiveIV.html",
    "href": "DescriptiveIV.html",
    "title": "4  Descriptive Stats IV",
    "section": "",
    "text": "4.1 The Range\nMeasures of dispersion provide insights into how much the values in a data set deviate from the central tendency. In a business context, understanding variability is essential for assessing risk, as it helps decision-makers anticipate fluctuations in revenue, costs, and market conditions. Below, we explore the main statistics that help us quantify variability and use them to make better business decisions.\nThis is the simplest measure of dispersion, defined as the difference between the largest and smallest values in a variable. While straightforward, the range only accounts for the extremes and does not reflect the variability within the variable. The range is calculated by:\n\\[Range=Maximum-Minimum\\]\nEx: Consider the data \\(x=\\{480, 1050, 1400, 2500, 3200\\}\\). The range is given by \\(Range=3200-480=2720\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Stats IV</span>"
    ]
  },
  {
    "objectID": "DescriptiveIV.html#the-variance",
    "href": "DescriptiveIV.html#the-variance",
    "title": "4  Descriptive Stats IV",
    "section": "4.2 The Variance",
    "text": "4.2 The Variance\nThe variance gives us a more comprehensive look at dispersion by considering how each data point deviates from the mean. It summarizes the squared deviations from the mean by finding their average. The population parameter is given by:\n\\[\\sigma^2= \\frac{\\sum (x_i-\\mu)^2}{N}\\]\nwhile the sample statistic is\n\\[s^2=\\frac{\\sum (x_i-\\bar{x})^2}{n-1}\\]\nExample: Let’s consider a sample of the price of Ether (a famous cryptocurrency). Below is a graph of the prices:\n\n\n\n\n\n\n\n\n\nThe y axis represents the price, while the x axis represents the period of time. The red line is the average price of Ether. The way the variance calculates dispersion, is by first finding the distance between each point and the average. The image below illustrates these distances:\n\n\n\n\n\n\n\n\n\nIdeally, our measure of dispersion would simply average the distances from each data point to the mean, giving us a clear indication of how much the data varies around this central point. Since the mean is a measure that balances the distances from the mean, the sum of all deviations is equal to zero. This “balancing” effect is visually apparent in the graph, where positive deviations are exactly offset by negative ones.\nTo address this issue, the variance squares each deviation from the mean before finding their average. This squaring eliminates negative values, ensuring a non-zero measure of spread. The table below illustrates these calculations:\n\n\n\n\n\n\n\n\n\nThe second column shows the deviations from the mean. You can convince yourself that these add up to zero. The third column squares the deviations. The sample variance, averages the numbers in the third column using the degrees of freedom (\\(n-1\\)). The result is \\(s^2=1221880\\), which means that the price of Ether varies on average \\(1,221,880\\) squared deviations from the mean. The result by itself is not very intuitive as it is measured in squared dollars. We can, however, compare the variances from different variables to assess which variable has the most dispersion.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Stats IV</span>"
    ]
  },
  {
    "objectID": "DescriptiveIV.html#the-standard-deviation",
    "href": "DescriptiveIV.html#the-standard-deviation",
    "title": "4  Descriptive Stats IV",
    "section": "4.3 The Standard Deviation",
    "text": "4.3 The Standard Deviation\nThe standard deviation is derived by taking the square root of the variance. Remember, the variance employs squared deviations to eliminate negative values and calculate spread. By taking the square root, the standard deviation reverts the measure of dispersion back to the original units of the variable. This transformation makes the standard deviation more intuitive; it directly quantifies how much each data point deviates from the mean in the same units as the variable itself. Consequently, the standard deviation is a clear and intuitive measure of variability.\nTo find the standard deviation, take the square root of the variance. For the population parameter use: \\[\\sigma=\\sqrt{\\sigma^2}\\]\nFor the sample standard deviation use: \\[s=\\sqrt{s^2}\\]\nEx: Consider once more the price of Ether. That is. \\(x=\\{480, 1050, 1400, 2500, 3200\\}\\). In the previous section we found that the variance \\(s^2=977594\\). Using the squared root we find that \\(s=\\sqrt{977594}=988.73\\). If the price of Ether is in dollars, then, the price varies from the mean 988.73 dollars on average.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Stats IV</span>"
    ]
  },
  {
    "objectID": "DescriptiveIV.html#mean-absolute-deviation",
    "href": "DescriptiveIV.html#mean-absolute-deviation",
    "title": "4  Descriptive Stats IV",
    "section": "4.4 Mean Absolute Deviation",
    "text": "4.4 Mean Absolute Deviation\nAnother measure of data variability is the Mean Absolute Deviation (MAD), which calculates variability using absolute deviations from the mean. This approach not only keeps the measure in the same units as the data but also makes it less sensitive to large deviations. Practically, the Mean Absolute Deviation measures the average deviation from the mean, by using absolute deviations. It is calculated by:\n\\[MAD=\\frac{\\sum |x_i-\\mu|}{N}\\] where the | | operator indicates absolute value. You can estimates the mad for a sample with:\n\\[mad=\\frac{\\sum |x_i-\\bar{x}|}{n}\\]\nExample: Let’s consider once more the price of Ether. Recall, that if we calculate the deviations from the mean we obtain the second column in the table below:\n\n\n\n\n\n\n\n\n\nFor the MAD, focus on the fourth column which lists the absolute deviations from the mean. Averaging these gives us the Mean Absolute Deviation (MAD), which equals \\(MAD=899.2\\). This result states that, on average, each data point is roughly \\(900\\) dollars away from the mean. Note that the standard deviation is higher (\\(988.73\\)) since the squaring of deviations disproportionately amplifies larger ones, pushing the standard deviation above the MAD.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Stats IV</span>"
    ]
  },
  {
    "objectID": "DescriptiveIV.html#coefficient-of-variation",
    "href": "DescriptiveIV.html#coefficient-of-variation",
    "title": "4  Descriptive Stats IV",
    "section": "4.5 Coefficient of Variation",
    "text": "4.5 Coefficient of Variation\nThe Coefficient of Variation simplifies comparisons of variability across variables with different units or scales, by dividing the standard deviation by the mean. It is calculated by:\n\\[CV=\\frac{s}{\\bar{x}}\\]\nExample: Consider the table below, that shows information on two different stocks:\n\n\n\n\n\n\n\n\n\nThe third column shows the standard deviation of each stock. One could conclude that both stocks vary the same as they have the same standard deviation of one. Recall that the coefficient of variation considers the variable’s scale by incorporating the mean into its calculation. Since one stock is centered around 1 dollar and the other around 100 dollars (second column), it is clear that they do not vary similarly percentage-wise. Stock A varies 100% from the mean, whereas Stock B only varies 1%. Hence, the coefficient of variation would identify Stock A as the more variable stock.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Stats IV</span>"
    ]
  },
  {
    "objectID": "DescriptiveIV.html#the-sharpe-ratio",
    "href": "DescriptiveIV.html#the-sharpe-ratio",
    "title": "4  Descriptive Stats IV",
    "section": "4.6 The Sharpe Ratio",
    "text": "4.6 The Sharpe Ratio\nThe Sharpe Ratio measures how much excess return an investor receives for the extra volatility (risk) taken on beyond the risk-free rate. It essentially uses the same principle of normalization as the CV but adds the dimension of risk-adjusted performance. If the CV tells you the variability of returns in relation to their mean, the Sharpe Ratio tells you if that variability is worth it by comparing it against what you could safely earn without risk. In essence, while the CV highlights variability, the Sharpe Ratio motivates investment choices by rewarding higher returns per unit of risk taken.\nIn sum, the Sharpe ratio quantifies the excess return of an investment over the risk free return. It is calculated by:\n\\[\\frac{\\bar{R_p}-R_f}{s}\\] where \\(\\bar{R_p}\\) is the mean return of the portfolio, \\(R_f\\) is the risk free return, and \\(s\\) is the standard deviation.\nExample: Consider the table below that includes a collection of investments.\n\n\n\n\n\n\n\n\n\nThe table shows four investments (Apple, Bitcoin, Shiba, and S&P). Just looking at the daily return, it is clear that Shiba provides the best returns. However, the cost for that high return is reflected in variability (22% for Shiba). The S&P is clearly the safest investment with a standard deviation of only 0.9%. The Sharpe Ratio, marries these two metrics showing the return per unit of risk taken over the free rate. If we assume a 0% risk free rate, the investment with the highest Sharpe Ratio is the S&P at 0.11.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Stats IV</span>"
    ]
  },
  {
    "objectID": "DescriptiveIV.html#measures-of-dispersion-in-r",
    "href": "DescriptiveIV.html#measures-of-dispersion-in-r",
    "title": "4  Descriptive Stats IV",
    "section": "4.7 Measures of Dispersion in R",
    "text": "4.7 Measures of Dispersion in R\nLet’s use some stock returns to explore R functions that allow us to calculate measures of dispersion. You can run the following command to get the data into R:\n\nlibrary(tidyverse)\nreturns&lt;-read_csv(\"https://jagelves.github.io/Data/returns.csv\")\n\nUse the glimpse() fucntion to view the data:\n\nglimpse(returns)\n\nRows: 1,182\nColumns: 3\n$ date   &lt;chr&gt; \"1/3/20\", \"1/6/20\", \"1/7/20\", \"1/8/20\", \"1/9/20\", \"1/10/20\", \"1…\n$ Stock  &lt;chr&gt; \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\",…\n$ Return &lt;dbl&gt; -0.009769540, 0.007936685, -0.004714194, 0.015958317, 0.0210183…\n\n\nIt seems like the data is stock returns for different companies. We can start by finding the average return of each stock. To do this, let’s use the group_by() and summarise() functions.\n\nreturns %&gt;% group_by(Stock) %&gt;% \n  summarise(Mean=mean(Return))\n\n# A tibble: 3 × 2\n  Stock     Mean\n  &lt;chr&gt;    &lt;dbl&gt;\n1 AAPL  0.00173 \n2 SPY   0.000828\n3 TSLA  0.00511 \n\n\nThe group_by() function makes a group out of every unique entry in the Stock variable. Since, there are three unique entries (AAPL, SPY and TSLA), it created three groups. The summarise() function allows us to combine (or use) all of the entries of a particular stock to find a summary statistic. Since we specified mean(), the command returns the mean of the particular stock returns. It seems like TSLA has the highest mean return at 0.5%.\nWe can keep adding to the table by specifying other measures. To calculate the variance we can use the var() function, for the standard deviation we can use the sd() function, and for the coefficient of variation we can find the ratio between the standard deviation and the mean.\n\nreturns %&gt;% group_by(Stock) %&gt;% \n  summarise(Mean=mean(Return),\n            Variance=var(Return),\n            SD=sd(Return),\n            CV=SD/Mean*100)\n\n# A tibble: 3 × 5\n  Stock     Mean Variance     SD    CV\n  &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 AAPL  0.00173  0.000659 0.0257 1483.\n2 SPY   0.000828 0.000313 0.0177 2138.\n3 TSLA  0.00511  0.00252  0.0502  983.\n\n\nTSLA seems to have the highest variation when following the standard deviation. However, if we look at the variation as a percentage of the mean, SPY seems to have the highest variation for the period considered. In the table below we once more use the group_by() and summarise() functions to calculate the other measures of dispersion.\n\nreturns %&gt;% group_by(Stock) %&gt;% \n  summarise(Range=diff(range(Return)),\n            MAD=mean(abs(Return-mean(Return))),\n            Sharpe=(mean(Return)-0.0001)/sd(Return))\n\n# A tibble: 3 × 4\n  Stock Range    MAD Sharpe\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 AAPL  0.251 0.0179 0.0635\n2 SPY   0.203 0.0107 0.0411\n3 TSLA  0.418 0.0354 0.0998\n\n\nIn this second table the range is calculated by finding the difference between the minimum and the maximum. We can retrieve the minimum and the maximum by using the range() function. The diff() function finds the difference (or range) between these two numbers. The MAD is calculated straight from the formula. Mainly, finding the absolute deviations from the mean and the averaging them out. Lastly, the Sharpe ratio assumes a risk free daily rate of 0.01%. Once again, TSLA seems like the investment that yields the highest return despite its higher variability.\nBelow is a list of the functions used:\n\nThe range() function returns the maximum and minimum of a vector of values.\nThe diff() function finds the first difference of a vector. Position 2 minus position 1, position 3 minus position 2, position 4 minus position 3, etc.\nThe var() function calculates the sample variance for a vector of values. To calculate the population variance, adjust the result by a factor of \\((n-1)/n\\).\nThe sd() function calculates the sample standard deviation.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Stats IV</span>"
    ]
  },
  {
    "objectID": "DescriptiveIV.html#exercises",
    "href": "DescriptiveIV.html#exercises",
    "title": "4  Descriptive Stats IV",
    "section": "4.8 Exercises",
    "text": "4.8 Exercises\nThe following exercises will help you practice the measures of dispersion. In particular, the exercises work on:\n\nCalculating the range, MAD, variance, and the standard deviation.\nUsing R to calculate measures of dispersion.\nCalculating and using the Sharpe ratio to select investments.\n\nAnswers are provided below. Try not to peak until you have a formulated your own answer and double checked your work for any mistakes.\n\nExercise 1\nFor the following exercises, make your calculations by hand and verify results using R functions when possible. Make sure to calculate the deviations from the mean.\n\nUse the following observations to calculate the Range, MAD, Variance and Standard Deviation. Assume that the data below is the entire population.\n\n\n\n70\n68\n4\n98\n\n\n\n\n\n\nAnswer\n\nThe mean is \\(60\\), the Range is \\(94\\), the MAD is \\(28\\), the variance is \\(1186\\) and the variance is \\(34.44\\).\nStart by crating a vector to hold the values:\n\nEx1&lt;-c(70,68,4,98)\n\nThe range can be calculated by using the range() and diff() functions in R.\n\n(Range&lt;-diff(range(Ex1)))\n\n[1] 94\n\n\nNext, we can create a table by hand that captures the deviations from the mean. Let’s calculate the mean first:\n\n(Average1&lt;-mean(Ex1))\n\n[1] 60\n\n\nNow we can use the mean to fill out a table of deviations:\n\n\n\n\\(x_{i}\\)\n\\(x_{i}-\\bar{x}\\)\n\\((x_{i}-\\bar{x})^2\\)\n\\(|x_{i}-\\bar{x}|\\)\n\n\n\n\n70\n10\n100\n10\n\n\n68\n8\n64\n8\n\n\n4\n-56\n3136\n56\n\n\n98\n38\n1444\n38\n\n\n\nThe variance averages out the squared deviations \\((x_{i}-\\bar{x})^2\\), the MAD averages out the absolute deviations \\(|x_{i}-\\bar{x}|\\), and the standard deviation is the square root of the variance.\nLet’s verify the variance in R:\n\nSquaredDeviations1&lt;-(Ex1-Average1)^2\nAverageDeviations1&lt;-mean(SquaredDeviations1)\nvar(Ex1)*3/4\n\n[1] 1186\n\n\nNote that R calculates the sample variance. Hence, we must multiply the result by \\(3/4\\) to get the population variance. The standard deviation is just the square root of the variance:\n\nsqrt(AverageDeviations1)\n\n[1] 34.43835\n\n\nLastly, the MAD is calculated by averaging the absolute deviations \\(|x_{i}-\\bar{x}|\\).\n\nAbsoluteDeviations1&lt;-abs(Ex1-Average1)\nmean(AbsoluteDeviations1)\n\n[1] 28\n\n\n\n\nUse the following observations to calculate the Range, MAD, Variance and Standard Deviation. Assume that the data below is a sample from the population.\n\n\n\n-4\n0\n-6\n1\n-3\n0\n\n\n\n\n\n\nAnswer\n\nThe mean is \\(-2\\), Range is \\(7\\), the MAD is \\(2.33\\), the variance is \\(7.6\\) and the standard deviation is \\(2.76\\).\nHere is the table of deviations from the mean:\n\n\n\n\\(x_{i}\\)\n\\(x_{i}-\\bar{x}\\)\n\\((x_{i}-\\bar{x})^2\\)\n\\(|x_{i}-\\bar{x}|\\)\n\n\n\n\n-4\n-2\n4\n2\n\n\n0\n2\n4\n2\n\n\n-6\n-4\n16\n4\n\n\n1\n3\n9\n3\n\n\n-3\n-1\n1\n1\n\n\n0\n2\n4\n2\n\n\n\nWe can check the results in R. Let’s start with the variance:\n\nEx2&lt;-c(-4,0,-6,1,-3,0)\nvar(Ex2)\n\n[1] 7.6\n\n\nThe standard deviation can be found with the sd() function:\n\nsd(Ex2)\n\n[1] 2.75681\n\n\nThe MAD is given by:\n\n(MAD&lt;-mean(abs(Ex2-mean(Ex2))))\n\n[1] 2.333333\n\n\nLastly, the range:\n\ndiff(range(Ex2))\n\n[1] 7\n\n\n\n\n\nExercise 2\nYou will need the Stocks data set to answer this question. You can find this data at https://jagelves.github.io/Data/Stocks.csv The data is a sample of daily stock prices for ticker symbols TSLA (Tesla), VTI (S&P 500) and GBTC (Bitcoin).\n\nCalculate the standard deviations for each stock. Which stock had the lowest standard deviation?\n\n\n\nAnswer\n\nFor the sample taken, GBTC has the less variation. The standard deviation of GBTC is \\(9.43\\), which is less than \\(16.57\\) for VTI or \\(50.38\\) for TSLA.\nStart by loading the data set from the website. Since the file is in csv format, we will use the read.csv() function.\n\nStockPrices&lt;-read.csv(\"https://jagelves.github.io/Data/Stocks.csv\")\n\nLet’s start with the standard deviation of the Tesla stock. The standard deviation is given by:\n\nsd(StockPrices$TSLA)\n\n[1] 50.38092\n\n\nNext, let’s find the standard deviation for the S&P 500 or VTI. The standard deviation is given by:\n\nsd(StockPrices$VTI)\n\n[1] 16.5731\n\n\nFinally, let’s calculate the standard deviation for GBTC or Bitcoin.\n\nsd(StockPrices$GBTC)\n\n[1] 9.434213\n\n\n\n\nCalculate the MAD. Does your answer in 1. remain the same?\n\n\n\nAnswer\n\nThe answer is the same, since the MAD for GBTC is \\(8.46\\) which is lower than \\(14.27\\) for VTI or \\(41.67\\) for TSLA.\nTo calculate the MAD for TSLA we can use the following command:\n\n(MADTSLA&lt;-mean(abs(StockPrices$TSLA-mean(StockPrices$TSLA))))\n\n[1] 41.67163\n\n\nThe MAD for VTI is:\n\n(MADVTI&lt;-mean(abs(StockPrices$VTI-mean(StockPrices$VTI))))\n\n[1] 14.27169\n\n\nThe MAD for GBTC is:\n\n(MADGBTC&lt;-mean(abs(StockPrices$GBTC-mean(StockPrices$GBTC))))\n\n[1] 8.458029\n\n\n\n\nFinally, calculate the coefficient of variation. Any changes to your conclusions?\n\n\n\nAnswer\n\nBy considering the magnitudes of the stock prices, it seems like VTI is the less volatile stock. VTI has a CV of \\(0.08\\) which is lower than \\(0.44\\) for GBTC or \\(0.18\\) for TSLA. In fact, by CV Bitcoin seems to be the most risky asset.\nThe coefficients of variations are as follows. For TSLA the CV is:\n\n(CVTSLA&lt;-sd(StockPrices$TSLA)/mean(StockPrices$TSLA))\n\n[1] 0.1793755\n\n\nFor VTI the CV is:\n\n(CVVTI&lt;-sd(StockPrices$VTI)/mean(StockPrices$VTI))\n\n[1] 0.07970004\n\n\nFor GBTC we get:\n\n(CVGBTC&lt;-sd(StockPrices$GBTC)/mean(StockPrices$GBTC))\n\n[1] 0.4442497\n\n\n\n\n\nExercise 3\nInstall the ISLR2 package. You will need the Portfolio data set to answer this question. The data has 100 records of the returns of two stocks.\n\nCalculate the mean and standard deviation for each stock. Which investment has higher returns on average? Which investment is safest as measured by the standard deviation?\n\n\n\nAnswer\n\nThe best performing stock on average is stock X. It has an average return of \\(-0.078\\)% vs. \\(0.097\\)% for stock Y. The safest stock is stock X as well, since the standard deviation is \\(1.062\\) percentage points vs. \\(1.14\\) percentage points for stock Y.\nStart by loading the ISLR2 package:\n\nlibrary(ISLR2)\n\nNext, calculate the mean for stock X:\n\nmean(Portfolio$X)\n\n[1] -0.07713211\n\n\nand stock Y.\n\nmean(Portfolio$Y)\n\n[1] -0.09694472\n\n\nThen, calculate the standard deviation for stock X\n\nsd(Portfolio$X)\n\n[1] 1.062376\n\n\nand stock Y.\n\nsd(Portfolio$Y)\n\n[1] 1.143782\n\n\n\n\nUse a Risk Free rate of return of 3.5% to calculate the Sharpe ratio for each stock. Which stock would you recommend?\n\n\n\nAnswer\n\nThe Sharpe Ratio measures the excess return per unit of risk taken. Stock X has the better Sharpe Ratio. \\(-0.106\\) vs. \\(-0.115\\). Stock X is recommended since it provides a higher excess return per unit of risk taken.\nTo calculate Sharpe Ratios use both the average return, and the standard deviation. For stock X, the Sharpe Ratio is:\n\n(mean(Portfolio$X)-0.035)/sd(Portfolio$X)\n\n[1] -0.1055484\n\n\nThe Sharpe Ratio for stock Y:\n\n(mean(Portfolio$Y)-0.035)/sd(Portfolio$Y)\n\n[1] -0.1153583\n\n\n\n\nCalculate the average return for a portfolio that has 30% of stock X and 70% of stock Y. What is the standard deviation of the portfolio?\n\n\n\nAnswer\n\nThe portfolio has an average return of \\(-0.091\\) which is worse than stock X but better than stock Y. The standard deviation is \\(1.00\\). This is better than stock X and Y separately. The Sharpe ratio of \\(-0.091\\) is also better for the portfolio than for each stock individually.\nThe mean of the portfolio is given by:\n\n(mean_return=0.3*mean(Portfolio$X)+0.7*mean(Portfolio$Y))\n\n[1] -0.09100094\n\n\nThe covariance matrix is given by:\n\n(risk&lt;-cov(Portfolio))\n\n          X         Y\nX 1.1286424 0.6263583\nY 0.6263583 1.3082375\n\n\nUsing the matrix we can now calculate the standard deviation:\n\n(standard&lt;-sqrt(t(c(0.3,0.7)) %*% (risk %*% c(0.3,0.7))))\n\n         [,1]\n[1,] 1.002838\n\n\nFinally, the Sharpe ration for the portfolio is:\n\nmean_return/standard[1]\n\n[1] -0.09074338",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Stats IV</span>"
    ]
  },
  {
    "objectID": "DescriptiveV.html",
    "href": "DescriptiveV.html",
    "title": "5  Descriptive Stats V",
    "section": "",
    "text": "5.1 Quantiles and Percentiles\nThere are statistical measures that describe the shape and distribution of the data beyond simple measures of central location or dispersion. They provide a view of how data is spread out, where it concentrates, and how it deviates from what might be expected. The tools shown below will help you describe the data’s shape, symmetry, and anomalies.\nA quantile is a location within a set of ranked numbers (or distribution), below which a certain proportion, \\(q\\), of that set lie. If we instead express quantiles as a percentage, they are referred to as percentiles.\nEx: Imagine all your data points lined up from smallest to largest. If you say you’re looking at the 25th percentile, it means you’re finding the value below which 25% of your data falls. If you had 100 test scores, the 25th percentile would be the score where 25 students scored lower than that, and 75 scored higher or equal. It’s a way to see where a value stands in relation to the rest of the data in terms of percentage.\nTo calculate a percentile we follow the steps below:\nExample: Let’s use the IQ scores for a group of students to find the 25th percentile. \\(IQ=\\{80,100,110,75,130,90\\}\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Stats V</span>"
    ]
  },
  {
    "objectID": "DescriptiveV.html#.quartiles",
    "href": "DescriptiveV.html#.quartiles",
    "title": "5  Descriptive Stats V",
    "section": "",
    "text": "Sort the data in ascending order. Each point has now a location. The smallest number will be first, the second smallest number will be second, etc.\nCompute the location of the percentile desired using \\(L_{p}=\\frac{(n+1)P}{100}\\) where \\(L_{p}\\) is the location (in the sorted data) of the \\(P_{th}\\) percentile, and \\(P\\) is the percentile desired.\nThe data point at location \\(L_p\\), is the the \\(P_{th}\\) percentile.\n\n\n\nWe sort the data: \\(IQ_{sorted}=\\{75,80,90,100,110,130\\}\\)\nFind the location of the 25th percentile: \\(L_{25}=7 \\times 0.25=1.75\\). The 25th percentile is in the position 1.75 of the sorted data.\nRetrieve the 25th percentile: Since position 1 is 75 and position 2 is 80, the 25th percentile lies 0.75 of the way between position 1 and 2. Hence, the 25th percentile is \\(P_{25}=75+0.75(5)=78.75\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Stats V</span>"
    ]
  },
  {
    "objectID": "DescriptiveV.html#chevyshevs-theorem",
    "href": "DescriptiveV.html#chevyshevs-theorem",
    "title": "5  Descriptive Stats V",
    "section": "5.2 Chevyshev’s Theorem",
    "text": "5.2 Chevyshev’s Theorem\nChebyshev’s Theorem is an important theorem, as it helps you form an expectation of the proportion of data that must lie between a given standard deviation from the mean. This offers a baseline to understanding the range and distribution of your data, and aids in detecting outliers. Formally, Chevyshev’s Theorem states that regardless of the shape of the distribution, at least (\\(1-1/z^2\\))% of the data lies between \\(z\\) standard deviations from the mean.\nEx: For a given data set, we want to know at least how much of the data is between two standard deviations. Substituting 2 into Chevyshev’s formula yields $1-1/4=0.75. Hence, 75% of the data lies between two standard deviations from the mean.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Stats V</span>"
    ]
  },
  {
    "objectID": "DescriptiveV.html#the-empirical-rule",
    "href": "DescriptiveV.html#the-empirical-rule",
    "title": "5  Descriptive Stats V",
    "section": "5.3 The Empirical Rule",
    "text": "5.3 The Empirical Rule\nWhereas Chevyshev’s theorem holds for any data distribution, the empirical rule is a bit more precise when looking at “bell shaped” data. Formally, the Empirical Rule or (\\(68\\),\\(95\\),\\(99.7\\) rule) states that \\(68\\)%, \\(95\\)%, and \\(99.7\\)% of the data lies between \\(1\\), \\(2\\), and \\(3\\) standard deviations from the mean respectively. The rule requires that the data be bell shape (normally) distributed.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Stats V</span>"
    ]
  },
  {
    "objectID": "DescriptiveV.html#outliers-z-scores",
    "href": "DescriptiveV.html#outliers-z-scores",
    "title": "5  Descriptive Stats V",
    "section": "5.4 Outliers Z-Scores",
    "text": "5.4 Outliers Z-Scores\nGiven the boundaries set by both the empirical rule and Chevyshev’s theorem, we can classify points as being common (normal) and not common (outliers). Specifically, outliers are extreme deviations from the mean. They are values that are not “common” or rarely occurring. Since both the empirical rule and Chevyshev’s theorem state that a large proportion of the data is between three standard deviations, it would be uncommon to have a data point that is more that three standard deviations away from the mean.\nTo identify outliers we use a z-score, which is a measure of distance from the mean in units of standard deviations. It can be calculated for any data point in your variable by using the formula \\(z_{i}=\\frac{x_i-\\bar{x}}{s_x}\\). By definition, \\(z\\)-scores above \\(3\\) are suspected to be outliers.\nEx: On Jan 22, 2006 Kobe Bryant scored 81 points against the Toronto Raptors. He had averaged 30 point per game with a standard deviation of 4 points. If we calculate the z-score we get: \\(z_{81}=\\frac{81-30}{4}=12.5\\). This mean that 81 is 12.5 standard deviations away from the mean, making this value extremely rare.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Stats V</span>"
    ]
  },
  {
    "objectID": "DescriptiveV.html#skew",
    "href": "DescriptiveV.html#skew",
    "title": "5  Descriptive Stats V",
    "section": "5.5 Skew",
    "text": "5.5 Skew\nA measurement of skew, identifies asymmetry in the distribution of data. If most of the data leans towards one side, it’s skewed. If it leans to the left, it’s left-skewed or negatively skewed, meaning the tail on the left side is longer. If it leans to the right, it’s right-skewed or positively skewed, with a longer tail on the right. If the data is evenly distributed, it’s not skewed at all, it’s symmetric. To determine if the data is skewed, calculate the Pearson’s Coefficient of Skew. \\(Sk=\\frac{3(\\bar {x}- Median)}{s_x}\\). The distribution is skewed to the left if \\(Sk&lt;0\\), skewed to the right is \\(Sk&gt;0\\), and symmetric if \\(Sk=0\\).\nThe image below shows the different types of skew:\n\n\n\n\n\n\n\n\n\nEx: Assume that for a variable the mean is 10, the median is 8, and the standard deviation is 3. The pearson coefficient of skew is equal to \\(Sk=3(10-8)/3=2\\). Since the skew is positive, we expect the distribution to be right skewed.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Stats V</span>"
    ]
  },
  {
    "objectID": "DescriptiveV.html#five-point-summary",
    "href": "DescriptiveV.html#five-point-summary",
    "title": "5  Descriptive Stats V",
    "section": "5.6 Five Point Summary",
    "text": "5.6 Five Point Summary\nA popular way to summarize data is by calculating the minimum, first quartile, median, third quartile and maximum (five point summary). This gives us a good idea of how data is distributed. We can additionally inquire how the middle 50% of the data varies. Recall, that we can use a range to assess dispersion. The interquartile range (IQR) quantifies the dispersion of the middle 50% of the data. Formally, the IQR is the difference between the third quartile (75th percentile) and the first quartile (25th percentile).\nExample: Let’s use the IQ scores for a group of students once more. Recall that the data is given by \\(IQ=\\{80,100,110,75,130,90\\}\\). The minimum and the maximum are easily identified as \\(Max=130\\) and \\(Min=75\\). The first quartile (\\(P_{25}\\)) was calculated in 1.1 as 78.75. Using the same steps the third quartile (\\(P_{75}\\)) is 115. The median is the average between the third and fourth numbers \\(Median=190/2=95\\). The five point summary is given in the table below:\n\n\n\n\\(Min\\)\n\\(P_{25}\\)\n\\(Median\\)\n\\(P_{75}\\)\n\\(Max\\)\n\n\n\n\n75\n78.75\n95\n115\n130\n\n\n\nTo calculate the interquartile range we find the difference between the 75th and 25th percentiles. \\(IQR=115-78.75=36.25\\) which means that the middle 50% of the data has a range of 36.25.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Stats V</span>"
    ]
  },
  {
    "objectID": "DescriptiveV.html#outliers-iqr",
    "href": "DescriptiveV.html#outliers-iqr",
    "title": "5  Descriptive Stats V",
    "section": "5.7 Outliers IQR",
    "text": "5.7 Outliers IQR\nAn alternate way to identify outliers is by using the interquartile range. Specifically, we first calculate \\(Q_1-1.5(IQR)\\) and \\(Q_3+1.5(IQR)\\), where \\(Q_1\\) is the first quartile, \\(Q_3\\) is the third quartile, and \\(IQR\\) is the interquartile range. If the observation (\\(x_i\\)) is less than \\(Q_1-1.5(IQR)\\) or greater than \\(Q_3+1.5(IQR)\\), then it is considered an outlier.\nEx: Consider once more the IQ data. \\(IQ=\\{80,100,110,75,130,90\\}\\). The lower limit for on outlier is given by \\(LL=Q_1-1.5(IQR)=78.75-1.5(36.25)\\) or \\(LL=24.375\\). The upper limit is given by \\(UL=Q_3+1.5(IQR)=115+1.5(36.25)\\) or \\(UL=169.375\\). Any data point outside the range [24.375,169.375] is considered an outlier. In other words, 200 and 20 would be outliers, but 100 would not.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Stats V</span>"
    ]
  },
  {
    "objectID": "DescriptiveV.html#quantiles-and-quartiles-in-r",
    "href": "DescriptiveV.html#quantiles-and-quartiles-in-r",
    "title": "5  Descriptive Stats V",
    "section": "5.8 Quantiles and Quartiles in R",
    "text": "5.8 Quantiles and Quartiles in R\nR quickly calculates quantiles for a given variable (vector) using the quantile() function. Below we use the IQ example once more.\n\nIQ &lt;- c(80,100,110,75,130,90)\nquantile(IQ, type=6)\n\n    0%    25%    50%    75%   100% \n 75.00  78.75  95.00 115.00 130.00 \n\n\nYou will notice that an extra argument type has been been passed into the quantile function. Since, there are several ways to calculate quantiles, R allows you to identify which method you want to use. In sec 1.1 we explained method 6.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Stats V</span>"
    ]
  },
  {
    "objectID": "DescriptiveV.html#outliers-in-r",
    "href": "DescriptiveV.html#outliers-in-r",
    "title": "5  Descriptive Stats V",
    "section": "5.9 Outliers in R",
    "text": "5.9 Outliers in R\nTo identify outliers we can us the scale() function. We’ll consider the first five observations in the faithful data set.\n\nhead(scale(faithful),5)\n\n    eruptions    waiting\n1  0.09831763  0.5960248\n2 -1.47873278 -1.2428901\n3 -0.13561152  0.2282418\n4 -1.05555759 -0.6544374\n5  0.91575542  1.0373644\n\n\nThe data shown above are z-scores for the first five observations of the faithful data set. As you can see non of the observations are outliers, as they are all less that 3 standard deviations away from the mean.\nIf we want to filter all observations that are say 1.3 standard deviations away from the mean, we can use the following command from tidyverse:\n\nlibrary(tidyverse)\nfaithful %&gt;% mutate(z_eruptions=scale(eruptions)) %&gt;%\n                      filter(scale(eruptions)&gt;1.3)\n\n  eruptions waiting z_eruptions\n1     5.067      76    1.383614\n2     5.100      96    1.412526\n3     5.033      77    1.353825\n4     5.000      88    1.324912\n\n\nThis confirms that there are no outliers in the eruptions variable.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Stats V</span>"
    ]
  },
  {
    "objectID": "DescriptiveV.html#box-plots-in-r",
    "href": "DescriptiveV.html#box-plots-in-r",
    "title": "5  Descriptive Stats V",
    "section": "5.10 Box Plots in R",
    "text": "5.10 Box Plots in R\nA box plot is a graph that shows the five point summary, outliers (if any), and the distribution of data. It can be easily constructed using geom_boxplot() in R. Let’s use the eruptions variable once more.\n\nlibrary(ggthemes)\nfaithful %&gt;% \n  ggplot() +\n  geom_boxplot(aes(y=eruptions),\n               fill=\"lightgrey\", alpha=0.5,\n               col=\"black\", width=0.3) +\n  theme_clean() +\n  scale_x_continuous(breaks = NULL, limits=c(-1,1))+\n  ylim(limits=c(0,6))\n\n\n\n\n\n\n\n\nThe boxplot highlights the minimum just below 2, the maximum around 5, the first quartile just above 2, the median at 4, and the third quartile just above 4. Any outlier would be shown as a point beyond the whiskers of the box plot.\nWe can also overlay the data to the boxplot by using the geom_jitter() function. The data points allow us to see how the data is distributed. The code is below:\n\nfaithful %&gt;% \n  ggplot() +\n  geom_boxplot(aes(y=eruptions),\n               fill=\"lightgrey\", alpha=0.5,\n               col=\"black\", width=0.3) +\n  theme_clean() +\n  scale_x_continuous(breaks = NULL, limits=c(-1,1))+\n  ylim(limits=c(0,6)) + labs(x=\"\") +\n  geom_jitter(aes(y=eruptions,x=0.5), \n              width=0.15, pch=21,\n              alpha=0.4, bg=\"grey\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Stats V</span>"
    ]
  },
  {
    "objectID": "DescriptiveV.html#exercises",
    "href": "DescriptiveV.html#exercises",
    "title": "5  Descriptive Stats V",
    "section": "5.12 Exercises",
    "text": "5.12 Exercises\nThe following exercises will help you practice other statistical measures. In particular, the exercises work on:\n\nConstructing a five point summary and a boxplot.\nApplying Chebyshev’s Theorem.\nIdentifying skewness.\nIdentifying outliers.\n\nAnswers are provided below. Try not to peak until you have a formulated your own answer and double checked your work for any mistakes.\n\nExercise 1\nFor the following exercises, make your calculations by hand and verify results using R functions when possible.\n\nUse the following observations to calculate the minimum, the first, second and third quartiles, and the maximum. Are there any outliers? Find the IQR to answer the question.\n\n\n\n3\n10\n4\n1\n0\n30\n6\n\n\n\n\n\n\nAnswer\n\nThe minimum is \\(0\\), the first quartile is \\(2\\), second quartile is \\(4\\), third quartile is \\(8\\), and maximum is \\(30\\). \\(30\\) is an outlier since it is beyond \\(Q_{3}+1.5 \\times IQR\\).\nQuartiles are calculated using the percentile formula \\((n+1)P/100\\). The data set has seven numbers. The first quartile’s location is \\(8/4=2\\), the second quartile’s location is \\(8/2=4\\) and the third quartile’s location is \\(24/4=6\\). The values at these location, when data is organized in ascending order, are \\(1\\), \\(4\\), and \\(10\\).\nIn R we can get the five number summary by using the quantile() function. Since there are various rules that can be used to calculate percentiles, we specify type \\(6\\) to match our rules.\n\nEx1&lt;-c(3,10,4,1,0,30,6)\nquantile(Ex1,type = 6)\n\n  0%  25%  50%  75% 100% \n   0    1    4   10   30 \n\n\nThe interquartile range is needed to determine if there are any outliers. The \\(IQR\\) for this data set is \\(Q_{3}-Q_{1}=9\\). This reveals that \\(30\\) is and outlier, since \\(10+1.5 \\times 9=23.5\\). Everything beyond \\(23.5\\) is an outlier.\n\n\nConfirm your finding of an outlier by calculating the \\(z\\)-score. Is \\(30\\) an outlier when using a \\(z\\)-Score?\n\n\n\nAnswer\n\nIf we use the \\(z\\)-score instead we find that \\(30\\) is not an outlier since the \\(z\\)-score is \\(Z_{30}=2.15\\). This observation is only \\(2.15\\) standard deviations away from the mean.\nIn R we can make a quick calculation of the \\(z\\)-Score to confirm our results. The \\(z\\)-score is given by \\(Z_{i}=\\frac{x_{30}-\\mu}{\\sigma}\\).\n\n(Z30&lt;-(30-mean(Ex1))/sd(Ex1))\n\n[1] 2.148711\n\n\n\n\nUse Chebyshev’s theorem to determine what percent of the data falls between the \\(z\\)-score found in \\(2\\).\n\n\n\nAnswer\n\nChebyshev’s theorem states that \\(1-\\frac{1}{z_{2}}\\) of the data lies between \\(z\\) standard deviation from the mean.\nSubstituting the \\(z\\)-score found in 2. we get \\(78.34\\)% of the data lies between the standard deviation calculated. In R:\n\n1-1/(Z30)^2\n\n[1] 0.7834073\n\n\n\n\n\nExercise 2\nYou will need the Stocks data set to answer this question. You can find this data at https://jagelves.github.io/Data/Stocks.csv The data is a sample of daily stock prices for ticker symbols TSLA (Tesla), VTI (S&P 500) and GBTC (Bitcoin).\n\nConstruct a boxplot for Stock A. Is the data skewed or symmetric?\n\n\n\nAnswer\n\nThe data is skewed to the right.\nStart by loading the data set:\n\nStockPrices&lt;-read.csv(\"https://jagelves.github.io/Data/Stocks.csv\")\n\nTo construct the boxplot in R, use the boxplot() command.\n\nStockPrices %&gt;% \n  ggplot() +\n  geom_boxplot(aes(y=VTI),\n               fill=\"lightgrey\", alpha=0.5,\n               col=\"black\", width=0.3) +\n  theme_clean() +\n  scale_x_continuous(breaks = NULL, limits=c(-1,1))\n\n\n\n\n\n\n\n\nThe boxplot shows that there are no outliers. The data also looks like it has a slight skew to the right.\n\n\nCreate a histogram of the data. Include a vertical line for the mean and median. Explain how the mean and median indicates a skew in the data. Calculate the skewness statistic to confirm your result.\n\n\n\nAnswer\n\nThe mean is more sensitive to outliers than the median. Hence, when the data is skewed to the right we expect that the mean is larger than the median.\nLet’s construct a histogram in R to search for skewness.\n\nStockPrices %&gt;% ggplot() +\n  geom_histogram(aes(VTI), bins = 8,\n                 binwidth = 8,\n                 col=\"black\", bg=\"grey\",\n                 boundary=179) +\n  theme_clean() +\n  geom_vline(xintercept = mean(StockPrices$VTI), col=\"black\")+\n  geom_vline(xintercept = median(StockPrices$VTI), col=\"orange\")\n\n\n\n\n\n\n\n\nThe lines are close to each other but the mean is slighlty larger than the median. Let’s confirm with the skewness statistic \\(3(mean-median)/sd\\).\n\n(skew&lt;-3*(mean(StockPrices$VTI-median(StockPrices$VTI))/sd(StockPrices$VTI)))\n\n[1] 0.2856304\n\n\nThis indicates that there is a slight skew to the right of the data.\n\n\nUse a line chart to plot your data. Can you explain why the data has a skew?\n\n\n\nAnswer\n\nThe line chart indicates that the data has a downward trend in the early periods. This creates a few points that are large. In later periods the stock price stabilizes to lower levels.\n\nStockPrices %&gt;% ggplot() +\n  geom_line(aes(y=VTI, x=seq(1,length(VTI)))) + theme_clean() +\n  labs(x=\"Period\")\n\n\n\n\n\n\n\n\n\n\n\nExercise 3\nYou will need the mtcars data set to answer this question. This data set is part of R. You don’t need to download any files to access it.\n\nConstruct a boxplot for the hp variable. Write a command in R that retrieves the outlier. Which car is the outlier?\n\n\n\nAnswer\n\nThe outlier is the Masserati Bora. The horse power is \\(335\\).\nIn R we can construct a boxplot with the following command:\n\nmtcars %&gt;% \n  ggplot() +\n  geom_boxplot(aes(y=hp),\n               fill=\"lightgrey\", alpha=0.5,\n               col=\"black\", width=0.3,\n               outlier.colour = \"red\") +\n  theme_clean() +\n  scale_x_continuous(breaks = NULL, limits=c(-1,1))\n\n\n\n\n\n\n\n\nFrom the graph it seems like the outlier is beyond a horsepower of 275. Let’s write an R command to retrieve the car.\n\nmtcars %&gt;% filter(hp&gt;300)\n\n              mpg cyl disp  hp drat   wt qsec vs am gear carb\nMaserati Bora  15   8  301 335 3.54 3.57 14.6  0  1    5    8\n\n\nIt’s the Masserati Bora!\n\n\nCreate a histogram of the data. Is the data skewed? Include a vertical line for the mean and median. Calculate the skewness statistic to confirm your result.\n\n\n\nAnswer\n\nThe histogram looks skewed to the right. This is confirmed by the estimation of a Pearson coefficient fo skewness of \\(1.04\\).\nIn R we can construct a histogram with vertical lines for the mean and median with the following code:\n\nmtcars %&gt;% ggplot() +\n  geom_histogram(aes(hp), bins = 5,\n                 binwidth = 60,\n                 col=\"black\", bg=\"grey\",\n                 boundary=50) +\n  theme_clean() +\n  geom_vline(xintercept = mean(StockPrices$VTI), col=\"black\")+\n  geom_vline(xintercept = median(StockPrices$VTI), col=\"orange\")\n\n\n\n\n\n\n\n\nThe histogram looks skewed to the right. Pearson’s Coefficient of Skewness is:\n\n(SkewHP&lt;-3*(mean(mtcars$hp)-median(mtcars$hp))/sd(mtcars$hp))\n\n[1] 1.036458\n\n\n\n\nTransform the data by taking a natural logarithm. Specifically, create a new variable called Loghp. Repeat the procedure in 2. Is the skew still there?\n\n\n\nAnswer\n\nThe skew is still there, but the distribution now look more symmetrical and the Skew coefficient has decreased to \\(0.44\\).\nIn R we can create an new variable that captures the log transformation. The log() function takes the natural logarithm of a number or vector.\n\nLogHP&lt;-log(mtcars$hp)\n\nLet’s use this new variable to create our histogram:\n\nggplot() +\n  geom_histogram(aes(LogHP), bins = 5,\n                 col=\"black\", bg=\"grey\") +\n  theme_clean() +\n  geom_vline(xintercept = mean(LogHP), col=\"black\")+\n  geom_vline(xintercept = median(LogHP), col=\"orange\")\n\n\n\n\n\n\n\n\nThe mean and the variance now look closer together. The tail of the distribution (skew) now also looks diminished. The Skewness coefficient has decreased significantly:\n\n(SkewLogHP&lt;-3*(mean(LogHP)-median(LogHP))/sd(LogHP))\n\n[1] 0.4402212",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Stats V</span>"
    ]
  },
  {
    "objectID": "RegressionI.html",
    "href": "RegressionI.html",
    "title": "6  Regression I",
    "section": "",
    "text": "6.1 The Covariance\nMeasures of association are essential tools in business statistics for analyzing relationships between variables. They help determine whether changes in one variable are linked to changes in another, providing valuable insights into patterns and dependencies. Understanding these relationships is critical for making informed decisions. By quantifying the strength and direction of associations, businesses can better interpret data, optimize strategies, and drive meaningful outcomes. Below we study important measures of association.\nThe covariance is a measure that determines the direction of the relationship between two variables. It is calculated by: \\[s_{xy}=\\frac {\\sum(x_i-\\bar{x})(y_i-\\bar{y})}{n-1}\\] The result of the covariance indicates the relationship between the two variables. If \\(s_{xy}&gt;0\\) there is a direct relationship, if \\(s_{xy}&lt;0\\) there is an inverse relationship, and if \\(s_{xy}=0\\) there is no relationship.\nExample: Let’s consider the following data that captures the price of stocks (SPY) and bonds (BND):\nThe idea behind calculating a covariance is to determine whether there is a relationship between the two variables. Let’s start by calculating the formula. We can do this by finding deviations from the mean for both SPY and BND. The table below shows the deviations from the mean for each variable in columns 3 and 4.\nColumn 5 calculates the product between column 3 and column 4. The covariance is simply the average of the numbers of column 5 by using the degrees of freedom \\(n-1\\). Hence, \\(s_{xy}=-71\\). Since \\(s_{xy}\\) is negative we can establish that there is an inverse relationship between SPY and BND.\nIntuitively, the covariance checks whether on average the products of the deviations from the mean is positive or negative. The image below explains the intuition.\nThe image plots the SPY and BDN in a scatter plot. You can see that there is an inverse relationship. The vertical and horizontal lines represents the mean of SPY and the mean of BND, respectively. The red numbers are the deviations from the mean (you can compare with the table). So whenever x lies below the mean and y lies above its mean, the product is negative. This happens as well when x lies above the mean and y underneath its mean. As a result, the points are aligned so that an inverse relationship is reflected. You’ll notice that the product of the deviations in these cases is negative, and ultimately measured with the covariance.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regression I</span>"
    ]
  },
  {
    "objectID": "RegressionI.html#the-correlation",
    "href": "RegressionI.html#the-correlation",
    "title": "6  Regression I",
    "section": "6.2 The Correlation",
    "text": "6.2 The Correlation\nThe correlation measures the strength of the linear relationship between two variables. It is calculated by \\[r= \\frac {s_{xy}}{s_x s_y}\\] The correlation coefficient is between \\([-1,1]\\). When the correlation coefficient is \\(1\\) (\\(-1\\)), there is a perfect direct (inverse) relationship between the two variables.\n\n\n\n\n\n\n\n\n\nThe image above shows three examples of correlation coefficients. When the correlation coefficient is -0.4 or 0.4 the direction of the relationship depends on the sign of the coefficient. The magnitude of 0.4 indicates that the strength of the relationship is moderate (i.e., a general linear pattern is observed but many points do not lie on the trend line). When the correlation coefficient is zero, there is no linear pattern in the relationship. When the absolute value of the correlation is 1, then there is a perfect linear relationship between the two variables.\nEx: Consider the SPY and BND data. The standard deviation of SPY is \\(1.58\\) and that of BDN is \\(50.37\\). Substituting into the correlation formula we get a correlation coefficient of \\(r= \\frac {-71}{1.58 \\times 50.37}=-0.89\\). This indicates that the relationship between SPY and BND is inverse and strong.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regression I</span>"
    ]
  },
  {
    "objectID": "RegressionI.html#the-coefficient-of-determination-r2",
    "href": "RegressionI.html#the-coefficient-of-determination-r2",
    "title": "6  Regression I",
    "section": "6.3 The Coefficient of Determination (\\(R^2\\))",
    "text": "6.3 The Coefficient of Determination (\\(R^2\\))\nThe coefficient of determination or \\(R^2\\), measures the percent of variation in \\(y\\) explained by variations in \\(x\\). It is calculated by:\n\\[R^2=r^2\\] The number that we get from the \\(R^2\\) tells us how well a variable (x) explains the variation in the another variable (y). The number could be anywhere from zero (indicating that the two variables are unrelated), to one (indicating that one variable explains entirely the variation of the other variable).\nEx: Consider once more the SPY and BND example. The correlation coefficient is given by \\(r=-0.89\\). The \\(R^2=(-0.89)^2=0.79\\). This indicates that about 80% of the variation in the BND can be explained by the changes in SPY. Hence, these two variables are closely related.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regression I</span>"
    ]
  },
  {
    "objectID": "RegressionI.html#measures-of-association-in-r",
    "href": "RegressionI.html#measures-of-association-in-r",
    "title": "6  Regression I",
    "section": "6.4 Measures of Association in R",
    "text": "6.4 Measures of Association in R\nR makes it very convenient to retrieve measures of association. Let’s get the data from SPY and BND example into R:\n\nlibrary(tidyverse)\nlibrary(ggthemes)\n\ndata&lt;- tibble(SPY=c(87,86,84,85,83), BND=c(316,380,416,430,440))\n\nNow we can retrieve the covariance by using the cov() command in R. Below is the code:\n\ncov(data$SPY,data$BND)\n\n[1] -71\n\n\nWe confirm that the covariance is -71 and that there is an inverse relationship between the variables. To verify the correlation coefficient, we use the code below:\n\ncor(data$SPY,data$BND)\n\n[1] -0.891549\n\n\nThe correlation coefficient of -0.89 indicates a strong inverse linear relationship between the two variables. Lastly, the coefficient of determination is calculated below:\n\ncor(data$SPY,data$BND)^2\n\n[1] 0.7948597\n\n\nIt seems that about 80% of the variation in the price of bonds (BND) is explained by the variation in the price of stocks (SPY). We can create a visual of the relationship between the two variables by using the geom_point() function in R.\n\ndata %&gt;% ggplot() +\n  geom_point(aes(y=SPY,x=BND), col=\"black\",\n             cex=2, bg=\"blue\", alpha=0.5, pch=21) +\n  theme_clean()\n\n\n\n\n\n\n\n\nThe visualization above is called a scatter plot. A scatter plot displays pairs of [\\(x\\),\\(y\\)] as points on the Cartesian plane. The plot acts as a visual aid to determine the relationship between two variables. We can see that the points are inversely related to each other.\nBelow is a list of the functions used:\n\nTo calculate the covariance use the cov() function. The input must be two vectors (variables).\nThe correlation coefficient is calculated with the cor() function. The input must be two vectors (variables).\nThe geom_point() function will create scatter plots. Make sure you include two variables in the aes() function. The argument cex increases the size of the points, pch changes the point character, bg selects the color, and alpha adjusts the transparency of the background color (bg).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regression I</span>"
    ]
  },
  {
    "objectID": "RegressionI.html#exercises",
    "href": "RegressionI.html#exercises",
    "title": "6  Regression I",
    "section": "6.5 Exercises",
    "text": "6.5 Exercises\nThe following exercises will help you understand statistical measures that establish the relationship between two variables. In particular, the exercises work on:\n\nCalculating covariance and correlation.\nUsing R to plot scatter diagrams.\nCalculating the coefficient of determination.\n\nAnswers are provided below. Try not to peak until you have a formulated your own answer and double checked your work for any mistakes.\n\nExercise 1\nFor the following exercises, make your calculations by hand and verify results using R functions when possible.\n\nConsider the data below. Calculate the covariance and correlation coefficient by finding deviations from the mean. Use R to verify your result. Is there a direct or inverse relationship between the two variables? How strong is the relationship?\n\n\n\n\nx\n20\n21\n15\n18\n25\n\n\n\n\ny\n17\n19\n12\n13\n22\n\n\n\n\n\nAnswer\n\nThe covariance is \\(14.9\\) and the correlation is \\(0.96\\). The results indicate that there is a strong direct relationship between the two variables.\nLet’s start by finding the deviations from the mean for the x variable in R.\n\nx&lt;-c(20,21,15,18,25)\n(devx&lt;-x-mean(x))\n\n[1]  0.2  1.2 -4.8 -1.8  5.2\n\n\nWe will do the same with y:\n\ny&lt;-c(17,19,12,13,22)\n(devy&lt;-y-mean(y))\n\n[1]  0.4  2.4 -4.6 -3.6  5.4\n\n\nNote that when the deviations in x are negative (positive), they are also negative (positive) in y. This is indicative of a direct relationship between the two variables. The covariance is given by:\n\n(Ex1Cov&lt;-sum(devx*devy)/(length(devx)-1))\n\n[1] 14.9\n\n\nWe can verify this by using cov() function in R.\n\ncov(x,y)\n\n[1] 14.9\n\n\nThe correlation coefficient is found by dividing the covariance over the product of standard deviations. In R:\n\n(Ex1Cor&lt;-Ex1Cov/(sd(x)*sd(y)))\n\n[1] 0.9678386\n\n\nWe can once more verify the result in R with the built in function cor().\n\ncor(x,y)\n\n[1] 0.9678386\n\n\n\n\nConsider the data below. Calculate the covariance and correlation coefficient by finding deviations from the mean. Use R to verify your result. Is there a direct or inverse relationship between the two variables? How strong is the relationship?\n\n\n\n\nw\n19\n16\n14\n11\n18\n\n\n\n\nz\n17\n20\n20\n16\n18\n\n\n\n\n\nAnswer\n\nThe covariance is \\(0.85\\) and the correlation is \\(0.148\\). The results indicate that there is a very weak direct relationship between the two variables. They might be unrelated.\nLet’s start with w and finding the deviations from the mean in R.\n\nw&lt;-c(19,16,14,11,18)\n(devw&lt;-w-mean(w))\n\n[1]  3.4  0.4 -1.6 -4.6  2.4\n\n\nWe will do the same with z:\n\nz&lt;-c(17,20,20,16,18)\n(devz&lt;-z-mean(z))\n\n[1] -1.2  1.8  1.8 -2.2 -0.2\n\n\nThe covariance is given by:\n\n(Ex2Cov&lt;-sum(devw*devz)/(length(devz)-1))\n\n[1] 0.85\n\n\nWe can verify this with the cov() function in R.\n\ncov(w,z)\n\n[1] 0.85\n\n\nThe correlation coefficient is found by dividing the covariance over the product of standard deviations. In R:\n\n(Ex2Cor&lt;-Ex2Cov/(sd(z)*sd(w)))\n\n[1] 0.1480558\n\n\nWe can once more verify the result in R with the built in function cor().\n\ncor(w,z)\n\n[1] 0.1480558\n\n\n\n\n\nExercise 2\nYou will need the mtcars data set to answer this question. This data set is part of R. You don’t need to download any files to access it.\n\nCalculate the correlation coefficient between hp and mpg. Explain the results. Specifically, the direction of the relationship and the strength given the context of the problem.\n\n\n\nAnswer\n\nThe correlation coefficient is \\(-0.78\\). This is indicative of a moderately strong inverse relationship between mpg and mp.\nIn R we can easily calculate the correlation coefficient with the cor() function.\n\ncor(mtcars$mpg,mtcars$hp)\n\n[1] -0.7761684\n\n\n\n\nCreate a scatter diagram of the two variables. Is the scatter diagram what you expected after you calculated the correlation coefficient?\n\n\n\nAnswer\n\nThe scatter diagram is downward sloping. Most points are close to the trend line. It is what was expected from a correlation coefficient of \\(-0.78\\).\n\nlibrary(tidyverse)\nlibrary(ggthemes)\nmtcars %&gt;% ggplot() + \n  geom_point(aes(y=mpg,x=hp), col=\"black\",\n             bg=\"grey\", pch=21) +\n  geom_smooth(aes(y=mpg,x=hp), formula=y~x,\n              method=\"lm\", se=F) +\n  theme_clean()\n\n\n\n\n\n\n\n\n\n\nCalculate the coefficient of determination. How close is it to one? What else could be explaining the variation in the mpg? Let your dependent variable be mpg.\n\n\n\nAnswer\n\nThe coefficient of determination is \\(0.6\\). This value is not very close to one. This is expected since miles per gallon can also vary because of the cars weight, and fuel efficiency. It makes sense that the hp only explains \\(60\\)% of the total variation.\nIn R we can calculate the coefficient of determination by squaring the correlation coefficient.\n\ncor(mtcars$mpg,mtcars$hp)^2\n\n[1] 0.6024373\n\n\n\n\n\nExercise 3\nYou will need the College data set to answer this question. You can find this data set here: https://jagelves.github.io/Data/College.csv\n\nCreate a scatter diagram between GRAD_DEBT_MDN (Median Debt) and MD_EARN_WNE_P10 (Median Earnings). What type of relationship do you observe between the variables?\n\n\n\nAnswer\n\nIt seems like there is a direct relationship between both variables. The more debt you take, the higher the salary.\nStart by loading the data. We’ll use the read_csv() function:\n\nlibrary(tidyverse)\nCollege&lt;-read_csv(\"https://jagelves.github.io/Data/College.csv\")\n\nThe two variables of interest are GRAD_DEBT_MDN and MD_EARN_WNE_P10. The following code creates the scatter plot:\n\nCollege %&gt;% ggplot() + \n  geom_point(aes(y=GRAD_DEBT_MDN,x=MD_EARN_WNE_P10), col=\"black\",\n             bg=\"grey\", pch=21) +\n  geom_smooth(aes(y=GRAD_DEBT_MDN,x=MD_EARN_WNE_P10), formula=y~x,\n              method=\"lm\", se=F) +\n  theme_clean()\n\n\n\n\n\n\n\n\n\n\nCalculate the correlation coefficient and the coefficient of determination. According to the data, are higher debts correlated with higher earnings?\n\n\n\nAnswer\n\nThe correlation coefficient shows a moderate direct relationship between earnings and debt \\(0.46\\). The coefficient of determination indicates that only \\(21\\)% of the variation in earnings can be explained by debt.\nIn R we can start with the correlation coefficient:\n\n(Correlation&lt;-cor(College$GRAD_DEBT_MDN,\n                  College$MD_EARN_WNE_P10,\"complete.obs\"))\n\n[1] 0.4615361\n\n\nWe can simply square the correlation to obtain the coefficient of determination:\n\nCorrelation^2\n\n[1] 0.2130155",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regression I</span>"
    ]
  },
  {
    "objectID": "RegressionII.html",
    "href": "RegressionII.html",
    "title": "7  Regression II",
    "section": "",
    "text": "7.1 The Regression Line\nQuantifying relationships between variables is an important skill in business analytics. The regression line, representing the best linear fit between two variables, enables businesses to make predictions, identify trends, and optimize strategies using data-driven insights. Understanding how to calculate and interpret a regression line provides the foundation for analyzing business relationships, such as the effect of advertising on sales or customer satisfaction on revenue. Below, we delve into generating and analyzing a regression line.\nThe regression line is calculated to minimize the average distance (or errors) between the line and the observed data points. It is defined by two key components: a slope (\\(\\beta\\)) and an intercept (\\(\\alpha\\)). Mathematically, the regression line is expressed as: \\[\\hat{y_i}=\\hat{\\alpha}+\\hat{\\beta}x_i\\]\nwhere \\(\\hat{y_i}\\) are the predicted values of \\(y\\) given the \\(x\\)’s. The slope determines the steepness of the line. The estimate quantifies how much a unit increase in \\(x\\) changes \\(y\\). We can easily calculate the slope by using the covariance between \\(y\\) and \\(x\\), and the variance of \\(x\\). Mathematically we have:\n\\[\\hat{\\beta}= \\frac {s_{xy}}{s_{x}^2}\\]\nThe intercept determines where the line crosses the \\(y\\) axis. In other words it returns the value of \\(y\\) when \\(x\\) is zero. Once we have the slope of the regression line we can estimate the intercept by: \\[\\hat{\\alpha}=\\bar{y}-\\hat{\\beta}\\bar{x}\\]\nExample: Let’s examine a data set on Price and Advertisement. In general, one expects that when a company advertises, it can convince consumers to pay more for their product. Below is the data used to calculate the regression line, followed by its interpretation.\nThe data shows a clear relationship between advertisement and price: when advertisement spending is high, price also tends to be high. This suggests a direct relationship between the two variables. In a previous chapter, we learned to quantify such relationships using covariance and the correlation coefficient.\nIn this section, we aim to answer two key questions:\nThe regression line allows us to answer these questions by quantifying the relationship between the two variables. Specifically, if we can estimate the regression line, as shown below, we can answer these questions.\nLet’s start by calculating the slope of the regression line, as this will help us answer the effectiveness question. The slope measures how much the price increases for every additional dollar spent on advertisement. It is calculated using the formula: \\(\\hat{\\beta}= \\frac {s_{xy}}{s_{x}^2}\\). Given that the covariance is \\(3.67\\) and the variance of \\(x\\) is \\(1.67\\), the slope of the regression line is \\(\\hat{\\beta}= \\frac{3.67}{1.67}=2.2\\). This tells us that for every additional dollar spent on advertisement, price increases by \\(2.2\\) on average. Thus, we have answered the effectiveness question.\nNext, we calculate the intercept to complete the regression line and answer the prediction question. The intercept represents the predicted price when advertisement spending is zero. It is calculated as: \\(\\hat{\\alpha}=\\bar{y}-\\hat{\\beta}\\bar{x}\\). Since the mean of advertisement is \\(2.5\\) and the mean of price is \\(7\\), the intercept is \\(\\hat{\\alpha}=7-2.2(2.5)=1.5\\). This means that if we do not advertise, the predicted price is 1.5.\nThe regression line can be completed by using the intercept and slope that we have estimated above. In particular, the regression line is \\(\\hat{y_i}=1.5+2.2x_i\\). With this equation we can now establish that if we had a budget of \\(6\\) for advertisement, our predicted price would be \\(\\hat{y_i}=1.5+2.2(6)=14.7\\). This answers our prediction question.\nIn conclusion regression line has allowed us to answer both questions: 1. The effectiveness of advertisement: For every dollar spent on advertisment, the price increases by \\(2.2\\). 2. The predicted price: With an advertisement budget of \\(6\\), the predicted price is \\(14.7\\).\nThis demonstrates the value of regression analysis in quantifying relationships and making informed predictions.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regression II</span>"
    ]
  },
  {
    "objectID": "RegressionII.html#the-regression-line",
    "href": "RegressionII.html#the-regression-line",
    "title": "7  Regression II",
    "section": "",
    "text": "Advertisement (x)\nPrice (y)\n\n\n\n\n2\n7\n\n\n1\n3\n\n\n3\n8\n\n\n4\n10\n\n\n\n\n\n\nEffectiveness: How much can we increase the price for every additional dollar spent on advertisement?\nPrediction: What is the predicted price if we have a budget of 6 for advertisement?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regression II</span>"
    ]
  },
  {
    "objectID": "RegressionII.html#measures-of-goodness-of-fit",
    "href": "RegressionII.html#measures-of-goodness-of-fit",
    "title": "7  Regression II",
    "section": "7.2 Measures of Goodness of Fit",
    "text": "7.2 Measures of Goodness of Fit\nWhen analyzing the effectiveness of a regression model, it is crucial to assess how well the model fits the data. This is where measures of goodness of fit come into play. Below we revisit the coefficient of determination or \\(R^2\\).\n\nCoefficient of Determination\nThe coefficient of determination or \\(R^2\\) is the percent of the variation in \\(y\\) that is explained by changes in \\(x\\). The higher the \\(R^2\\) the better the explanatory power of the model. The \\(R^2\\) is always between [0,1]. To calculate use \\(R^2=SSR/SST\\). Below you can find how to calculate each component of the \\(R^2\\).\n\n\\(SSR\\) (Sum of Squares due to Regression) is the part of the variation in \\(y\\) explained by the model. Mathematically, \\(SSR=\\sum{(\\hat{y_i}-\\bar{y})^2}\\).\n\\(SSE\\) (Sum of Squares due to Error) is the part of the variation in \\(y\\) that is unexplained by the model. Mathematically, \\(SSE=\\sum{(y_i-\\hat{y_i})^2}\\).\n\\(SST\\) (Sum of Squares Total) is the total variation of \\(y\\) with respect to the mean. Mathematically, \\(SST=\\sum{(y_i-\\bar{y})^2}\\).\nNote that \\(SST=SSR+SSE\\).\n\nExample: Let’s consider data on the Weight (\\(y\\)) and Exercise (\\(x\\)) of a particular person.\n\n\n\nWeight (y)\nExercise (x)\n\n\n\n\n165\n45\n\n\n170\n10\n\n\n168\n25\n\n\n164\n30\n\n\n165\n40\n\n\n\nNow, if we were to predict the weight of this person, we could consider just using the mean value (i.e., \\(166.4\\)). If this were our guess, then the mistakes (errors) we would have made with this prediction are quantified by the \\(SST=25.2\\). Below, you can see these mistakes visually.\n\n\n\n\n\n\n\n\n\nFor example, in day \\(1\\) the mean prediction is \\(166.4\\), but the actual measure is \\(165\\). We have made a mistake of \\(-1.4\\) by using the mean as our predicted weight. To get a sense of the overall mistake we have made, we add up all of the squared errors made by using the mean as a prediction. Hence, \\(SST= (-1.4)^2 + (3.6)^2 + (1.6)^2 + (-2.4)^2 + (-1.4)^2= 25.2\\).\nThe idea behind regression is that another variable related to weight can help us make a better prediction. Generally speaking, you burn calories if you exercise, so your weight should be lower when exercising. Hence, adding information on how many minutes a person exercises in a day should allow us to predict someone’s weight better. The \\(SSE\\) quantifies the mistakes made by a prediction generated from regression. To understand this, let’s start by looking at the regression line for Weight and Exercise.\n\n\n\n\n\n\n\n\n\nThe image highlights how the regression line can reduce the errors in our prediction. Consider the point with \\(y\\) coordinate \\(168\\). The error using the mean is \\(1.6\\) (the green and yellow lines). However, if we update our prediction using the regression line, the mistake is reduced to \\(168-167.17=0.83\\) (the green line). Squaring all these deviations from the line and adding them gives us \\(SSE=0.81+0.29+0.69+5.76+0.02=7.57\\). Note how using the regression line has helped us reduce the errors in our prediction.\nIf regression has helped us make smaller mistakes, it has also helped us explain more of the variation in Weight (\\(y\\)). In the graph, the gap between the mean and the \\(168\\) point has been reduced by the amount highlighted in yellow (\\(0.77\\)). If we square and add up all of the “improvements,” we get \\(SSR=5.29+9.4+0.59+0+2.35=17.63\\). In sum, the regression line has closed the gap of the prediction errors made by using the mean by \\(R^2=\\frac{SSR}{SST} \\approx 0.7\\) or 70%.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regression II</span>"
    ]
  },
  {
    "objectID": "RegressionII.html#exercises",
    "href": "RegressionII.html#exercises",
    "title": "7  Regression II",
    "section": "7.5 Exercises",
    "text": "7.5 Exercises\nThe following exercises will help you get practice on Regression Line estimation and interpretation. In particular, the exercises work on:\n\nEstimating the slope and intercept.\nCalculating measures of goodness of fit.\nPrediction using the regression line.\n\nAnswers are provided below. Try not to peak until you have a formulated your own answer and double checked your work for any mistakes.\n\nExercise 1\nFor the following exercises, make your calculations by hand and verify results using R functions when possible.\n\nConsider the data below. Calculate the deviations from the mean for each variable and use the results to estimate the regression line. Use R to verify your result. On average by how much does y increase per unit increase of x?\n\n\n\n\nx\n20\n21\n15\n18\n25\n\n\n\n\ny\n17\n19\n12\n13\n22\n\n\n\n\n\nAnswer\n\nThe regression lines is \\(\\hat{y}=-4.93+1.09x\\). For each unit increase in x, y increases on average \\(1.09\\).\nStart by generating the deviations from the mean for each variable. For x the deviations are:\n\nx&lt;-c(20,21,15,18,25)\n(devx&lt;-x-mean(x))\n\n[1]  0.2  1.2 -4.8 -1.8  5.2\n\n\nNext, find the deviations for y:\n\ny&lt;-c(17,19,12,13,22)\n(devy&lt;-y-mean(y))\n\n[1]  0.4  2.4 -4.6 -3.6  5.4\n\n\nFor the slope we need to find the deviation squared of the x’s. This can easily be done in R:\n\n(devx2&lt;-devx^2)\n\n[1]  0.04  1.44 23.04  3.24 27.04\n\n\nThe slope is calculated by \\(\\frac{\\sum_{i=i}^{n} (x_{i}-\\bar{x})(y_{i}-\\bar{y})}{\\sum_{i=i}^{n} (x_{i}-\\bar{x})^2}\\). In R we can just find the ratio between the summations of (devx)(devy) and devx2.\n\n(slope&lt;-sum(devx*devy)/sum(devx2))\n\n[1] 1.087591\n\n\nThe intercept is given by \\(\\bar{y}-\\beta(\\bar{x})\\). In R we find that the intercept is equal to:\n\n(intercept&lt;-mean(y)-slope*mean(x))\n\n[1] -4.934307\n\n\nOur results can be easily verified by using the lm() and coef() functions in R.\n\nfitEx1&lt;-lm(y~x)\ncoef(fitEx1)\n\n(Intercept)           x \n  -4.934307    1.087591 \n\n\n\n\nCalculate SST, SSR, and SSE. Confirm your results in R. What is the \\(R^2\\)? What is the Standard Error estimate? Is the regression line a good fit for the data?\n\n\n\nAnswer\n\nSST is \\(69.2\\), SSR is \\(64.82\\) and SSE is \\(4.38\\) (note that \\(SSR+SSE=SST\\)). The \\(R^2\\) is just \\(\\frac{SSR}{SST}=0.94\\) and the Standard Error estimate is \\(1.21\\). They both indicate a great fit of the regression line to the data.\nLet’s start by calculating the SST. This is just \\(\\sum{(y_{i}-\\bar{y})^2}\\).\n\n(SST&lt;-sum((y-mean(y))^2))\n\n[1] 69.2\n\n\nNext, we can calculate SSR. This is calculated by the following formula \\(\\sum{(\\hat{y_{i}}-\\bar{y})^2}\\). To obtain the predicted values in R, we can use the output of the lm() function. Recall our fitEx1 object created in Exercise 1. It has fitted.values included:\n\n(SSR&lt;-sum((fitEx1$fitted.values-mean(y))^2))\n\n[1] 64.82044\n\n\nThe ratio of SSR to SST is the \\(R^2\\):\n\n(R2&lt;-SSR/SST)\n\n[1] 0.9367115\n\n\nFinally, let’s calculate SSE \\(\\sum{(y_{i}-\\hat{y_{i}})^2}\\):\n\n(SSE&lt;-sum((y-fitEx1$fitted.values)^2))\n\n[1] 4.379562\n\n\nWith the SSE we can calculate the Standard Error estimate:\n\nsqrt(SSE/3)\n\n[1] 1.208244\n\n\nWe can confirm these results using the summary() function.\n\nsummary(fitEx1)\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n      1       2       3       4       5 \n 0.1825  1.0949  0.6204 -1.6423 -0.2555 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  -4.9343     3.2766  -1.506  0.22916   \nx             1.0876     0.1632   6.663  0.00689 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.208 on 3 degrees of freedom\nMultiple R-squared:  0.9367,    Adjusted R-squared:  0.9156 \nF-statistic:  44.4 on 1 and 3 DF,  p-value: 0.00689\n\n\n\n\nAssume that x is observed to be 32, what is your prediction of y? How confident are you in this prediction?\n\n\n\nAnswer\n\nIf \\(x=32\\) then \\(\\hat{y}=29.87\\). The regression is a good fit, so we can feel good about our prediction. However, we would be concerned about the sample size of the data.\nIn R we can obtain a prediction by using the predict() function. This function requires a data frame as an input for new data.\n\npredict(fitEx1, newdata = data.frame(x=c(32)))\n\n       1 \n29.86861 \n\n\n\n\n\nExercise 2\nYou will need the Education data set to answer this question. You can find the data set at https://jagelves.github.io/Data/Education.csv . The data shows the years of education (Education), and annual salary in thousands (Salary) for a sample of \\(100\\) people.\n\nEstimate the regression line using R. By how much does an extra year of education increase the annual salary on average? What is the salary of someone without any education?\n\n\n\nAnswer\n\nAn extra year of education increases the annual salary about \\(5,300\\) dollars (slope). A person that has no education would be expected to earn \\(17,2582\\) dollars (intercept).\nStart by loading the data in R:\n\nlibrary(tidyverse)\nEducation&lt;-read_csv(\"https://jagelves.github.io/Data/Education.csv\")\n\nNext, let’s use the lm() function to estimate the regression line and obtain the coefficients:\n\nfitEducation&lt;-lm(Salary~Education, data = Education)\ncoefficients(fitEducation)\n\n(Intercept)   Education \n  17.258190    5.301149 \n\n\n\n\nConfirm that the regression line is a good fit for the data. What is the estimated salary of a person with \\(16\\) years of education?\n\n\n\nAnswer\n\nThe \\(R^2\\) is \\(0.668\\) and the standard error is \\(21\\). The line is a moderately good fit. If someone has \\(16\\) years of experience, the regression line would predict a salary of \\(102,000\\) dollars.\nLet’s get the \\(R^2\\) and the Standard Error estimate by using the summary() function and fitEx1 object.\n\nsummary(fitEducation)\n\n\nCall:\nlm(formula = Salary ~ Education, data = Education)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-62.177  -9.548   1.988  15.330  45.444 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  17.2582     4.0768   4.233  5.2e-05 ***\nEducation     5.3011     0.3751  14.134  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.98 on 98 degrees of freedom\nMultiple R-squared:  0.6709,    Adjusted R-squared:  0.6675 \nF-statistic: 199.8 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nLastly, let’s use the regression line to predict the salary for someone who has \\(16\\) years of education.\n\npredict(fitEducation, newdata = data.frame(Education=c(16)))\n\n       1 \n102.0766 \n\n\n\n\n\nExercise 3\nYou will need the FoodSpend data set to answer this question. You can find this data set at https://jagelves.github.io/Data/FoodSpend.csv .\n\nOmit any NA’s that the data has. Create a dummy variable that is equal to \\(1\\) if an individual owns a home and \\(0\\) if the individual doesn’t. Find the mean of your dummy variable. What proportion of the sample owns a home?\n\n\n\nAnswer\n\nApproximately, \\(36\\)% of the sample owns a home.\nStart by loading the data into R and removing all NA’s:\n\nSpend&lt;-read_csv(\"https://jagelves.github.io/Data/FoodSpend.csv\")\n\nRows: 80 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): OwnHome\ndbl (1): Food\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nSpend&lt;-na.omit(Spend)\n\nTo create a dummy variable for OwnHome we can use the ifelse() function:\n\nSpend$dummyOH&lt;-ifelse(Spend$OwnHome==\"Yes\",1,0)\n\nThe average of the dummy variable is given by:\n\nmean(Spend$dummyOH)\n\n[1] 0.3625\n\n\n\n\nRun a regression with Food being the dependent variable and your dummy variable as the independent variable. What is the interpretation of the intercept and slope?\n\n\n\nAnswer\n\nThe intercept is the average food expenditure of individuals without homes (\\(6417\\)). The slope, is the difference in food expenditures between individuals that do have homes minus those who don’t. We then conclude that individuals that do have a home spend about \\(-2516\\) less on food than those who don’t have homes.\nTo run the regression use the lm() function:\n\nlm(Food~dummyOH,data=Spend)\n\n\nCall:\nlm(formula = Food ~ dummyOH, data = Spend)\n\nCoefficients:\n(Intercept)      dummyOH  \n       6473        -3418  \n\n\n\n\nNow run a regression with Food being the independent variable and your dummy variable as the dependent variable. What is the interpretation of the intercept and slope? Hint: you might want to plot the scatter diagram and the regression line.\n\n\n\nAnswer\n\nThe scatter plot shows that most of the points for home owners are below \\(6000\\). For non-home owners they are mainly above \\(6000\\). The line can be used to predict the likelihood of owning a home given someones food expenditure. The intercept is above one, but still it gives us the indication that it is likely that low food expenditures are highly predictive of owning a home. The slope tells us how that likelihood changes as the food expenditures increase by 1. In general, the likelihood of owning a home decreases as the food expenditure increases.\nRun the lm() function once again:\n\nfitFood&lt;-lm(dummyOH~Food,data=Spend)\ncoefficients(fitFood)\n\n  (Intercept)          Food \n 1.4320766616 -0.0002043632 \n\n\nFor the scatter plot use the following code:\n\nlibrary(ggthemes)\nSpend %&gt;% ggplot() + \n  geom_point(aes(y=dummyOH,x=Food), \n             col=\"black\", pch=21, bg=\"grey\") +\n  geom_smooth(aes(y=dummyOH,x=Food), method=\"lm\",\n              formula=y~x, se=F) + \n  theme_clean()\n\n\n\n\n\n\n\n\n\n\n\nExercise 4\nYou will need the Population data set to answer this question. You can find this data set at https://jagelves.github.io/Data/Population.csv .\n\nRun a regression of Population on Year. How well does the regression line fit the data?\n\n\n\nAnswer\n\nIf we follow the \\(R^2=0.81\\) the model fits the data very well.\nLet’s load the data from the web:\n\nPopulation&lt;-read_csv(\"https://jagelves.github.io/Data/Population.csv\")\n\nNew names:\nRows: 16492 Columns: 4\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): Country.Name dbl (3): ...1, Year, Population\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\nNow let’s filter the data so that we can focus on the population for Japan.\n\nJapan&lt;-filter(Population,Country.Name==\"Japan\")\n\nNext, we can run the regression of Population against the Year. Let’s also run the summary() function to obtain the fit and the coefficients.\n\nfit&lt;-lm(Population~Year,data=Japan)\nsummary(fit)\n\n\nCall:\nlm(formula = Population ~ Year, data = Japan)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-9583497 -4625571  1214644  4376784  5706004 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -988297581   68811582  -14.36   &lt;2e-16 ***\nYear            555944      34569   16.08   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4871000 on 60 degrees of freedom\nMultiple R-squared:  0.8117,    Adjusted R-squared:  0.8086 \nF-statistic: 258.6 on 1 and 60 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nCreate a prediction for Japan’s population in 2030. What is your prediction?\n\n\n\nAnswer\n\nThe prediction for \\(2030\\) is about \\(140\\) million people.\nLet’s use the predict() function:\n\npredict(fit,newdata=data.frame(Year=c(2030)))\n\n        1 \n140268585 \n\n\n\n\nCreate a scatter diagram and include the regression line. How confident are you of your prediction after looking at the diagram?\n\n\n\nAnswer\n\nAfter looking at the scatter plot, it seems unlikely that the population in Japan will hit \\(140\\) million. Population has been decreasing in Japan!\nUse the plot() and abline() functions to create the figure.\n\nJapan %&gt;% ggplot() +\n  geom_point(aes(y=Population,x=Year), \n             col=\"black\", pch=21, bg=\"grey\") +\n  geom_smooth(aes(y=Population,x=Year), \n              formula=y~x, method=\"lm\", se=F) +\n  theme_clean()",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regression II</span>"
    ]
  },
  {
    "objectID": "ProbabilityI.html",
    "href": "ProbabilityI.html",
    "title": "8  Probability I",
    "section": "",
    "text": "8.1 Frequentist Vs. Bayesian Statistics\nDecision-making under uncertainty is at the core of business strategy, risk management, and forecasting. This section will study tools and concepts that will make us smarter when making data-driven decisions. Mastering probability in finance, marketing, supply chain, or operations helps professionals analyze risks, optimize strategies, and stay competitive in uncertain environments. We start with the basic concepts and ideas of probability below.\nThere are two interpretations of probability. The frequentist interpretation assumes that probabilities represent proportions of specific events occurring over infinitely identical trials. In contrast, the Bayesian interpretation assumes that probabilities are subjective beliefs about the relative likelihood of events. Despite their philosophical differences, both approaches adhere to the core rules of probability —including the Addition Rule, Multiplication Rule, and Bayes’ Theorem— which provide a consistent mathematical foundation for making probabilistic inferences.\nEx: A company might use frequentist statistics to determine the average return rate of a product, but apply Bayesian methods to adjust sales forecasts based on changing consumer behavior.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability I</span>"
    ]
  },
  {
    "objectID": "ProbabilityI.html#concepts",
    "href": "ProbabilityI.html#concepts",
    "title": "8  Probability I",
    "section": "",
    "text": "Frequentist Vs. Bayesian\nThe frequentist interpretation assumes that probabilities represent proportions of specific events occurring over infinitely identical trials.\nThe Bayesian interpretation assumes that probabilities are subjective beliefs about the relative likelihood of events.\n\n\nExperiments and Sets\nAn experiment is a process that leads to one of several outcomes. Ex: Tossing a Die, Tossing a Coin, Drawing a Card, etc.\nAn outcome is the result of an experiment. Ex: A coin landing on heads, drawing the ace of spades.\nThe sample space \\((S)\\) of an experiment contains all possible outcomes of the experiment. Ex: \\(S\\)={\\(1\\),\\(2\\),\\(3\\),\\(4\\),\\(5\\),\\(6\\)} is the sample space for tossing a die.\nAn event is a subset of the sample space. \\(A\\)={\\(2\\),\\(4\\),\\(6\\)} is the event of tossing an even number when rolling a die.\n\n\nBasic Probability Concepts\nA probability is a numerical value that measures the likelihood that an event occurs.\nTo calculate probabilities, find the ratio between favorable outcomes and total outcomes. \\(p=favorable/total\\).\n\nThe probability of any event \\(A\\) is a value between \\(0\\) and \\(1\\) inclusive. Formally, \\(0\\leq P(A) \\leq1\\).\nWhen the probability of the event is \\(0\\) then the event is impossible. When the probability is \\(1\\) then the event is certain.\nThe sum of the probabilities of a list of mutually exclusive and exhaustive events equals \\(1\\). Formally, \\(\\sum P(x_i)=1\\).\n\nMutually exclusive events do not share any common outcomes. The occurrence of one event precludes the occurrence of others.\nExhaustive events include all outcomes in the sample space.\n\n\nTo assign probabilities you can use the Empirical, Classical, or Subjective Methods.\n\nEmpirical: calculated as a relative frequency of occurrence.\nClassical: based on logical analysis.\nSubjective: calculated by drawing on personal and subjective judgement.\n\n\n\nProbability Rules\nThe Complement Rule: \\(P(A^c)=1-P(A)\\), where \\(A^c\\) is the complement of \\(A\\).\nThe Addition Rule: \\(P(A \\cup B)=P(A)+P(B)-P(A \\cap B)\\), where \\(\\cap\\) is intersection and \\(\\cup\\) is union.\nThe Multiplication Rule:\n\nif events are dependent \\(P(A \\cap B)= P(A|B)P(B)\\), where \\(P(A|B)\\) is the conditional probability.\nif events are independent \\(P(A \\cap B)= P(A)P(B)\\).\n\nThe Law of Total Probability: \\(P(A)=P(A|B)P(B)+P(A|B^c)P(B^c)\\).\nBayes’ Theorem: \\(P(A|B)=P(B|A)P(A)/P(B)\\).\n\n\nCounting Rules\nThe Combination function counts the number of ways to choose \\(x\\) objects from a total of \\(n\\) objects. The order in which the \\(x\\) objects are listed does not matter.\n\nIf repetition is not allowed use \\(C_n^x= \\frac{n!}{(n-x)!x!}\\).\nIf repetition is allowed use \\(\\frac{(x+n-1)!}{(n-1)!x!}\\).\n\nThe Permutation function also counts the number of ways to choose \\(x\\) objects from a total of \\(n\\) objects. However, the order in which the \\(x\\) objects are listed does matter.\n\nIf repetition is not allowed use \\(P_n^x= \\frac{n!}{(n-x)!}\\).\nIf repetition is allowed use \\(n^x\\).\n\n\n\nUseful R Functions\nThe table() function can be used to construct frequency distributions.\nThe factorial() function returns the factorial of a number.\nThe gtools package contains the combinations() and permutations() functions used to calculate combinations and permutations. Use the repeats.allowed argument to specify counting with repetition or no repetition. The v argument allows you to specify a vector of elements.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability I</span>"
    ]
  },
  {
    "objectID": "ProbabilityI.html#exercises",
    "href": "ProbabilityI.html#exercises",
    "title": "8  Probability I",
    "section": "8.9 Exercises",
    "text": "8.9 Exercises\nThe following exercises will help you practice some probability concepts and formulas. In particular, the exercises work on:\n\nCalculating simple probabilities.\nApplying probability rules.\nUsing counting rules.\n\nAnswers are provided below. Try not to peak until you have a formulated your own answer and double checked your work for any mistakes.\n\nExercise 1\nFor the following exercises, make your calculations by hand and verify results with a calculator or R.\n\nA sample space \\(S\\) yields five equally likely events, \\(A\\), \\(B\\), \\(C\\), \\(D\\), and \\(E\\). Find \\(P(D)\\), \\(P(B^c)\\), and \\(P(A \\cup C \\cup E)\\).\n\n\n\nAnswer\n\n\n\\(P(D)=1/5=0.2\\) since all events are equally likely.\n\\(P(B^c)=4/5=0.8\\)\n\\(P(A \\cup C \\cup E)=P(A + C + E)=3/5=0.6\\).\n\n\n\nConsider the roll of a die. Define \\(A\\) as {1,2,3}, \\(B\\) as {1,2,3,5,6}, \\(C\\) as {4,6}, and \\(D\\) as {4,5,6}. Are the events \\(A\\) and \\(B\\) mutually exclusive, exhaustive, both or none? What about events \\(A\\) and \\(D\\)?\n\n\n\nAnswer\n\n\nEvents \\(A\\) and \\(B\\) are not mutually exclusive since they share some of the same elements.\nThe events are not exhaustive since the union of both doesn’t create the sample space.\n\n\n\nA recent study suggests that \\(33.1\\)% of the adult U.S. population is overweight and \\(35.7\\)% obese. What is the probability that a randomly selected adult in the U.S. is either obese or overweight? Are the events mutually exclusive and exhaustive? What is the probability that their weight is normal?\n\n\n\nAnswer\n\n\nThe probability is \\(68.8\\)%.\nThe events are mutually exclusive. If someone is classified as obese, the person is not classified again as overweight.\nThe events are not exhaustive since there are people in the U.S. that have a normal weight.\nThe probability that the person drawn has normal weight is \\(31.2\\)%.\n\n\n\n\nExercise 2\nFor the following exercises, make your calculations by hand and verify results with a calculator or R.\n\nLet \\(P(A)=0.65\\), \\(P(B)=0.3\\), and \\(P(A|B)=0.45\\). Calculate \\(P(A \\cap B)\\), \\(P(A \\cup B)\\), and \\(P(B|A)\\).\n\n\n\nAnswer\n\nFrom the multiplication rule, \\[\nP(A|B)*P(B)=P(A \\cap B)\n\\]\nsubstituting values yields: \\[\nP(A \\cap B)=0.45*0.3=0.135\n\\] From the addition rule, \\[\nP(A \\cup B)=P(A)+P(B)-P(A \\cap B)\n\\] substituting yields: \\[P(A \\cup B)=0.65+0.3-0.135=0.815\\] From the multiplication rule once again, \\[P(B|A)=\\frac{P(A \\cap B)}{P(A)}\\] substituting values yields, \\[P(B|A)=0.135/0.65=0.2076923\\]\n\n\nLet \\(P(A)=0.4\\), \\(P(B)=0.5\\), and \\(P(A^c \\cap B^c)=0.24\\). Calculate \\(P(A^c|B^c)\\), \\(P(A^c \\cup B^c)\\), and \\(P(A \\cup B)\\).\n\n\n\nAnswer\n\nFrom the complement rule we have that \\(P(A^c)=0.6\\) and \\(P(B^c)=0.5\\).\nUsing the multiplication rule, \\[P(A^c|B^c)=\\frac{P(A^c \\cap B^c)}{P(B^c)}\\] substituting yields: \\[P(A^c|B^c)=0.24/0.5=0.48\\] From the addition rule, \\[P(A^c \\cup B^c)=P(A^c)+P(B^c)-P(A^c \\cap B^c)\\] substituting yields: \\[P(A^c \\cup B^c)=0.6+0.5-0.24=0.86\\] The event that has no elements of \\(A\\) or \\(B\\) is given by \\(P(A^c \\cap B^c)\\). Therefore \\(P(A \\cup B)=1-0.24=0.76\\) has all the elements of A and B.\n\n\nStock \\(A\\) will rise in price with a probability of \\(0.4\\), stock \\(B\\) will rise with a probability of \\(0.6\\). If stock \\(B\\) rises in price, then \\(A\\) will also rise with a probability of \\(0.5\\). What is the probability that at least one of the stocks will rise in price? Prove that events \\(A\\) and \\(B\\) are (are not) mutually exclusive (independent).\n\n\n\nAnswer\n\nIn short, the problem states \\(P(A)=0.4\\), \\(P(B)=0.6\\), and \\(P(A|B)=0.5\\). Where \\(A\\) and \\(B\\) are events of stocks rising in price. The question asks for: \\[P(A \\cup B)=P(A)+P(B)-P(A \\cap B)\\] Using the multiplication rule, \\[P(A \\cap B)=0.5*0.6=0.3\\] Hence, \\[P(A \\cup B)=0.4+0.6-0.3=0.7\\] The events are not mutually exclusive since \\(P(A \\cap B)=0.3 \\neq 0\\). The events are also not independent since \\(P(A|B)=0.5 \\neq 0.4=P(A)\\).\n\n\n\nExercise 3\n\nCreate a joint probability table from the contingency table below. Find \\(P(A)\\), \\(P(A \\cap B)\\), \\(P(A|B)\\), and \\(P(B|A^c)\\). Determine whether the events are independent or mutually exclusive.\n\n\n\n\n\n\\(B\\)\n\\(B^c\\)\n\n\n\\(A\\)\n26\n34\n\n\n\\(A^c\\)\n14\n26\n\n\n\n\n\nAnswer\n\nBelow is the joint probability table. The \\(P(A)=0.26+0.34=0.6\\), \\(P(A \\cap B)=0.26\\), \\(P(A|B)=0.26/0.4=0.65\\), and \\(P(B|A^c)=0.14/0.4=0.35\\). Events \\(A\\) and \\(B\\) are not independent since \\(P(A) \\neq P(A|B)\\). The events are not mutually exclusive since \\(P(A \\cap B)=0.26 \\neq 0\\).\n\n\n\n\n\\(B\\)\n\\(B^c\\)\nTotal\n\n\n\\(A\\)\n0.26\n0.34\n0.6\n\n\n\\(A^c\\)\n0.14\n0.26\n0.4\n\n\nTotal\n0.4\n0.6\n1\n\n\n\n\n\n\nExercise 4\nYou will need the Crash data set and R to answer this question. The data shows information on several car crashes. Specifically, if the crash was Head-On or Not Head-On and whether there was Daylight or No Daylight. You can find the data here: https://jagelves.github.io/Data/Crash.csv\n\nCreate a contingency table.\n\n\n\nAnswer\n\nThe probability of a Head-On crash is \\((166+108)/4858=0.056\\). The probability of a daylight crash is \\((166+3258)/4858=0.70\\). The probability that the car crash is Head-On given daylight is \\(166/(166+3258)=0.048\\).\nStart by loading the data into R.\n\nlibrary(tidyverse)\nCrash&lt;-read_csv(\"https://jagelves.github.io/Data/Crash.csv\")\n\nTo create a contingency table use the table() command in R.\n\n(freq&lt;-table(Crash$`Crash Type`,Crash$`Light Condition`))\n\n             \n              Daylight Not Daylight\n  Head-on          166          108\n  Not Head-On     3258         1326\n\n\nThis table is used to calculate probabilities. We can pass it through the prop.table() function to get the contingency table.\n\nround(prop.table(freq),2)\n\n             \n              Daylight Not Daylight\n  Head-on         0.03         0.02\n  Not Head-On     0.67         0.27\n\n\n\n\nFind the probability that a) a car crash is Head-On, b) a car crash is in daylight c) a car crash is Head-On given that there is daylight.\n\n\n\nAnswer\n\nThe probability of a head-on crash is:\n\\[P(\\text{Head-on})=0.03+0.02=0.05\\] The probability that the crash is in daylight is:\n\\[P(Daylight)=0.03+0.67=0.70\\] \\[P(Daylight)=\\frac{P(\\text{Head-on})}{P(Daylight)}=\\frac{0.03}{0.70}\\]\n\n\nShow that Crashes and Light are dependent.\n\n\n\nAnswer\n\nThe two variables are dependent since \\(P(Head-On|Daylight) \\neq P(Head-On)\\), that is \\(0.048 \\neq 0.56\\).\n\n\n\nExercise 5\n\nUse Bayes’ Theorem in the following question. Let \\(P(A)=0.7\\), \\(P(B|A)=0.55\\), and \\(P(B|A^c)=0.10\\). Find \\(P(A^c)\\), \\(P(A \\cap B)\\), \\(P(A^c \\cap B)\\), \\(P(B)\\), and \\(P(A|B)\\).\n\n\n\nAnswer\n\n\\(P(A^c)=1-P(A)=1-0.7=0.3\\), \\(P(A \\cap B)=𝑃(𝐵|𝐴)𝑃(𝐴) = 0.55(0.70) = 0.385\\), \\(P(A^c \\cap B)=𝑃(B|A^c)𝑃(A^c) = 0.10(0.30) = 0.03\\), \\(P(B)= 𝑃(A \\cap B) + 𝑃(𝐴^c \\cap 𝐵) = 0.385 + 0.03 = 0.415\\), and \\(P(A|B)= \\frac{𝑃(A \\cap B)}{P(B)}=0.385/0.415=0.9277\\).\n\n\nSome find tutors helpful when taking a course. Julia has a 40% chance to fail a course if she does not have a tutor. With a tutor, the probability of failing is only 10%. There is a 50% chance that Julia finds an available tutor. What is the probability that Julia will fail the course? If she ends up failing the course, what is the probability that she had a tutor?\n\n\n\nAnswer\n\nLet the event of failing be \\(F\\), the event of not failing be \\(NF\\), the event of having a tutor be \\(T\\), and the event of not having a tutor be \\(NT\\). The probability of failing the course is \\(0.25\\). \\((𝐹) = 𝑃(𝐹 \\cap 𝑇) + 𝑃(𝐹 \\cap 𝑇^c) = 𝑃(𝐹|𝑇)𝑃(𝑇) + 𝑃(𝐹|𝑇^c)𝑃(𝑇^c) = 0.10(0.50) + 0.40(0.50) = 0.05 + 0.20 = 0.25\\) The probability of not having a tutor, given that she failed the course is \\(0.2\\). \\(P(𝑇|𝐹) = \\frac{𝑃(𝐹\\cap𝑇)}{𝑃(𝐹\\cap𝑇)+𝑃(𝐹 \\cap𝑇^c)}= 0.05/0.25 = 0.20\\).\n\n\n\nExercise 6\n\nCalculate the following values and verify your results using R. a) 3!, b) 4!, c) \\(C_6^8\\), d) \\(P_6^8\\).\n\n\n\nAnswer\n\n\\(3!=3 \\times 2 \\times 1=6\\), \\(4!=6 \\times 4=24\\), \\(C_6^8=28\\), and \\(P_6^8=20,160\\)\nIn R we can just use the factorial command. So \\(3!\\) is:\n\nfactorial(3)\n\n[1] 6\n\n\nand \\(4!\\) is:\n\nfactorial(4)\n\n[1] 24\n\n\nFor combinations and permutations we can use the gtools package:\n\nlibrary(gtools)\nC&lt;-combinations(8,6)\nnrow(C)\n\n[1] 28\n\n\n\n\nThere are 10 players in a local basketball team. If we chose 5 players to randomly start a game, in how many ways can we select the five players if order doesn’t matter? What if order matters?\n\n\n\nAnswer\n\nIf order doesn’t matter, there are \\(252\\) ways. If order matters, then there are \\(30,240\\) ways.\nIn R we can once more use the combination and permutation functions:\n\nB1&lt;-combinations(10,5)\nnrow(B1)\n\n[1] 252\n\n\n\nB2&lt;-permutations(10,5)\nnrow(B2)\n\n[1] 30240",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability I</span>"
    ]
  },
  {
    "objectID": "ProbabilityI.html#answers",
    "href": "ProbabilityI.html#answers",
    "title": "8  Probability I",
    "section": "8.8 Answers",
    "text": "8.8 Answers\n\nExercise 1\n\n\\(P(D)=1/5=0.2\\) since all events are equally likely. \\(P(B^c)=4/5=0.8\\), and \\(P(A \\cup C \\cup E)=P(A + C + E)=3/5=0.6\\).\nEvents \\(A\\) and \\(B\\) are not mutually exclusive since they share some of the same elements. They are not exhaustive since the union of both doesn’t create the sample space.\nThe probability is \\(68.8\\)%. The events are mutually exclusive. If someone is classified as obese, the person is not classified again as overweight. The events are not exhaustive since there are people in the U.S. that have a normal weight. The probability that the person drawn has normal weight is \\(31.2\\)%.\n\n\n\nExercise 2\n\nFrom the multiplication rule, \\(P(A|B)*P(B)=P(A \\cap B)\\).\nSubstituting values yields, \\(P(A \\cap B)=0.45*0.3=0.135\\).\nFrom the addition rule, \\(P(A \\cup B)=P(A)+P(B)-P(A \\cap B)\\).\nSubstituting yields, \\(P(A \\cup B)=0.65+0.3-0.135=0.815\\).\nFrom the multiplication rule once again, \\(P(B|A)=\\frac{P(A \\cap B)}{P(A)}\\). Substituting yields, \\(P(B|A)=0.135/0.65=0.2076923\\).\nFrom the complement rule we have that \\(P(A^c)=0.6\\) and \\(P(B^c)=0.5\\).\nUsing the multiplication rule, \\(P(A^c|B^c)=\\frac{P(A^c \\cap B^c)}{P(B^c)}\\). Substituting yields \\(P(A^c|B^c)=0.24/0.5=0.48\\).\nFrom the addition rule \\(P(A^c \\cup B^c)=P(A^c)+P(B^c)-P(A^c \\cap B^c)\\).\nSubstituting yields \\(P(A^c \\cup B^c)=0.6+0.5-0.24=0.86\\).\nThe event that has no elements of \\(A\\) or \\(B\\) is given by \\(P(A^c \\cap B^c)\\). Therefore \\(P(A \\cup B)=1-0.24=0.76\\) has all the elements of A and B.\nIn short the problem states \\(P(A)=0.4\\), \\(P(B)=0.6\\), and \\(P(A|B)=0.5\\). Where \\(A\\) and \\(B\\) are events of stocks rising in price. The question asks for \\(P(A \\cup B)=P(A)+P(B)-P(A \\cap B)\\).\nUsing the multiplication rule \\(P(A \\cap B)=0.5*0.6=0.3\\).\nHence, \\(P(A \\cup B)=0.4+0.6-0.3=0.7\\).\nThe events are not mutually exclusive since \\(P(A \\cap B)=0.3 \\neq 0\\).\nThe events are also not independent since \\(P(A|B)=0.5 \\neq 0.4=P(A)\\).\n\n\n\nExercise 3\n\nBelow is the joint probability table. The \\(P(A)=0.26+0.34=0.6\\), \\(P(A \\cap B)=0.26\\), \\(P(A|B)=0.26/0.4=0.65\\), and \\(P(B|A^c)=0.14/0.4=0.35\\). Events \\(A\\) and \\(B\\) are not independent since \\(P(A) \\neq P(A|B)\\). The events are not mutually exclusive since \\(P(A \\cap B)=0.26 \\neq 0\\).\n\n\n\n\n\n\\(B\\)\n\\(B^c\\)\nTotal\n\n\n\\(A\\)\n0.26\n0.34\n0.6\n\n\n\\(A^c\\)\n0.14\n0.26\n0.4\n\n\nTotal\n0.4\n0.6\n1\n\n\n\n\n\nExercise 4\n\nThe probability of a Head-On crash is \\((166+108)/4858=0.056\\). The probability of a daylight crash is \\((166+3258)/4858=0.70\\). The probability that the car crash is Head-On given daylight is \\(166/(166+3258)=0.048\\).\n\nStart by loading the data into R.\n\nCrash&lt;-read.csv(\"https://jagelves.github.io/Data/Crash.csv\")\n\nTo create a contingency table use the table() command in R.\n\n(freq&lt;-table(Crash$Crash.Type,Crash$Light.Condition))\n\n             \n              Daylight Not Daylight\n  Head-on          166          108\n  Not Head-On     3258         1326\n\n\nThis table is used to calculate probabilities. We can pass it through the prop.table() function to get the contingency table.\n\nround(prop.table(freq),2)\n\n             \n              Daylight Not Daylight\n  Head-on         0.03         0.02\n  Not Head-On     0.67         0.27\n\n\n\nThe two variables are dependent since \\(P(Head-On|Daylight) \\neq P(Head-On)\\), that is \\(0.048 \\neq 0.56\\).\n\n\n\nExercise 5\n\n\\(P(A^c)=1-P(A)=1-0.7=0.3\\), \\(P(A \\cap B)=𝑃(𝐵|𝐴)𝑃(𝐴) = 0.55(0.70) = 0.385\\), \\(P(A^c \\cap B)=𝑃(B|A^c)𝑃(A^c) = 0.10(0.30) = 0.03\\), \\(P(B)= 𝑃(A \\cap B) + 𝑃(𝐴^c \\cap 𝐵) = 0.385 + 0.03 = 0.415\\), and \\(P(A|B)= \\frac{𝑃(A \\cap B)}{P(B)}=0.385/0.415=0.9277\\).\nLet the event of failing be \\(F\\), the event of not failing be \\(NF\\), the event of having a tutor be \\(T\\), and the event of not having a tutor be \\(NT\\). The probability of failing the course is \\(0.25\\). \\((𝐹) = 𝑃(𝐹 \\cap 𝑇) + 𝑃(𝐹 \\cap 𝑇^c) = 𝑃(𝐹|𝑇)𝑃(𝑇) + 𝑃(𝐹|𝑇^c)𝑃(𝑇^c) = 0.10(0.50) + 0.40(0.50) = 0.05 + 0.20 = 0.25\\) The probability of not having a tutor, given that she failed the course is \\(0.2\\). \\(P(𝑇|𝐹) = \\frac{𝑃(𝐹\\cap𝑇)}{𝑃(𝐹\\cap𝑇)+𝑃(𝐹 \\cap𝑇^c)}= 0.05/0.25 = 0.20\\)\n\n\n\nExercise 6\n\n\\(3!=3 \\times 2 \\times 1=6\\), \\(4!=6 \\times 4=24\\), \\(C_6^8=28\\), and \\(P_6^8=20,160\\)\n\nIn R we can just use the factorial command. So \\(3!\\) is:\n\nfactorial(3)\n\n[1] 6\n\n\nand \\(4!\\) is:\n\nfactorial(4)\n\n[1] 24\n\n\nFor combinations and permutations we can use the gtools package:\n\nlibrary(gtools)\nC&lt;-combinations(8,6)\nnrow(C)\n\n[1] 28\n\n\n\nIf order doesn’t matter, there are \\(252\\) ways. If order matters, then there are \\(30,240\\) ways.\n\nIn R we can once more use the combination and permutation functions:\n\nB1&lt;-combinations(10,5)\nnrow(B1)\n\n[1] 252\n\n\n\nB2&lt;-permutations(10,5)\nnrow(B2)\n\n[1] 30240",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability I</span>"
    ]
  },
  {
    "objectID": "ProbabilityII.html",
    "href": "ProbabilityII.html",
    "title": "9  Probability II",
    "section": "",
    "text": "9.1 Random Variables\nStudying discrete probability distributions is essential for understanding and predicting outcomes in uncertain scenarios, particularly in business, finance, and data science. These distributions model events with distinct, countable outcomes, such as customer purchases, defect counts, or market trends, enabling decision-makers to quantify risks and optimize strategies. Below, we introduce some popular distributions and their business application.\nThe study of probability distributions begins with a random variable. A random variable assigns a numerical value to each possible experimental outcome. The table below presents a collection of experiments along with their corresponding random variables.\nFor restaurant owners, the number of customers they attract is a crucial variable to monitor. Tracking this variable and its possible values helps in making informed business decisions. Below we will study how this type of experiment and several others behave, so that we can make better decisions. Note that since the outcomes are integer values, the resulting distribution is discrete. The probability mass function (PMF) summarizes the possible values generated by a discrete probability distribution.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability II</span>"
    ]
  },
  {
    "objectID": "ProbabilityII.html#concepts",
    "href": "ProbabilityII.html#concepts",
    "title": "9  Probability II",
    "section": "",
    "text": "Random Variables\nA random variable associates a numerical value with each possible experimental outcome. Specifically, the random variable takes on a value with some probability.\nA random variable is fully characterized by its probability density function (PDF) if continuous or the probability mass function (PMF) if discrete.\n\n\nExpected Value and Variance\nWhen summarizing a random variable, we are mostly interested in the variable’s central tendency (Expected Value) and dispersion (Variance).\nThe expected value (mean) is a measure of central location. For a discrete random variable it is given by \\(E(x)=\\mu=\\sum xf(x)\\), where \\(f(x)\\) is the probability mass function. For a continuous random variable it is given by \\(E(x)= \\int_{-\\infty}^{\\infty} x f(x) dx\\), where \\(f(x)\\) is the probability density function.\nThe variance summarizes the deviation of the values of the random variable from the mean. It is calculated by \\(var(x)=E[(x-E(x))^2]=E[x^2]-E[x]^2\\). Note that this formula can be used for both discrete and continuous random variables.\n\n\nDiscrete Uniform Distribution\nThe discrete uniform distribution is a probability distribution that assigns equal probability to each outcome in a finite set of possible outcomes. In other words, each outcome in the set is equally likely to occur.\nThe probability mass function is given by \\(f(x)=1/n\\), where \\(n\\) is the number of elements in the sample space (all possible outcomes).\nThe expected value is given by \\(E(x)=\\frac {\\sum x_i}{n}\\), where \\(x_i\\) are the possible values, and \\(n\\) is the number of possible values.\nThe variance is given by \\(var(x)=\\frac {\\sum (x_i-E(x))^2}{n-1}\\).\n\n\nBinomial Distribution\nThe binomial distribution is a probability distribution that describes the outcome of a sequence of \\(n\\) independent Bernoulli trials. In a Bernoulli trial, there are only two possible outcomes: “success” and “failure”. The probability of success is denoted by \\(p\\), and the probability of failure is denoted by \\(q = 1 - p\\). In a sequence of \\(n\\) independent Bernoulli trials, the number of successes (\\(x\\)) is a random variable that follows a binomial distribution.\nThe probability mass function is given by \\(f(x)=C_x^n (p^x)(1-p)^{n-x}\\), where \\(n\\) is the number of trials, \\(x\\) is the number of successes, \\(p\\) is the probability of success, and \\(C_x^n\\) is the number of ways there can be \\(x\\) successes in \\(n\\) trials.\nThe expected value of the binomial distribution is \\(E(x)=np\\).\nThe variance of the binomial distribution is \\(var(x)=np(1-p)\\).\n\n\nThe Hypergeometric Distribution\nThe hypergeometric distribution is a probability distribution that describes the outcome of drawing a sample from a population without replacement. It is used to calculate the probability of drawing a certain number of successes (\\(x\\)) in a sample of a given size (\\(n\\)), where the success or failure of each individual draw is not dependent on the success or failure of other draws.\nThe hypergeometric experiment differs from the binomial since:\n\ntrials are not independent.\nthe probability of success changes from trial to trial.\n\nThe probability mass function is given by \\(f(x)=\\frac {C_x^r C_{n-x}^{N-r}}{C_n^N}\\), where \\(n\\) is the number of trials, \\(x\\) is the number of successes, \\(r\\) is the number of elements in the population labeled as success, and \\(N\\) is the number of elements in the population.\nThe expected value of the hypergeometric distribution is \\(E(x)=n \\frac {r}{N}\\).\nThe variance of the hypergeometric distribution is \\(var(x)= n \\frac {r}{N} (1- \\frac {r}{N}) (\\frac {N-n}{N-1})\\).\n\n\nPoisson Distribution\nThe Poisson distribution estimates the number of successes (\\(x\\)) over a specified interval of time or space.\nThe probability mass function is given by \\(f(x)= \\frac {\\mu e^{-x}}{x!}\\), where \\(\\mu\\) is the expected number of successes in any given interval and also the variance, and \\(e\\) is Euler’s number (2.71828…).\nAn experiment satisfies a Poisson process if:\n\nThe number of successes with a specified time or space interval equals any integer between zero and infinity.\nThe number of successes counted in non-overlapping intervals are independent.\nThe probability of success in any interval is the same for all intervals of equal size and is proportional to the size of the interval.\n\n\n\nUseful R Functions\nTo calculate probabilities based on discrete random variables use the pbinom(), phyper(), and ppois() functions. For the uniform distribution use the extraDistr package and the pdunif() function.\nTo calculate cumulative probabilities use the dbinom(), dhyper(), dpois(), and ddunif() functions.\nTo calculate quantiles use the qbinom(), qhyper(), qpois(), and qdunif() functions.\nTo generate random numbers use the rbinom(), rhyper(), rpois(), and rdunif() functions.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability II</span>"
    ]
  },
  {
    "objectID": "ProbabilityII.html#exercises",
    "href": "ProbabilityII.html#exercises",
    "title": "9  Probability II",
    "section": "9.11 Exercises",
    "text": "9.11 Exercises\nThe following exercises will help you practice some probability concepts and formulas. In particular, the exercises work on:\n\nCalculating probabilities for discrete random variables.\nCalculating the expected value and standard deviation.\nApplying the binomial, Poisson and hypergeometric probability distributions.\n\nAnswers are provided below. Try not to peak until you have a formulated your own answer and double checked your work for any mistakes.\n\nExercise 1\nFor the following exercises, make your calculations by hand and verify results with a calculator or R.\n\nConsider the table below. Calculate the mean and standard deviation. What is the probability that \\(x&lt;15\\)?\n\n\n\n\n\\(x\\)\n5\n10\n15\n20\n\n\n\\(P(X=x)\\)\n0.35\n0.3\n0.2\n0.15\n\n\n\n\n\nAnswer\n\nThe expected value is \\(10.75\\) and the standard deviation is \\(5.31\\). The probability of \\(x&lt;15\\) is \\(0.65\\).\nIn R we can create vectors for both \\(x\\) and the probabilities \\(P(X=x)\\).\n\nx&lt;-c(5,10,15,20)\npx&lt;-c(0.35,0.3,0.2,0.15)\n\nThe expected value is the sum product of probabilities and values. Formally, \\(\\sum_{i=1}^{n}x_{i}p_{i}\\) and in R:\n\n(ex&lt;-sum(x*px))\n\n[1] 10.75\n\n\nThe standard deviation is given by \\(\\sqrt{\\sum_{i=1}^{n}(x_{i}-\\mu)^2p_{i}}\\). We can calculate it in R with the following code:\n\n(sd&lt;-sqrt(sum((x-ex)^2*px)))\n\n[1] 5.30919\n\n\n\n\nConsider the table below. Calculate the mean and standard deviation. What is the probability that \\(x\\geq-9\\)?\n\n\n\n\n\\(y\\)\n-23\n-17\n-9\n-3\n\n\n\\(P(Y=y)\\)\n0.5\n0.25\n0.15\n0.1\n\n\n\n\n\nAnswer\n\nThe expected value is \\(-17.4\\) and the standard deviation is \\(6.86\\). The probability of is \\(0.25\\).\nLet’s create the vectors once more in R.\n\ny&lt;-c(-23,-17,-9,-3)\npy&lt;-c(0.5,0.25,0.15,0.1)\n\nThe expected value is given by:\n\n(ey&lt;-sum(y*py))\n\n[1] -17.4\n\n\nThe standard deviation is given by:\n\n(sdy&lt;-sqrt(sum((y-ey)^2*py)))\n\n[1] 6.858571\n\n\n\n\nThe returns on a couple of funds depends on the state of the economy. The economy is expected to be Good with a probability of 20%, Fair with probability of 50% and Poor with probability of 30%. Which fund would you choose if you want to maximize your return? What would you choose if you really dislike risk?\n\n\n\n\nState of Economy\nFund 1\nFund 2\n\n\n\n\nGood\n20\n40\n\n\nFair\n10\n20\n\n\nPoor\n-10\n-40\n\n\n\n\n\nAnswer\n\nBoth funds have the same expected return of \\(6\\). The safest return comes from fund 1 since the standard deviation is only \\(11.14\\) vs. \\(31.05\\) for fund 2.\nIn R we can create a data frame with probabilities and the performance of the funds.\n\nfunds&lt;-data.frame(probs=c(0.2,0.5,0.3),fund1=c(20,10,-10), fund2=c(40,20,-40))\n\nLet’s create a function for the expected value and standard deviation. For the expected value:\n\nExpected_Value&lt;-function(x,p){\n  sum(x*p)\n}\n\nNow we can use the formula to calculate the expected value of fund1:\n\nExpected_Value(funds$fund1,funds$probs)\n\n[1] 6\n\n\nand fund 2:\n\nExpected_Value(funds$fund2,funds$probs)\n\n[1] 6\n\n\nFor the standard deviation we can create another function:\n\nStandard_Deviation&lt;-function(x,p){\n  sqrt(sum((x-Expected_Value(x,p))^2*p))\n}\n\nUsing the function to get the standard deviation of fund 1 we get:\n\nStandard_Deviation(funds$fund1,funds$probs)\n\n[1] 11.13553\n\n\nand for fund 2:\n\nStandard_Deviation(funds$fund2,funds$probs)\n\n[1] 31.04835\n\n\n\n\n\nExercise 2\n\nUse the table below. A portfolio has 200,000 dollars invested in Asset \\(X\\) and 300,000 dollars in asset \\(Y\\). If the correlation coefficient between the two investments is \\(0.4\\), what is the expected return and standard deviation of the portfolio?\n\n\n\n\nMeasure\nX\nY\n\n\n\n\nExpected Return (%)\n8\n12\n\n\nStandard Deviation (%)\n12\n20\n\n\n\n\n\nAnswer\n\nThe expected return of the portfolio is \\(10.4\\) and the standard deviation is \\(14.60\\).\nIn R we can start by calculating the expected return. This is given by the formula \\(\\alpha R_{1} +\\beta R_{2}\\):\n\n(ER&lt;-(2/5)*8+(3/5)*12)\n\n[1] 10.4\n\n\nNext we can find the standard deviation with the formula \\(\\sqrt{\\alpha^2 \\sigma_{1}^2+\\beta^2 \\sigma_{2}+\\alpha \\beta \\rho \\sigma_{1} \\sigma_{2}}\\):\n\n(Risk&lt;-sqrt(0.4^2*12^2 + 0.6^2*20^2+2*0.4*0.6*0.4*12*20))\n\n[1] 14.59863\n\n\n\n\n\nExercise 3\n\nLet \\(Z\\) be a binomial random variable with \\(n=5\\) and \\(p=0.35\\) use the binomial formula to find \\(P(Z=1)\\), \\(P(Z \\geq 2)\\). What is the expected value and standard deviation of \\(Z\\)?\n\n\n\nAnswer\n\n\\(P(Z=1)=0.31\\), and \\(P(Z \\geq 2)=0.57\\). The expected value is \\(np=1.75\\) and the standard deviation is \\(\\sqrt{np(1-p)}=1.067\\).\nLet’s use R and the dbinom() function to find \\(P(Z=1)\\).\n\ndbinom(1,5,0.35)\n\n[1] 0.3123859\n\n\nWe can now use pbinom() to find the cumulative distribution. Since we want the right tale of the distribution, we will specify this with an argument.\n\npbinom(1,5,0.35, lower.tail=F)\n\n[1] 0.571585\n\n\n\n\nLet \\(W\\) be a binomial random variable with \\(n=200\\) and \\(p=0.77\\) use the binomial formula to find \\(P(W&gt;160)\\), \\(P(155 \\leq W \\leq 165)\\). What is the expected value and standard deviation of \\(W\\)?\n\n\n\nAnswer\n\n\\(P(W&gt;160)=0.14\\), and \\(P(155 \\leq W \\leq 165 )=0.45\\). The expected value is \\(np=154\\) and the standard deviation is \\(\\sqrt{np(1-p)}=5.95\\).\nUsing the pbinom() function we find that \\(P(W&gt;160)\\).\n\npbinom(160,200,0.77, lower.tail = F)\n\n[1] 0.136611\n\n\nWe make two calculations to find the probability. First, \\(P(W \\leq 165)\\) and then \\(P(W \\geq 154)\\). The difference between these two, gives us the desired outcome.\n\npbinom(165,200,0.77, lower.tail=T)-pbinom(154,200,0.77, lower.tail=T)\n\n[1] 0.4487104\n\n\n\n\nSixty percent of a firm’s employees are men. Suppose four of the firm’s employees are randomly selected. What is more likely, finding three men and one woman, or two men and one woman? Does your answer change if the proportion falls to \\(50\\)%?\n\n\n\nAnswer\n\nThe probabilities are the same. Each event has a probability of \\(0.3456\\). If the probability changes to \\(0.5\\) now the event of two women and two men is more likely.\nLet’s calculate the probabilities in R. First, the probability of three men and one woman.\n\ndbinom(3,4,0.6)\n\n[1] 0.3456\n\n\nNow the probability of two men and two women.\n\ndbinom(2,4,0.6)\n\n[1] 0.3456\n\n\nChanging the probabilities reveals that:\n\ndbinom(3,4,0.5)\n\n[1] 0.25\n\n\n\ndbinom(2,4,0.5)\n\n[1] 0.375\n\n\nHaving two of each is the most likely outcome.\n\n\n\nExercise 4\n\nAssume that \\(S\\) is a Poisson process with mean of \\(\\mu=1.5\\). Calculate \\(P(S=2)\\) and \\(P(S \\geq 2)\\). What is the mean and standard deviation of \\(S\\)?\n\n\n\nAnswer\n\nThe \\(P(S=2)=0.25\\) and \\(P(S \\geq 2)=0.44\\). The expected value and the variance is \\(1.5\\).\nIn R we will make use of the dpois() function:\n\ndpois(2,1.5)\n\n[1] 0.2510214\n\n\nFor the second probability we will use ppois():\n\nppois(1,1.5, lower.tail=F)\n\n[1] 0.4421746\n\n\n\n\nAssume that \\(T\\) is a Poisson process with mean of \\(\\mu=20\\). Calculate \\(P(T=14)\\) and \\(P(18 \\leq T \\leq 23)\\).\n\n\n\nAnswer\n\nThe \\(P(T=14)=0.039\\) and \\(P(18 \\leq T \\leq 23)=0.49\\).\nUsing the dpois() function once more:\n\ndpois(14,20)\n\n[1] 0.03873664\n\n\nFor the second probability we will find the difference between two probabilities:\n\nppois(23,20, lower.tail=T)-ppois(17,20, lower.tail=T)\n\n[1] 0.4904644\n\n\n\n\nA local pharmacy administers on average \\(84\\) Covid-19 vaccines per week. The vaccines shots are evenly administered across all days. Find the probability that the number of vaccine shots administered on a Wednesday is more than eight but less than \\(12\\).\n\n\n\nAnswer\n\nThe probability of administering more than \\(8\\) but less than \\(12\\) shots is \\(0.3\\).\nLet’s first note that if \\(84\\) shots are administered on average weekly, then \\(12\\) are administered daily. Now we can use this average and the ppois() function to find the probability:\n\nppois(11,12)-ppois(8,12)\n\n[1] 0.3065696\n\n\n\n\n\nExercise 5\n\nAssume that \\(X\\) is a hypergeometric random variable with \\(N=25\\), \\(S=3\\), and \\(n=4\\). Calculate \\(P(X=0)\\), \\(P(X=1)\\), and \\(P(X \\leq 1)\\).\n\n\n\nAnswer\n\n\\(P(X=0)=0.58\\), \\(P(X=1)=0.37\\), and \\(P(X \\leq 1)=0.94\\).\nIn R we can use the dhyper() function\n\ndhyper(0,3,22,4)\n\n[1] 0.5782609\n\n\nonce more for the second probability:\n\ndhyper(1, 3, 22, 4)\n\n[1] 0.3652174\n\n\nFor the last probability we can add the previous probabilities or use the phyper() function:\n\nphyper(1, 3, 22, 4)\n\n[1] 0.9434783\n\n\n\n\nCompute the probability of at least eight successes in a random sample of \\(20\\) items obtained from a population of \\(100\\) items that contains \\(25\\) successes. What are the expected value and standard deviation of the number of successes?\n\n\n\nAnswer\n\nThe probability is \\(0.545\\).\nIn R we use the dhyper() function once more:\n\ndhyper(0, 2, 10, 3)\n\n[1] 0.5454545\n\n\n\n\nFor \\(1\\) dollar a player gets to select six numbers for the base game of Powerball. In the game, five balls are randomly drawn from \\(59\\) consecutively numbered white balls. One ball, called the Powerball, is randomly drawn from \\(39\\) consecutively numbered red balls. What is the probability that a player is able to match two out of five randomly drawn white balls? What is the probability of winning the jackpot?\n\n\n\nAnswer\n\nThe probability of matching two white balls is \\(5%\\). Winning the jackpot is extremely unlikely! A probability of \\(0.00000000512\\). It is more likely to be struck by lightning according to the CDC.\nIn R use the dhyper() function:\n\ndhyper(2, 5, 54, 5)\n\n[1] 0.04954472\n\n\nFor the jackpot we first calculate the probability of getting all of the white balls.\n\noptions(digits = 5,scipen=999)\ndhyper(5, 5, 54, 5)\n\n[1] 0.00000019974\n\n\nNow the probability of getting the Powerball.\n\ndhyper(1, 1, 38, 1)\n\n[1] 0.025641\n\n\nSince the two events are independent, we can multiply them to find the probability of a jackpot.\n\ndhyper(5, 5, 54, 5)*dhyper(1, 1, 38, 1)\n\n[1] 0.0000000051217",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability II</span>"
    ]
  },
  {
    "objectID": "ProbabilityII.html#answers",
    "href": "ProbabilityII.html#answers",
    "title": "9  Probability II",
    "section": "9.3 Answers",
    "text": "9.3 Answers\n\nExercise 1\n\nThe expected value is \\(10.75\\) and the standard deviation is \\(5.31\\). The probability of \\(x&lt;15\\) is \\(0.65\\).\n\nIn R we can create vectors for both \\(x\\) and the probabilities \\(P(X=x)\\).\n\nx&lt;-c(5,10,15,20)\npx&lt;-c(0.35,0.3,0.2,0.15)\n\nThe expected value is the sum product of probabilities and values. Formally, \\(\\sum_{i=1}^{n}x_{i}p_{i}\\) and in R:\n\n(ex&lt;-sum(x*px))\n\n[1] 10.75\n\n\nThe standard deviation is given by \\(\\sqrt{\\sum_{i=1}^{n}(x_{i}-\\mu)^2p_{i}}\\). We can calculate it in R with the following code:\n\n(sd&lt;-sqrt(sum((x-ex)^2*px)))\n\n[1] 5.30919\n\n\n\nThe expected value is \\(-17.4\\) and the standard deviation is \\(6.86\\). The probability of is \\(0.25\\).\n\nLet’s create the vectors once more in R.\n\ny&lt;-c(-23,-17,-9,-3)\npy&lt;-c(0.5,0.25,0.15,0.1)\n\nThe expected value is given by:\n\n(ey&lt;-sum(y*py))\n\n[1] -17.4\n\n\nThe standard deviation is given by:\n\n(sdy&lt;-sqrt(sum((y-ey)^2*py)))\n\n[1] 6.858571\n\n\n\nBoth funds have the same expected return of \\(6\\). The safest return comes from fund 1 since the standard deviation is only \\(11.14\\) vs. \\(31.05\\) for fund 2.\n\nIn R we can create a data frame with probabilities and the performance of the funds.\n\nfunds&lt;-data.frame(probs=c(0.2,0.5,0.3),fund1=c(20,10,-10), fund2=c(40,20,-40))\n\nLet’s create a function for the expected value and standard deviation. For the expected value:\n\nExpected_Value&lt;-function(x,p){\n  sum(x*p)\n}\n\nNow we can use the formula to calculate the expected value of fund1:\n\nExpected_Value(funds$fund1,funds$probs)\n\n[1] 6\n\n\nand fund 2:\n\nExpected_Value(funds$fund2,funds$probs)\n\n[1] 6\n\n\nFor the standard deviation we can create another function:\n\nStandard_Deviation&lt;-function(x,p){\n  sqrt(sum((x-Expected_Value(x,p))^2*p))\n}\n\nUsing the function to get the standard deviation of fund 1 we get:\n\nStandard_Deviation(funds$fund1,funds$probs)\n\n[1] 11.13553\n\n\nand for fund 2:\n\nStandard_Deviation(funds$fund2,funds$probs)\n\n[1] 31.04835\n\n\n\n\nExercise 2\n\nThe expected return of the portfolio is \\(10.4\\) and the standard deviation is \\(14.60\\).\n\nIn R we can start by calculating the expected return. This is given by the formula \\(\\alpha R_{1} +\\beta R_{2}\\):\n\n(ER&lt;-(2/5)*8+(3/5)*12)\n\n[1] 10.4\n\n\nNext we can find the standard deviation with the formula \\(\\sqrt{\\alpha^2 \\sigma_{1}^2+\\beta^2 \\sigma_{2}+\\alpha \\beta \\rho \\sigma_{1} \\sigma_{2}}\\):\n\n(Risk&lt;-sqrt(0.4^2*12^2 + 0.6^2*20^2+2*0.4*0.6*0.4*12*20))\n\n[1] 14.59863\n\n\n\n\nExercise 3\n\n\\(P(Z=1)=0.31\\), and \\(P(Z \\geq 2)=0.57\\). The expected value is \\(np=1.75\\) and the standard deviation is \\(\\sqrt{np(1-p)}=1.067\\).\n\nLet’s use R and the dbinom() function to find \\(P(Z=1)\\).\n\ndbinom(1,5,0.35)\n\n[1] 0.3123859\n\n\nWe can now use pbinom() to find the cumulative distribution. Since we want the right tale of the distribution, we will specify this with an argument.\n\npbinom(1,5,0.35, lower.tail=F)\n\n[1] 0.571585\n\n\n\n\\(P(W&gt;160)=0.14\\), and \\(P(155 \\leq W \\leq 165 )=0.45\\). The expected value is \\(np=154\\) and the standard deviation is \\(\\sqrt{np(1-p)}=5.95\\).\n\nUsing the pbinom() function we find that \\(P(W&gt;160)\\).\n\npbinom(160,200,0.77, lower.tail = F)\n\n[1] 0.136611\n\n\nWe make two calculations to find the probability. First, \\(P(W \\leq 165)\\) and then \\(P(W \\geq 154)\\). The difference between these two, gives us the desired outcome.\n\npbinom(165,200,0.77, lower.tail=T)-pbinom(154,200,0.77, lower.tail=T)\n\n[1] 0.4487104\n\n\n\nThe probabilities are the same. Each event has a probability of \\(0.3456\\). If the probability changes to \\(0.5\\) now the event of two women and two men is more likely.\n\nLet’s calculate the probabilities in R. First, the probability of three men and one woman.\n\ndbinom(3,4,0.6)\n\n[1] 0.3456\n\n\nNow the probability of two men and two women.\n\ndbinom(2,4,0.6)\n\n[1] 0.3456\n\n\nChanging the probabilities reveals that:\n\ndbinom(3,4,0.5)\n\n[1] 0.25\n\n\n\ndbinom(2,4,0.5)\n\n[1] 0.375\n\n\nHaving two of each is the most likely outcome.\n\n\nExercise 4\n\nThe \\(P(S=2)=0.25\\) and \\(P(S \\geq 2)=0.44\\). The expected value and the variance is \\(1.5\\).\n\nIn R we will make use of the dpois() function:\n\ndpois(2,1.5)\n\n[1] 0.2510214\n\n\nFor the second probability we will use ppois():\n\nppois(1,1.5, lower.tail=F)\n\n[1] 0.4421746\n\n\n\nThe \\(P(T=14)=0.039\\) and \\(P(18 \\leq T \\leq 23)=0.49\\).\n\nUsing the dpois() function once more:\n\ndpois(14,20)\n\n[1] 0.03873664\n\n\nFor the second probability we will find the difference between two probabilities:\n\nppois(23,20, lower.tail=T)-ppois(17,20, lower.tail=T)\n\n[1] 0.4904644\n\n\n\nThe probability of administering more than \\(8\\) but less than \\(12\\) shots is \\(0.3\\).\n\nLet’s first note that if \\(84\\) shots are administered on average weekly, then \\(12\\) are administered daily. Now we can use this average and the ppois() function to find the probability:\n\nppois(11,12)-ppois(8,12)\n\n[1] 0.3065696\n\n\n\n\nExercise 5\n\n\\(P(X=0)=0.58\\), \\(P(X=1)=0.37\\), and \\(P(X \\leq 1)=0.94\\).\n\nIn R we can use the dhyper() function\n\ndhyper(0,3,22,4)\n\n[1] 0.5782609\n\n\nonce more for the second probability:\n\ndhyper(1, 3, 22, 4)\n\n[1] 0.3652174\n\n\nFor the last probability we can add the previous probabilities or use the phyper() function:\n\nphyper(1, 3, 22, 4)\n\n[1] 0.9434783\n\n\n\nThe probability is \\(0.545\\).\n\nIn R we use the dhyper() function once more:\n\ndhyper(0, 2, 10, 3)\n\n[1] 0.5454545\n\n\n\nThe probability of matching two white balls is \\(5%\\). Winning the jackpot is extremely unlikely! A probability of \\(0.00000000512\\). It is more likely to be struck by lightning according to the CDC.\n\nIn R use the dhyper() function:\n\ndhyper(2, 5, 54, 5)\n\n[1] 0.04954472\n\n\nFor the jackpot we first calculate the probability of getting all of the white balls.\n\noptions(digits = 5,scipen=999)\ndhyper(5, 5, 54, 5)\n\n[1] 0.00000019974\n\n\nNow the probability of getting the Powerball.\n\ndhyper(1, 1, 38, 1)\n\n[1] 0.025641\n\n\nSince the two events are independent, we can multiply them to find the probability of a jackpot.\n\ndhyper(5, 5, 54, 5)*dhyper(1, 1, 38, 1)\n\n[1] 0.0000000051217",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability II</span>"
    ]
  },
  {
    "objectID": "ProbabilityIII.html",
    "href": "ProbabilityIII.html",
    "title": "10  Probability III",
    "section": "",
    "text": "10.1 Continuous Random Variables\nStudying continuous probability distributions is crucial for understanding and predicting outcomes in uncertain scenarios where variables can take on any value within a range, rather than being limited to distinct, countable outcomes. These distributions are widely used in fields such as engineering, economics, natural sciences, and machine learning to model phenomena like time to failure, stock prices, rainfall amounts, or human heights. By mastering continuous probability distributions, analysts and decision-makers can assess uncertainties, estimate probabilities of events, and develop data-driven strategies to address real-world challenges. Below, we introduce some popular continuous distributions and their practical applications.\nContinuous random variables are characterized by their probability density function \\(f(x)\\). The probability density function does not directly provide probabilities!\nThe probability of a continuous random variable assuming a single value is zero. Instead, probabilities are defined for intervals. These are calculated by areas under the PDF curve (integral).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Probability III</span>"
    ]
  },
  {
    "objectID": "ProbabilityIII.html#concepts",
    "href": "ProbabilityIII.html#concepts",
    "title": "10  Probability III",
    "section": "",
    "text": "Continuous Random Variables\nContinuous random variables are characterized by their probability density function \\(f(x)\\). The probability density function does not directly provide probabilities!\nThe probability of a continuous random variable assuming a single value is zero. Instead, probabilities are defined for intervals. These are calculated by areas under the PDF curve (integral).\n\n\nUniform Distribution\nThe uniform probability density function is given by \\(f(x)= \\frac {1}{b-a}\\) when \\(a \\leq x \\leq b\\) and \\(0\\) otherwise.\nThe expected value of the uniform distribution is \\(E(x)= \\frac {a+b}{2}\\).\nThe variance of the uniform distribution is \\(var(x)= \\frac {(b-a)^2} {12}\\)\n\n\nNormal Distribution\nThe normal PDF is given by \\(f(x)= \\frac {1}{\\sigma \\sqrt{2\\pi}} e^{\\frac {-1}{2} (\\frac {x-\\mu}{\\sigma})}\\), where \\(\\mu\\) is the mean, \\(\\sigma\\) is the standard deviation, \\(\\pi\\) is 3.1415… , and \\(e\\) is 2.7282… . The normal distribution has the following properties:\n\nThe normal curve is symmetrical about the mean \\(\\mu\\).\nThe mean is at the middle and divides the area of the distribution into halves.\nThe total area under the curve is equal to 1.\nThe distribution is completely determined by its mean and standard deviation.\n\nThe standard normal distribution has a mean of \\(0\\) and a standard deviation of \\(1\\).\n\n\nExponential Distribution\nThe exponential distribution is useful in computing probabilities for the time it takes to complete a task. It describes the time between events in a Poisson process.\nThe probability density function is given by \\(f(x)=\\frac {1}{\\mu}e^{ \\frac {-x}{\\mu}}\\).\n\n\nTriangular Distribution\nThe triangular distribution is characterized by a single mode (the peak of the distribution) and two boundaries. It is often used in situations where the lower and upper bounds of a potential outcome are known, but the exact likelihood of the outcome is uncertain.\nThe probability density function is given by \\(f(x)=\\frac {2(x-a)}{(b-a)(c-a)}\\) for \\(a \\leq x &lt; c\\); \\(f(x)=\\frac {2}{(b-a)}\\) for \\(x=c\\); \\(f(x)=\\frac {2(b-x)}{(b-a)(b-c)}\\) for \\(c &lt; x \\leq b\\), and \\(f(x)=0\\) otherwise.\nThe expected value of the distribution is \\(E(x)= \\frac {a+b+c}{3}\\).\nThe variance of the triangular distribution is \\(var(x) = \\frac {a^2+b^2+c^2-ab-ac-bc}{18}\\).\n\n\nUseful R Functions\nTo calculate the density of continuous random variables use the dunif(), dnorm(), and dexp() functions. For the triangular distribution use the extraDistr package and the dtriang() function.\nTo calculate probabilities of continuous random variables use the punif(), pnorm(), pexp(), and ptriang() functions.\nTo calculate quartiles of continuous random variables use the qunif(), qnorm(),qexp(), and qtriang() functions.\nTo calculate generate random variables based on continuous random variables use the runif(), rnorm(), rexp(), and rtriang() functions.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Probability III</span>"
    ]
  },
  {
    "objectID": "ProbabilityIII.html#exercises",
    "href": "ProbabilityIII.html#exercises",
    "title": "10  Probability III",
    "section": "10.7 Exercises",
    "text": "10.7 Exercises\nThe following exercises will help you practice some probability concepts and formulas. In particular, the exercises work on:\n\nCalculating probabilities for continuous random variables.\nCalculating the expected value and standard deviation.\nApplying the uniform, normal, and exponential distributions.\n\nAnswers are provided below. Try not to peak until you have a formulated your own answer and double checked your work for any mistakes.\n\nExercise 1\nFor the following exercises, make your calculations by hand and verify results with a calculator or R.\n\nA random variable \\(X\\) follows a continuous uniform distribution with minimum of \\(-2\\) and maximum of \\(4\\). Determine the height of the density function \\(f(x)\\), the mean, the standard deviation, and calculate \\(P(X \\leq -1)\\).\n\n\n\nAnswer\n\nThe height of the density function \\(f(x)=0.1667\\), the mean is \\(1\\), standard deviation is \\(1.73\\), and \\(P(X \\leq -1)=0.1667\\).\n\\(f(x)\\) can be easily estimated by using the formula of the continuous uniform random variable. \\(f(x)=\\frac{1}{b-a}\\). Using R as a calculator we find:\n\n1/(4-(-2))\n\n[1] 0.1666667\n\n\nThe mean is given by \\(\\mu = \\frac{a+b}{2}\\). In R we determine that the mean is:\n\n(-2+4)/2\n\n[1] 1\n\n\nThe standard deviation is \\(\\sigma = \\sqrt {\\frac{(b-a)^2}{12}}\\). Using R we find:\n\nsqrt((4-(-2))^2/12)\n\n[1] 1.732051\n\n\nFinally, we can find the probability of \\(Z\\) being less than \\(-1\\) by using the punif() function:\n\npunif(-1,-2,4)\n\n[1] 0.1666667\n\n\n\n\nYour internet provider will arrive sometime between 10:00 am and 12:00 pm. Suppose you have to run a quick errand at 10:00 am. If it takes \\(15\\) minutes to run the errand, what is the probability that you will be back before the internet provider arrives? What if you take \\(30\\) minutes?\n\n\n\nAnswer\n\nThe probability that you will arrive on time is \\(0.875\\). If the time of the errand is 30 minutes, then the probability goes down to \\(0.75\\).\nThere is a \\(120\\) minute interval in which the IP can arrive. The density function is given by \\(f(x)=1/120\\). Using R we can find \\(P(X&gt;15)\\):\n\npunif(15,0,120,lower.tail=F)\n\n[1] 0.875\n\n\nOnce more we can find \\(P(X&gt;30)\\):\n\npunif(30,0,120,lower.tail=F)\n\n[1] 0.75\n\n\n\n\n\nExercise 2\n\nA random variable \\(Z\\) follows a standard normal distribution. Find \\(P(-0.67 \\leq Z \\leq -0.23)\\), \\(P(0 \\leq Z \\leq 1.96)\\), \\(P(-1.28 \\leq Z \\leq 0)\\) and \\(P(Z &gt; 4.2)\\).\n\n\n\nAnswer\n\n\\(P(-0.67 \\leq Z \\leq -0.23)=0.158\\), \\(P(0 \\leq Z \\leq 1.96)=0.475\\), \\(P(-1.28 \\leq Z \\leq 0)=0.4\\) and \\(P(Z &gt; 4.2) \\approx 0\\).\nUse the pnorm() function to find the probabilities. \\(P(-0.67 \\leq Z \\leq -0.23)\\):\n\npnorm(-0.23)-pnorm(-0.67)\n\n[1] 0.157617\n\n\n\\(P(0 \\leq Z \\leq 1.96)\\)\n\npnorm(1.96)-pnorm(0)\n\n[1] 0.4750021\n\n\n\\(P(-1.28 \\leq Z \\leq 0)\\)\n\npnorm(0)-pnorm(-1.28)\n\n[1] 0.3997274\n\n\n\\(P(Z &gt; 4.2)\\)\n\noptions(scipen=999)\npnorm(4.2,lower.tail = F)\n\n[1] 0.00001334575\n\n\n\n\nLet \\(Y\\) be normally distributed with \\(\\mu=2.5\\) and \\(\\sigma=2\\). Find \\(P(Y&gt;7.6)\\), \\(P(7.4 \\leq Y \\leq 10.6)\\), a \\(y\\) such that \\(P(Y&gt;y)=0.025\\), and a \\(y\\) such that \\(P(y \\leq Y \\leq 2.5)=0.4943\\).\n\n\n\nAnswer\n\n\\(P(Y&gt;7.6)=0.005386\\), \\(P(7.4 \\leq Y \\leq 10.6)=0.0071\\), a \\(y\\) such that \\(P(Y&gt;y)=0.025\\) is \\(6.42\\), and a \\(y\\) such that \\(P(y \\leq Y \\leq 2.5)\\) is \\(-2.56\\).\nLet’s use once more the pnorm() function in R.\n\\(P(Y&gt;7.6)\\)\n\npnorm(7.6,2.5,2,lower.tail = F)\n\n[1] 0.005386146\n\n\n\\(P(7.4 \\leq Y \\leq 10.6)\\)\n\npnorm(10.6,2.5,2)-pnorm(7.4,2.5,2)\n\n[1] 0.007117202\n\n\n\\(y\\) such that \\(P(Y&gt;y)=0.025\\)\n\nqnorm(0.025,2.5,2,lower.tail = F)\n\n[1] 6.419928\n\n\n\\(y\\) such that \\(P(y \\leq Y \\leq 2.5)=0.4943\\). Note that \\(2.5\\) is the mean. Hence we are looking for a \\(y\\) that has \\(0.5-0.4943=0.0057\\) on the left:\n\nqnorm(0.0057,2.5,2)\n\n[1] -2.560385\n\n\n\n\nAssume that football game times are normally distributed with a mean of \\(3\\) hours and a standard deviation of \\(0.4\\) hour. What is the probability that the game lasts at most \\(2.5\\) hours? Find the maximum value for a game to be in the bottom \\(1\\)% of the distribution.\n\n\n\nAnswer\n\nThe probability is \\(10.56\\)%. A game lasting no more than \\(2.069\\) hours would be in the bottom \\(1\\)%.\nLet’s use pnorm() once more in R.\n\npnorm(2.5,3,0.4)\n\n[1] 0.1056498\n\n\nFor the threshold we can use qnorm():\n\nqnorm(0.01,3,0.4)\n\n[1] 2.069461\n\n\n\n\n\nExercise 3\n\nRandom variable \\(S\\) is exponentially distributed with mean of \\(0.1\\). What is the standard deviation of \\(S\\)? What is \\(P(0.10 \\leq S \\leq 0.2)\\)?\n\n\n\nAnswer\n\nThe standard deviation is equal to the mean \\(0.1\\). \\(P(0.10 \\leq S \\leq 0.2)=0.2325\\)\nLet’s use pexp() in R:\n\npexp(0.2,rate = 10)-pexp(0.1,rate = 10)\n\n[1] 0.2325442\n\n\n\n\nA tollbooth operator has observed that cars arrive randomly at a rate of \\(360\\) cars per hour. What is the mean time between car arrivals? What is the probability that the next car will arrive within ten seconds?\n\n\n\nAnswer\n\nThe mean time between car arrivals is \\(1/360=0.002778\\). The probability that the next car will arrive within the next 10 seconds is \\(0.6321\\).\nOnce more we use pexp() in R:\n\npexp(1/360,360)\n\n[1] 0.6321206",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Probability III</span>"
    ]
  },
  {
    "objectID": "ProbabilityIII.html#answers",
    "href": "ProbabilityIII.html#answers",
    "title": "10  Probability III",
    "section": "10.3 Answers",
    "text": "10.3 Answers\n\nExercise 1\n\nThe height of the density function \\(f(x)=0.1667\\), the mean is \\(1\\), standard deviation is \\(1.73\\), and \\(P(X \\leq -1)=0.1667\\).\n\n\\(f(x)\\) can be easily estimated by using the formula of the continuous uniform random variable. \\(f(x)=\\frac{1}{b-a}\\). Using R as a calculator we find:\n\n1/(4-(-2))\n\n[1] 0.1666667\n\n\nThe mean is given by \\(\\mu = \\frac{a+b}{2}\\). In R we determine that the mean is:\n\n(-2+4)/2\n\n[1] 1\n\n\nThe standard deviation is \\(\\sigma = \\sqrt {\\frac{(b-a)^2}{12}}\\). Using R we find:\n\nsqrt((4-(-2))^2/12)\n\n[1] 1.732051\n\n\nFinally, we can find the probability of \\(Z\\) being less than \\(-1\\) by using the punif() function:\n\npunif(-1,-2,4)\n\n[1] 0.1666667\n\n\n\nThe probability that you will arrive on time is \\(0.875\\). If the time of the errand is 30 minutes, then the probability goes down to \\(0.75\\).\n\nThere is a \\(120\\) minute interval in which the IP can arrive. The density function is given by \\(f(x)=1/120\\). Using R we can find \\(P(X&gt;15)\\):\n\npunif(15,0,120,lower.tail=F)\n\n[1] 0.875\n\n\nOnce more we can find \\(P(X&gt;30)\\):\n\npunif(30,0,120,lower.tail=F)\n\n[1] 0.75\n\n\n\n\nExercise 2\n\n\\(P(-0.67 \\leq Z \\leq -0.23)=0.158\\), \\(P(0 \\leq Z \\leq 1.96)=0.475\\), \\(P(-1.28 \\leq Z \\leq 0)=0.4\\) and \\(P(Z &gt; 4.2) \\approx 0\\).\n\nUse the pnorm() function to find the probabilities. \\(P(-0.67 \\leq Z \\leq -0.23)\\):\n\npnorm(-0.23)-pnorm(-0.67)\n\n[1] 0.157617\n\n\n\\(P(0 \\leq Z \\leq 1.96)\\)\n\npnorm(1.96)-pnorm(0)\n\n[1] 0.4750021\n\n\n\\(P(-1.28 \\leq Z \\leq 0)\\)\n\npnorm(0)-pnorm(-1.28)\n\n[1] 0.3997274\n\n\n\\(P(Z &gt; 4.2)\\)\n\noptions(scipen=999)\npnorm(4.2,lower.tail = F)\n\n[1] 0.00001334575\n\n\n\n\\(P(Y&gt;7.6)=0.005386\\), \\(P(7.4 \\leq Y \\leq 10.6)=0.0071\\), a \\(y\\) such that \\(P(Y&gt;y)=0.025\\) is \\(6.42\\), and a \\(y\\) such that \\(P(y \\leq Y \\leq 2.5)\\) is \\(-2.56\\).\n\nLet’s use once more the pnorm() function in R.\n\\(P(Y&gt;7.6)\\)\n\npnorm(7.6,2.5,2,lower.tail = F)\n\n[1] 0.005386146\n\n\n\\(P(7.4 \\leq Y \\leq 10.6)\\)\n\npnorm(10.6,2.5,2)-pnorm(7.4,2.5,2)\n\n[1] 0.007117202\n\n\n\\(y\\) such that \\(P(Y&gt;y)=0.025\\)\n\nqnorm(0.025,2.5,2,lower.tail = F)\n\n[1] 6.419928\n\n\n\\(y\\) such that \\(P(y \\leq Y \\leq 2.5)=0.4943\\). Note that \\(2.5\\) is the mean. Hence we are looking for a \\(y\\) that has \\(0.5-0.4943=0.0057\\) on the left:\n\nqnorm(0.0057,2.5,2)\n\n[1] -2.560385\n\n\n\nThe probability is \\(10.56\\)%. A game lasting no more than \\(2.069\\) hours would be in the bottom \\(1\\)%.\n\nLet’s use pnorm() once more in R.\n\npnorm(2.5,3,0.4)\n\n[1] 0.1056498\n\n\nFor the threshold we can use qnorm()\n\nqnorm(0.01,3,0.4)\n\n[1] 2.069461\n\n\n\n\nExercise 3\n\nThe standard deviation is equal to the mean \\(0.1\\). \\(P(0.10 \\leq S \\leq 0.2)=0.2325\\)\n\nLet’s use pexp() in R:\n\npexp(0.2,rate = 10)-pexp(0.1,rate = 10)\n\n[1] 0.2325442\n\n\n\nThe mean time between car arrivals is \\(1/360=0.002778\\). The probability that the next car will arrive within the next 10 seconds is \\(0.6321\\).\n\nOnce more we use pexp() in R\n\npexp(1/360,360)\n\n[1] 0.6321206",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Probability III</span>"
    ]
  },
  {
    "objectID": "InferenceI.html",
    "href": "InferenceI.html",
    "title": "11  Inference I",
    "section": "",
    "text": "11.1 Statistical Inference\nStatistical inference is a cornerstone of modern business decision-making. It enables companies to extract meaningful insights from data amid uncertainty and complexity. Businesses can predict customer behavior, optimize marketing strategies, and assess risks by drawing reliable conclusions from sample data. In this chapter, we explore how we can draw insights from the population using sample data.\nThe power of statistical inference lies in allowing us to learn about a population by studying the properties of the sample. The process relies on estimating a sample statistic, like the sample mean (i.e., the sample statistic). The sample mean gives us a good guess about the population mean (i.e., the population parameter), since it comes from a fair and random selection. Some of the properties of the sample mean are as follows:\nIn conclusion, with a sufficiently large random sample, the sample mean follows a normal distribution—regardless of the population’s distribution—with a mean equal to the population mean and a standard deviation smaller than that of the population.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Inference I</span>"
    ]
  },
  {
    "objectID": "InferenceI.html#concepts",
    "href": "InferenceI.html#concepts",
    "title": "11  Inference I",
    "section": "",
    "text": "Statistical Inference\nThe goal of statistical inference is gain insight on a population parameter by using a sample statistic. It is required that the sample statistic be calculated from a random sample from the population where each element is selected independently.\nA sample mean is used to infer the population mean. Some properties of the sample mean are:\n\nThe expected value of the sample means is equal to the population mean (i.e., the sample mean is unbiased). Formally, \\(E(\\bar x_i) = \\mu\\).\nThe standard deviation of the sample means is lower than the population standard deviation. \\(\\sigma_{\\bar x}= \\sigma/\\sqrt{n}\\). We call this measure the standard error.\nIf the population is normally distributed, then the sample means (\\(\\bar x\\)’s) are normally distributed.\nIf the population is not normally distributed, the the sample means are also normally distributed if the sample size is large (i.e., \\(n&gt;30\\)). This is the central limit theorem.\n\n\n\nProportions\nRecall that the binomial distribution describes the number of successes \\(x\\) in \\(n\\) trials of a Bernoulli process where \\(p\\) is the probability of success. Here, \\(x/n\\) is the proportion of successes.\n\nTo estimate the population proportion use the sample proportion \\(\\bar p = x/n\\). This estimate is unbiased (i.e., \\(E(\\bar p)=P\\)), where \\(P\\) is the population proportion.\nThe standard error of the estimate is \\(se(\\bar P)= \\sqrt { \\frac {p(1-p)}{n}}\\), where \\(p\\) is the sample proportion, and \\(n\\) is the sample size.\nBy the central limit theorem, the sampling distribution of \\(\\bar p\\) is approximately normal when \\(np \\geq 5\\) and \\(n(1-p)\\geq 5\\).\n\n\n\nUseful R Functions\nHere are some functions that are handy when simulating data in R.\nThe pnorm() and punif() functions calculate probabilities for the normal and uniform distributions, respectively.\nThe rnorm() and runif() functions generate random numbers from a normal and uniform distribution, respectively.\nThe for() function creates a loop that repeats a procedure a specified amount of times.\nThe set.seed() function is used to create reproducible results in R when random numbers are used.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Inference I</span>"
    ]
  },
  {
    "objectID": "InferenceI.html#exercises",
    "href": "InferenceI.html#exercises",
    "title": "11  Inference I",
    "section": "11.4 Exercises",
    "text": "11.4 Exercises\nThe following exercises will help you test your knowledge on the Inference. In particular, the exercises work on:\n\nThe Central Limit Theorem.\nSampling Distribution for means.\nSampling Distribution for proportions.\n\nAnswers are provided below. Try not to peak until you have a formulated your own answer and double checked your work for any mistakes.\n\nExercise 1\nIn this exercise we will be simulating the central limit theorem. You will need R to complete this problem.\n\nCreate a random sample of 1000 data points and store it in an object called Population. Use the uniform distribution with min of 100 and max of 200 to generate the sample. Calculate the mean and standard deviation of the random sample and call PopMean and PopSD, respectively.\n\n\n\nAnswer\n\nLet’s start by creating the random sample. We can use the runif() function in R to do this. We will set a seed so that results are reproducible.\n\nset.seed(15)\nPopulation&lt;-runif(1000,100,200)\n\nNext, we can save the mean and the standard deviation of the population in two different object:\n\nPopMean&lt;-mean(Population)\nPopSD&lt;-sd(Population)\n\nThe mean and standard deviation are \\(150.53\\) and \\(29.2\\). Let’s quickly create a histogram of population, so that we can convince ourselves that the data is uniformly distributed.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggthemes)\n\nggplot() + \n  geom_histogram(aes(Population),col=\"black\",\n                 bg=\"#F5F5F5\", bins=10, \n                 boundary=100, binwidth = 10) +\n  theme_clean()\n\n\n\n\n\n\n\n\n\n\nCreate a for loop (with 1000 iterations) that takes a sample of 10 points from population, calculate the mean, and then store the result in a vector called SampleMeans. Calculate the mean of the SampleMeans object. How does this mean compare to PopMean? How does the standard deviation compare to PopSD?\n\n\n\nAnswer\n\nNow let’s create a for loop that allows us to sample the population several times. In fact, we will sample the population 1000 times and record the mean of the samples.\n\nnrep&lt;-1000\nSampleMeans&lt;-c()\nfor (i in 1:nrep){\n  x&lt;-sample(Population,10,replace=T)\n  SampleMeans&lt;-c(SampleMeans,mean(x))\n}\n\nNow we can calculate the mean of the sample means in R:\n\nmean(SampleMeans)\n\n[1] 151.4461\n\n\nNote that the mean is very close to PopMean. In the limit (that is if we take many more samples), these two values are equal to each other. Now let’s calculate the standard deviation of the sample means.\n\nsd(SampleMeans)\n\n[1] 9.364747\n\n\nAs you can see, the standard deviation is much lower. In fact, if we take PopSD and divide by 10 (the size of the sample), we should get close to the standard deviation of the sample means.\n\nPopSD/sqrt(10)\n\n[1] 9.172602\n\n\n\n\nCreate a histogram for the sample means. Is the distribution uniform? Is it normal? What is the probability that the sample mean is between 140 and 160?\n\n\n\nAnswer\n\nTo create the histogram we use the geom_histogram() function once more:\n\nggplot() + \n  geom_histogram(aes(SampleMeans),bg=\"#F5F5F5\",\n                 col=\"black\", bins=10) +\n  theme_clean()\n\n\n\n\n\n\n\n\nThe distribution looks normal. To be clear, if the population follows a uniform distribution, we have shown that the distribution of the sample means is normal with a mean equal to the population mean and a smaller standard deviation. We can use the distribution of the sample means to calculate the desired probability. Noting the the distribution is normal:\n\npnorm(160,mean(SampleMeans),sd(SampleMeans))-pnorm(140,mean(SampleMeans),sd(SampleMeans))\n\n[1] 0.708682\n\n\nThere is a \\(70.87\\)% probability that the sample mean is between \\(140\\) and \\(160\\).\n\n\n\nExercise 2\n\nA random sample of \\(n=100\\) is taken from a population with mean \\(\\mu=80\\) and standard deviation \\(\\sigma=14\\). Calculate the expected value and standard error for the sampling distribution of the sampling means. What is the probability that the sample mean falls between \\(77\\) and \\(85\\)?\n\n\n\nAnswer\n\nThe expected value is \\(80\\) since it is equal to the mean of the population. The standard error is \\(1.4\\). The probability is \\(98.38\\)%.\nWe can use R as a calculator to find the standard error.\n\n14/sqrt(100)\n\n[1] 1.4\n\n\nWe can use pnorm() to find the probability:\n\npnorm(85,80,1.4)-pnorm(77,80,1.4)\n\n[1] 0.9837602\n\n\n\n\nAssume that miles-per-gallons of combustion cars are normally distributed with mean of \\(33.8\\) and standard deviation of \\(3.5\\). What is the probability that the mean mpg of four randomly selected cars is more than \\(35\\)? What is the probability that all four selected cars have mpg greater than \\(35\\)?\n\n\n\nAnswer\n\nThe probabilities are \\(24.66\\)% and \\(1.8\\)%.\nFor the first probability we can use a sample size of \\(4\\) and use the standard error in the pnorm() function.\n\npnorm(35,33.8,3.5/sqrt(4),lower.tail = F)\n\n[1] 0.2464466\n\n\nFor the second probability we can first calculate the probability that a randomly selected car has mpg greater than \\(35\\). In R:\n\n(p35&lt;-pnorm(35,33.8,3.5,lower.tail = F))\n\n[1] 0.365853\n\n\nSince draws are independent we get:\n\np35^4\n\n[1] 0.01791539\n\n\n\n\n\nExercise 3\n\nA random sample of \\(n=200\\) is taken from a population with a proportion of \\(p=0.75\\). Calculate the expected value and standard error of the proportion sampling distribution. What is the probability that the sample proportion is between \\(0.7\\) and \\(0.8\\)?\n\n\n\nAnswer\n\nThe expected value is \\(0.75\\), the same as the population. The standard error is \\(\\sqrt{p(1-p)/n}=0.03\\). The probability for a sample of \\(200\\) is \\(0.8975\\).\nThe standard error is given by:\n\nsqrt(0.75*0.25/200)\n\n[1] 0.03061862\n\n\nIn R we can use the pnorm() function one more time to find the probability.\n\npnorm(0.8,0.75,sqrt(0.75*0.25/200))-pnorm(0.7,0.75,sqrt(0.75*0.25/200))\n\n[1] 0.8975296\n\n\n\n2.Twenty-three percent of employees at a fintech firm work from home. If we take a sample of 50 employees, what is the probability that more than 20% of them are working from home? What if the sample increases to 200? Why does the probability change?\n\n\nAnswer\n\nThe probability with a sample of \\(50\\) is \\(69.29\\)%. When the sample is \\(200\\) the probability is \\(84.33\\)%. As the sample size increases the standard error goes down. This means that the distribution of the sample proportions gets tighter and there is more area to the right of \\(\\bar{p}=0.2\\).\nIn R we can use the pnorm() function one more time with a mean of \\(0.2\\) and \\(n=50\\).\n\npnorm(0.2,0.23,sqrt(0.23*0.77/50),lower.tail = F)\n\n[1] 0.6928964\n\n\nUpdating the code so that \\(n=200\\) yields:\n\npnorm(0.2,0.23,sqrt(0.23*0.77/200),lower.tail = F)\n\n[1] 0.8433098\n\n\n\n\n\nExercise 4\n\nA production process for energy drinks is being evaluated. The machine that fills the cans is calibrated so that each can has \\(350\\)ml of drink with a standard deviation of \\(10\\)ml. Every hour, ten cans are sampled and the average amount of drink is recorded (see table below). Is the machine working properly?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\n\\(\\bar{x}=310\\)\n\\(\\bar{x}=315\\)\n\\(\\bar{x}=325\\)\n\\(\\bar{x}=330\\)\n\\(\\bar{x}=328\\)\n\\(\\bar{x}=347\\)\n\\(\\bar{x}=339\\)\n\\(\\bar{x}=350\\)\n\n\n\n\n\nAnswer\n\nThe process seems to be out of control. In the early samples, the machine is not filling the cans with enough drink. Although, in the later periods the machine reverts back to the expected performance, it seems unlikely that it will remain functioning correctly.\nLet’s start by calculating the upper and lower limits in R.\n\ndataEx1&lt;-c(310,315,325,330,328,347,339,350)\nulEx1&lt;-350+3*(10/sqrt(10))\nllEx1&lt;-350-3*(10/sqrt(10))\n\nWe can graph the samples and the limits to determine the stability of the production process.\n\nggplot() +\n  geom_point(aes(y=dataEx1, x=1:8), pch=21,\n             bg=\"blue\", alpha=0.3)+\n  geom_hline(yintercept = ulEx1, col=\"red\", lty=2) +\n  geom_hline(yintercept = llEx1, col=\"red\",lty=2) +\n  theme_clean() +\n  labs(y=\"Volume (ml)\", x=\"Sample\")\n\n\n\n\n\n\n\n\n\n\nThe production of Good Guy dolls has a \\(1\\)% defective rate. A quality inspector takes five samples of size \\(1000\\). The proportions are shown in the table below. Is the production process under control?\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n\n\n\n\n\\(\\bar{p}=0.009\\)\n\\(\\bar{p}=0.012\\)\n\\(\\bar{p}=0.008\\)\n\\(\\bar{p}=0.011\\)\n\\(\\bar{p}=0.0102\\)\n\n\n\n\n\nAnswer\n\nGood Dolls production looks good. All proportions fall between three standard errors of the mean.\nOnce more we can calculate upper and lower limits for the proportions.\n\ndataEx2&lt;-c(0.009,0.012,0.008,0.011,0.0102)\nulEx2&lt;-0.01+3*sqrt(0.01*0.99/1000)\nllEx2&lt;-0.01-3*sqrt(0.01*0.99/1000)\n\nGraphing the results in R we can observe the production process and the sample proportions.\n\nggplot() +\n  geom_point(aes(y=dataEx2, x=1:5), pch=21,\n             bg=\"blue\", alpha=0.3)+\n  geom_hline(yintercept = ulEx2, col=\"red\", lty=2) +\n  geom_hline(yintercept = llEx2, col=\"red\",lty=2) +\n  theme_clean() +\n  labs(y=\"Defective Rate\", x=\"Sample\")",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Inference I</span>"
    ]
  },
  {
    "objectID": "InferenceI.html#answers",
    "href": "InferenceI.html#answers",
    "title": "11  Inference I",
    "section": "11.3 Answers",
    "text": "11.3 Answers\n\nExercise 1\nLet’s start by creating the random sample. We can use the runif() function in R to do this. We will set a seed so that results are reproducible.\n\nset.seed(10)\nPopulation&lt;-runif(1000,100,200)\n\nNext, we can save the mean and the standard deviation of the population in two different object:\n\nPopMean&lt;-mean(Population)\nPopSD&lt;-sd(Population)\n\nThe mean and standard deviation are \\(150.53\\) and \\(29.2\\). Let’s quickly create a histogram of population, so that we can convince ourselves that the data is uniformly distributed.\n\nhist(Population, main=\"\", ylim=c(0,160), col=\"#F5F5F5\")\n\n\n\n\n\n\n\n\n\nNow let’s create a for loop that allows us to sample the population several times. In fact, we will sample the population 1000 times and record the mean of the samples.\n\n\nnrep&lt;-1000\nSampleMeans&lt;-c()\nfor (i in 1:nrep){\n  x&lt;-sample(Population,10,replace=T)\n  SampleMeans&lt;-c(SampleMeans,mean(x))\n}\n\nNow we can calculate the mean of the sample means in R:\n\nmean(SampleMeans)\n\n[1] 150.4177\n\n\nNote that the mean is very close to PopMean. In the limit (that is if we take many more samples), these two values are equal to each other. Now let’s calculate the standard deviation of the sample means.\n\nsd(SampleMeans)\n\n[1] 9.134147\n\n\nAs you can see, the standard deviation is much lower. In fact, if we take PopSD and divide by 10 (the size of the sample), we should get close to the standard deviation of the sample means.\n\nPopSD/sqrt(10)\n\n[1] 9.233644\n\n\n\nTo create the histogram we use the hist() function once more:\n\n\nhist(SampleMeans, main=\"\", ylim=c(0,300), col=\"#F5F5F5\")\n\n\n\n\n\n\n\n\nThe distribution looks normal. To be clear, if the population follows a uniform distribution, we have shown that the distribution of the sample means is normal with a mean equal to the population mean and a smaller standard deviation.\nWe can use the distribution of the sample means to calculate the probability. Noting the the distribution is normal:\n\npnorm(160,mean(SampleMeans),sd(SampleMeans))-pnorm(140,mean(SampleMeans),sd(SampleMeans))\n\n[1] 0.7258913\n\n\nThere is a \\(72.59\\)% probability that the sample mean is between \\(140\\) and \\(160\\).\n\n\nExercise 2\n\nThe expected value is \\(80\\) since it is equal to the mean of the population. The standard error is \\(1.4\\). The probability is \\(98.38\\)%.\n\nWe can use R as a calculator to find the standard error.\n\n14/sqrt(100)\n\n[1] 1.4\n\n\nWe can use pnorm() to find the probability:\n\npnorm(85,80,1.4)-pnorm(77,80,1.4)\n\n[1] 0.9837602\n\n\n\nThe probabilities are \\(24.66\\)% and \\(1.8\\)%.\n\nFor the first probability we can use a sample size of \\(4\\) and use the standard error in the pnorm() function.\n\npnorm(35,33.8,3.5/sqrt(4),lower.tail = F)\n\n[1] 0.2464466\n\n\nFor the second probability we can first calculate the probability that a randomly selected car has mpg greater than \\(35\\). In R:\n\n(p35&lt;-pnorm(35,33.8,3.5,lower.tail = F))\n\n[1] 0.365853\n\n\nSince draws are independent we get:\n\np35^4\n\n[1] 0.01791539\n\n\n\n\nExercise 3\n\nThe expected value is \\(0.75\\), the same as the population. The standard error is \\(\\sqrt{p(1-p)/n}=0.03\\). The probability for a sample of \\(200\\) is \\(0.8975\\).\n\nThe standard error is given by:\n\nsqrt(0.75*0.25/200)\n\n[1] 0.03061862\n\n\nIn R we can use the pnorm() function one more time to find the probability.\n\npnorm(0.8,0.75,sqrt(0.75*0.25/200))-pnorm(0.7,0.75,sqrt(0.75*0.25/200))\n\n[1] 0.8975296\n\n\n\nThe probability with a sample of \\(50\\) is \\(69.29\\)%. When the sample is \\(200\\) the probability is \\(84.33\\)%. As the sample size increases the standard error goes down. This means that the distribution of the sample proportions gets tighter and there is more area to the right of \\(\\bar{p}=0.2\\).\n\nIn R we can use the pnorm() function one more time with a mean of \\(0.2\\) and \\(n=50\\).\n\npnorm(0.2,0.23,sqrt(0.23*0.77/50),lower.tail = F)\n\n[1] 0.6928964\n\n\nUpdating the code so that \\(n=200\\) yields:\n\npnorm(0.2,0.23,sqrt(0.23*0.77/200),lower.tail = F)\n\n[1] 0.8433098\n\n\n\n\nExercise 4\n\nThe process seems to be out of control. In the early samples, the machine is not filling the cans with enough drink. Although, in the later periods the machine reverts back to the expected performance, it seems unlikely that it will remain functioning correctly.\n\nLet’s start by calculating the upper and lower limits in R.\n\ndataEx1&lt;-c(310,315,325,330,328,347,339,350)\nulEx1&lt;-350+3*(10/sqrt(10))\nllEx1&lt;-350-3*(10/sqrt(10))\n\nWe can graph the samples and the limits to determine the stability of the production process.\n\nplot(dataEx1, type=\"b\", ylab=\"Mean Gallons\",\n     xlab=\"Period\", pch=21, bg=\"blue\",ylim=c(280,380))\nabline(h=ulEx1,col=\"red\")\nabline(h=llEx1,col=\"red\")\n\n\n\n\n\n\n\n\n\nGood Dolls production looks good. All proportions fall between three standard errors of the mean.\n\nOnce more we can calculate upper and lower limits for the proportions.\n\ndataEx2&lt;-c(0.009,0.012,0.008,0.011,0.0102)\nulEx2&lt;-0.01+3*sqrt(0.01*0.99/1000)\nllEx2&lt;-0.01-3*sqrt(0.01*0.99/1000)\n\nGraphing the results in R we can observe the production process and the sample proportions.\n\nplot(dataEx2, type=\"b\", ylab=\"Proportion Defective\",\n     xlab=\"Period\", pch=21, bg=\"blue\",ylim=c(0,0.02))\nabline(h=ulEx2,col=\"red\")\nabline(h=llEx2,col=\"red\")",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Inference I</span>"
    ]
  },
  {
    "objectID": "InferenceII.html",
    "href": "InferenceII.html",
    "title": "12  Inference II",
    "section": "",
    "text": "12.1 Confidence Intervals\nConfidence intervals are a powerful tool for making sense of data and drawing reliable conclusions about a population. Whether you’re estimating a population mean or proportion, confidence intervals provide a range of values that give you a sense of certainty—backed by statistical rigor—about where the true parameter lies. By mastering this concept, you’ll gain the ability to quantify uncertainty, assess the reliability of your estimates, and make informed decisions based on data. The information here equips you with the foundational formulas, key concepts like confidence levels and significance, and practical R functions to confidently apply these techniques in real-world scenarios.\nA confidence interval provides a range of values that, with a certain level of confidence, contains the population parameter of interest. For proper confidence intervals ensure that the sampling distributions are normal.\nA \\(95\\)% confidence level, indicates that if the interval were constructed many times (from independent samples of the population), it would include the true population parameter \\(95\\)% of the time.\nA significance level (\\(\\alpha\\)) of \\(5\\)%, means that the confidence interval would would not include the true population parameter \\(5\\)% of the time.\nThe interval for the population mean when the population standard deviation is unknown is given by \\(\\bar x \\pm t_{\\alpha/2, df} \\frac {s}{\\sqrt{n}}\\), where \\(\\bar x\\) is the point estimate, \\(t_{a/2, df} \\frac {s}{\\sqrt{n}}\\) is the margin of error, \\(\\alpha\\) is the allowed probability that the interval does not include \\(\\mu\\), and \\(df\\) are the degrees of freedom \\(n-1\\).\nThe interval for the population proportion mean is given by \\(\\bar p \\pm z_{\\alpha/2} \\sqrt{\\frac {\\bar p (1-\\bar p)}{n}}\\).",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Inference II</span>"
    ]
  },
  {
    "objectID": "InferenceII.html#concepts",
    "href": "InferenceII.html#concepts",
    "title": "12  Inference II",
    "section": "",
    "text": "Confidence Intervals\nA confidence interval provides a range of values that, with a certain level of confidence, contains the population parameter of interest. For proper confidence intervals ensure that the sampling distributions are normal.\nA \\(95\\)% confidence level, indicates that if the interval were constructed many times (from independent samples of the population), it would include the true population parameter \\(95\\)% of the time.\nA significance level (\\(\\alpha\\)) of \\(5\\)%, means that the confidence interval would would not include the true population parameter \\(5\\)% of the time.\nThe interval for the population mean when the population standard deviation is unknown is given by \\(\\bar x \\pm t_{\\alpha/2, df} \\frac {s}{\\sqrt{n}}\\), where \\(\\bar x\\) is the point estimate, \\(t_{a/2, df} \\frac {s}{\\sqrt{n}}\\) is the margin of error, \\(\\alpha\\) is the allowed probability that the interval does not include \\(\\mu\\), and \\(df\\) are the degrees of freedom \\(n-1\\).\nThe interval for the population proportion mean is given by \\(\\bar p \\pm z_{\\alpha/2} \\sqrt{\\frac {\\bar p (1-\\bar p)}{n}}\\).\n\n\nUseful R Functions\nThe qnorm() and qt() functions calculate quartiles for the normal and \\(t\\) distributions, respectively.\nThe if() function creates a conditional statement in R.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Inference II</span>"
    ]
  },
  {
    "objectID": "InferenceII.html#exercises",
    "href": "InferenceII.html#exercises",
    "title": "12  Inference II",
    "section": "12.3 Exercises",
    "text": "12.3 Exercises\nThe following exercises will help you test your knowledge on Statistical Inference. In particular, the exercises work on:\n\nSimulating confidence intervals.\nEstimating confidence intervals in R.\nEstimating confidence intervals for proportions.\n\nTry not to peek at the answers until you have formulated your own answer and double-checked your work for any mistakes.\n\nExercise 1\nIn this exercise you will be simulating confidence intervals.\n\nSet the seed to \\(9\\). Create a random sample of 1000 data points and store it in an object called Population. Use the exponential distribution with rate of \\(0.02\\) to generate the data. Calculate the mean and standard deviation of Population and call them PopMean and PopSD respectively. What are the mean and standard deviation of Population?\n\n\nAnswer\n\nThe mean of Population is 48.61. The standard deviation is 47.94.\nStart by generating values from the exponential distribution. You can use the rexp() function in R to do this. Setting the seed to 9 yields:\n\nset.seed(9)\nPopulation &lt;- rexp(1000, 0.02)\n\nThe population mean is:\n\n(PopMean &lt;- mean(Population))\n\n[1] 48.61053\n\n\nThe standard deviation is:\n\n(PopSD &lt;- sd(Population))\n\n[1] 47.94411\n\n\n\nCreate a for loop (with 10,000 iterations) that takes a sample of 50 points from Population, calculates the mean, and then stores the result in a vector called SampleMeans. What is the mean of the SampleMeans?\n\n\nAnswer\n\nThe mean is very close to the population mean 48.83. The standard deviation is 6.83.\nIn R you can use a for loop to create the vector of sample means.\n\nnrep &lt;- 10000\nSampleMeans &lt;- c()\nfor (i in 1:nrep){\n  x &lt;- sample(Population, 50, replace = T)\n  SampleMeans &lt;- c(SampleMeans, mean(x))\n}\n\nThe mean of SampleMeans is:\n\n(xbar &lt;- mean(SampleMeans))\n\n[1] 48.7005\n\n\nThe standard deviation is:\n\n(Standard &lt;- sd(SampleMeans))\n\n[1] 6.827595\n\n\n\nCreate a \\(90\\)% confidence interval using the first data point in the SampleMeans vector. Does the confidence interval include PopMean?\n\n\nAnswer\n\nThe confidence interval is [47.71, 70.17]. Since the population mean is equal to 48.61, the confidence interval does include the population mean.\nLet’s construct the upper and lower limits of the interval in R.\n\n(ll &lt;- SampleMeans[1] + qnorm(0.05) * Standard)\n\n[1] 47.71385\n\n(ul &lt;- SampleMeans[1] - qnorm(0.05) * Standard)\n\n[1] 70.17464\n\n\n\nNow take the minimum of the SampleMeans vector. Create a new \\(90\\)% confidence interval. Does the interval include PopMean? Out of the \\(10,000\\) intervals that you could construct with the vector SampleMeans, how many would you expect to include PopMean?\n\n\nAnswer\n\nThe confidence interval is [14.86, 37.32]. This interval does not include the population mean of 48.61. Out of the 10,000 confidence intervals, one would expect about 9,000 to include the population mean.\nLet’s find the confidence interval limits using R.\n\n(Minll &lt;- min(SampleMeans) + qnorm(0.05) * Standard)\n\n[1] 14.85631\n\n(Minul &lt;- min(SampleMeans) - qnorm(0.05) * Standard)\n\n[1] 37.31709\n\n\nWe can confirm in R that about 9,000 of the intervals include PopMean. Once more, let’s use a for loop to construct confidence intervals for each element in SampleMeans and check whether the PopMean is included. The count variable keeps track of how many intervals include the population mean.\n\ncount = 0\n\nfor (i in SampleMeans){\n  (ll &lt;- i + qnorm(0.05) * Standard)\n  (ul &lt;- i - qnorm(0.05) * Standard)\n  if (PopMean &lt;= ul & PopMean &gt;= ll){\n    count = count + 1\n  }\n}\n\ncount\n\n[1] 8978\n\n\n\n\n\n\nExercise 2\n\nA random sample of \\(24\\) observations is used to estimate the population mean. The sample mean is \\(104.6\\) and the standard deviation is \\(28.8\\). The population is normally distributed. Construct a \\(90\\)% and \\(95\\)% confidence interval for the population mean. How does the confidence level affect the size of the interval?\n\n\nAnswer\n\nThe 90% confidence interval is [94.52, 114.67] and the 95% confidence interval is [92.68, 116.76]. The larger the confidence level, the larger the interval.\nLet’s construct the intervals using R. Since the population standard deviation is unknown we will use the t-distribution. The interval is constructed as ({x} t_{/2} ).\n\n(ul90 &lt;- 104.6 - qt(0.05, 23) * 28.8 / sqrt(24))\n\n[1] 114.6755\n\n(ll90 &lt;- 104.6 + qt(0.05, 23) * 28.8 / sqrt(24))\n\n[1] 94.52453\n\n\nFor the 95% confidence interval we adjust the significance level accordingly.\n\n(ul95 &lt;- 104.6 - qt(0.025, 23) * 28.8 / sqrt(24))\n\n[1] 116.7612\n\n(ll95 &lt;- 104.6 + qt(0.025, 23) * 28.8 / sqrt(24))\n\n[1] 92.43883\n\n\n\nA random sample from a normally distributed population yields a mean of \\(48.68\\) and a standard deviation of \\(33.64\\). Compute a \\(95\\)% confidence interval assuming a) that the sample size is \\(16\\) and b) the sample size is \\(25\\). What happens to the confidence interval as the sample size increases?\n\n\nAnswer\n\nThe confidence interval for a sample size of 16 is [30.75, 66.61]. The confidence interval when the sample size is 25 is [34.79, 62.57]. As the sample size gets larger, the confidence interval gets narrower and more precise.\nLet’s use R again to calculate the confidence interval. For a sample size of 16 the interval is:\n\n(ul16 &lt;- 48.68 - qt(0.025, 15) * 33.64 / sqrt(16))\n\n[1] 66.60549\n\n(ll16 &lt;- 48.68 + qt(0.025, 15) * 33.64 / sqrt(16))\n\n[1] 30.75451\n\n\nIncreasing the sample size to 25 yields:\n\n(ul25 &lt;- 48.68 - qt(0.025, 24) * 33.64 / sqrt(25))\n\n[1] 62.56591\n\n(ll25 &lt;- 48.68 + qt(0.025, 24) * 33.64 / sqrt(25))\n\n[1] 34.79409\n\n\n\n\n\n\nExercise 3\nYou will need the sleep data set for this problem. The data is built into R, and displays the effect of two sleep inducing drugs on students. Calculate a \\(95\\)% confidence interval for group 1 and for group 2. Which drug would you expect to be more effective at increasing sleeping times?\n\n\nAnswer\n\nThe 95% confidence interval for group 1 is [-0.53, 2.03]. Let’s first calculate the standard error for group 1.\n\n(se1 &lt;- sd(sleep$extra[sleep$group == 1]) / sqrt(length(sleep$extra[sleep$group == 1])))\n\n[1] 0.5657345\n\n\nWe can now use the standard error to estimate the lower and upper limits of the confidence interval.\n\n(ll1 &lt;- mean(sleep$extra[sleep$group == 1]) + qt(0.025, 9) * se1)\n\n[1] -0.5297804\n\n(ul1 &lt;- mean(sleep$extra[sleep$group == 1]) - qt(0.025, 9) * se1)\n\n[1] 2.02978\n\n\nThe 95% confidence interval for group 2 is [0.90, 3.76].Let’s repeat the procedure for group 2. Start by finding the standard error.\n\n(se2 &lt;- sd(sleep$extra[sleep$group == 2]) / sqrt(length(sleep$extra[sleep$group == 2])))\n\n[1] 0.6331666\n\n\nUsing the standard error we can complete the confidence interval.\n\n(ll2 &lt;- mean(sleep$extra[sleep$group == 2]) + qt(0.025, 9) * se2)\n\n[1] 0.8976775\n\n(ul2 &lt;- mean(sleep$extra[sleep$group == 2]) - qt(0.025, 9) * se2)\n\n[1] 3.762322\n\n\nDrug 2 is more effective. Drug 2 does not include zero in the interval, and the interval is to the right of zero. It is unlikely that drug 2 has no effect on students’ sleeping time. Additionally, Drug 2’s mean increase in sleeping hours is 2.33 vs. 0.75 for drug 1.\n\n\n\nExercise 4\n\nA random sample of \\(100\\) observations results in \\(40\\) successes. Construct a \\(90\\)% and \\(95\\)% confidence interval for the population proportion. Can we conclude at either confidence level that the population proportion differs from \\(0.5\\)?\n\n\nAnswer\n\nThe 90% and 95% confidence intervals are [0.319, 0.481], and [0.304, 0.496] respectively. Since they do not include 0.5, we can conclude that the population proportion is significantly different from 0.5.\nWe can create an object that stores the sample proportion and sample in R:\n\n(p &lt;- 0.4)\n\n[1] 0.4\n\n(n &lt;- 100)\n\n[1] 100\n\n\nThe 90% confidence interval is given by:\n\n(Ex1ll90 &lt;- p + qnorm(0.05) * sqrt(p * (1 - p) / 100))\n\n[1] 0.319419\n\n(Ex1ul90 &lt;- p - qnorm(0.05) * sqrt(p * (1 - p) / 100))\n\n[1] 0.480581\n\n\nThe 95% confidence interval is:\n\n(Ex1ll95 &lt;- p + qnorm(0.025) * sqrt(p * (1 - p) / 100))\n\n[1] 0.3039818\n\n(Ex1ul95 &lt;- p - qnorm(0.025) * sqrt(p * (1 - p) / 100))\n\n[1] 0.4960182\n\n\n\nYou will need the HairEyeColor data set for this problem. The data is built into R, and displays the distribution of hair and eye color for \\(592\\) statistics students. Construct a \\(95\\) confidence interval for the proportion of Hazel eye color students.\n\n\nAnswer\n\nThe 95% confidence interval is [0.128, 0.186].\nThe data can easily be viewed by calling HairEyeColor in R.\n\nHairEyeColor\n\n, , Sex = Male\n\n       Eye\nHair    Brown Blue Hazel Green\n  Black    32   11    10     3\n  Brown    53   50    25    15\n  Red      10   10     7     7\n  Blond     3   30     5     8\n\n, , Sex = Female\n\n       Eye\nHair    Brown Blue Hazel Green\n  Black    36    9     5     2\n  Brown    66   34    29    14\n  Red      16    7     7     7\n  Blond     4   64     5     8\n\n\nNote that there are three dimensions to this table (Hair, Eye, Sex). We can calculate the proportion of Hazel eye colored students with the following command that makes use of indexing:\n\n(p &lt;- (sum(HairEyeColor[, 3, 1]) + sum(HairEyeColor[, 3, 2])) / sum(HairEyeColor))\n\n[1] 0.1570946\n\n\nNow we can use this proportion to construct the intervals. Recall that for proportions the interval is calculated by ({p} z_{/2} ). The 95% confidence interval is given by:\n\n(Ex2ll95 &lt;- p + qnorm(0.025) * sqrt(p * (1 - p) / 592))\n\n[1] 0.1277818\n\n(Ex2ul95 &lt;- p - qnorm(0.025) * sqrt(p * (1 - p) / 592))\n\n[1] 0.1864074",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Inference II</span>"
    ]
  },
  {
    "objectID": "InferenceII.html#answers",
    "href": "InferenceII.html#answers",
    "title": "12  Inference II",
    "section": "12.3 Answers",
    "text": "12.3 Answers\n\nExercise 1\n\nThe mean of Population is \\(48.61\\). The standard deviation is \\(47.94\\).\n\nStart by generating values from the exponential distribution. You can use the rexp() function in R to do this. Setting the seed to \\(9\\) yields:\n\nset.seed(9)\nPopulation&lt;-rexp(1000,0.02)\n\nThe population mean is:\n\n(PopMean&lt;-mean(Population))\n\n[1] 48.61053\n\n\nThe standard deviation is:\n\n(PopSD&lt;-sd(Population))\n\n[1] 47.94411\n\n\n\nThe mean is very close to the population mean \\(48.83\\). The standard deviation is \\(6.83\\).\n\nIn R you can use a for loop to create the vector of sample means.\n\nnrep&lt;-10000\nSampleMeans&lt;-c()\nfor (i in 1:nrep){\n  x&lt;-sample(Population,50,replace=T)\n  SampleMeans&lt;-c(SampleMeans,mean(x))\n}\n\nThe mean of SampleMeans is:\n\n(xbar&lt;-mean(SampleMeans))\n\n[1] 48.7005\n\n\nThe standard deviation is:\n\n(Standard&lt;-sd(SampleMeans))\n\n[1] 6.827595\n\n\n\nThe confidence interval is [\\(47.71\\),\\(70.17\\)]. Since the population mean is equal to \\(48.61\\), the confidence interval does include the population mean.\n\nLet’s construct the upper an lower limits of the interval in R.\n\n(ll&lt;-SampleMeans[1]+qnorm(0.05)*Standard)\n\n[1] 47.71385\n\n(ul&lt;-SampleMeans[1]-qnorm(0.05)*Standard)\n\n[1] 70.17464\n\n\n\nThe confidence interval is [\\(14.86\\),\\(37.32\\)]. This interval does not include the population mean of \\(48.61\\). Out of the \\(10,000\\) confidence intervals, one would expect about \\(9,000\\) to include the population mean.\n\nLet’s find the confidence interval limits using R.\n\n(Minll&lt;-min(SampleMeans)+qnorm(0.05)*Standard)\n\n[1] 14.85631\n\n(Minul&lt;-min(SampleMeans)-qnorm(0.05)*Standard)\n\n[1] 37.31709\n\n\nWe can confirm in R that about \\(9,000\\) of the intervals include PopMean. Once more, let’s use a for loop to construct confidence intervals for each element in SampleMeans and check whether the PopMean is included. The count variable keeps track of how many intervals include the population mean.\n\ncount=0\n\nfor (i in SampleMeans){\n  (ll&lt;-i+qnorm(0.05)*Standard)\n  (ul&lt;-i-qnorm(0.05)*Standard)\n  if (PopMean&lt;=ul & PopMean&gt;=ll){\n    count=count+1\n  }\n}\n\ncount\n\n[1] 8978\n\n\n\n\nExercise 2\n\nThe \\(90\\)% confidence interval is [\\(94.52\\),\\(114.67\\)] and the \\(95\\)% confidence interval is [\\(114.68\\),\\(116.76\\)]. The larger the confidence level, the larger the interval.\n\nLet’s construct the intervals using R. Since the population standard deviation is unknown we will use the t-distribution. The interval is constructed as \\(\\bar{x}\\pm t_{\\alpha/2} \\frac{s}{\\sqrt{n}}\\).\n\n(ul90&lt;-104.6-qt(0.05,23)*28.8/sqrt(24))\n\n[1] 114.6755\n\n(ll90&lt;-104.6+qt(0.05,23)*28.8/sqrt(24))\n\n[1] 94.52453\n\n\nFor the \\(95\\)% confidence interval we adjust the significance level accordingly.\n\n(ul95&lt;-104.6-qt(0.025,23)*28.8/sqrt(24))\n\n[1] 116.7612\n\n(ll95&lt;-104.6+qt(0.025,23)*28.8/sqrt(24))\n\n[1] 92.43883\n\n\n\nThe confidence interval for a sample size of \\(16\\) is [\\(30.75\\),\\(66.61\\)]. The confidence interval when the sample size is \\(25\\) is [\\(34.79\\),\\(62.57\\)]. As the sample size gets larger, the confidence interval gets narrower and more precise.\n\nLet’s use R again to calculate the confidence interval. For a sample size of \\(16\\) the interval is:\n\n(ul16&lt;-48.68-qt(0.025,15)*33.64/sqrt(16))\n\n[1] 66.60549\n\n(ll16&lt;-48.68+qt(0.025,15)*33.64/sqrt(16))\n\n[1] 30.75451\n\n\nIncreasing the ample size to \\(25\\) yields:\n\n(ul25&lt;-48.68-qt(0.025,24)*33.64/sqrt(25))\n\n[1] 62.56591\n\n(ll25&lt;-48.68+qt(0.025,24)*33.64/sqrt(25))\n\n[1] 34.79409\n\n\n\n\nExercise 3\n\nThe 95% confidence interval for group 1 is [\\(-0.53\\),\\(2.03\\)].\n\nLet’s first calculate the standard error for group 1.\n\n(se1&lt;-sd(sleep$extra[sleep$group==1])/sqrt(length(sleep$extra[sleep$group==1])))\n\n[1] 0.5657345\n\n\nWe can now use the standard error to estimate the lower and upper limits of the confidence interval.\n\n(ll1&lt;-mean(sleep$extra[sleep$group==1])+qt(0.025,9)*se1)\n\n[1] -0.5297804\n\n(ul1&lt;-mean(sleep$extra[sleep$group==1])-qt(0.025,9)*se1)\n\n[1] 2.02978\n\n\n\nThe 95% confidence interval for group 2 is [\\(0.90\\),\\(3.76\\)].\n\nLet’s repeat the procedure for group 2. Start by finding the standard error.\n\n(se2&lt;-sd(sleep$extra[sleep$group==2])/sqrt(length(sleep$extra[sleep$group==2])))\n\n[1] 0.6331666\n\n\nUsing the standard error we can complete the confidence interval.\n\n(ll2&lt;-mean(sleep$extra[sleep$group==2])+qt(0.025,9)*se2)\n\n[1] 0.8976775\n\n(ul2&lt;-mean(sleep$extra[sleep$group==2])-qt(0.025,9)*se2)\n\n[1] 3.762322\n\n\n\nDrug 2. Drug 2 does not include zero in the interval, and the interval is to the right of zero. It is unlikely, that drug 2 has no effect on students sleeping time. Additionally, Drug 2’s mean increase in sleeping hours is \\(2.33\\) vs. \\(0.75\\) for drug 1.\n\n\n\nExercise 4\n\nThe 90% and 95% confidence intervals are [\\(0.319\\),\\(0.481\\)], and [\\(0.304\\),\\(0.496\\)] respectively. Since they do not include 0.5, we can conclude that the population proportion is significantly different from 0.5.\n\nWe can create an object that stores the sample proportion and sample in R:\n\n(p&lt;-0.4)\n\n[1] 0.4\n\n(n&lt;-100)\n\n[1] 100\n\n\nThe \\(90\\)% confidence interval is given by:\n\n(Ex1ll90&lt;-p+qnorm(0.05)*sqrt(p*(1-p)/100))\n\n[1] 0.319419\n\n(Ex1ul90&lt;-p-qnorm(0.05)*sqrt(p*(1-p)/100))\n\n[1] 0.480581\n\n\nThe \\(95\\)% confidence interval is:\n\n(Ex1ll90&lt;-p+qnorm(0.025)*sqrt(p*(1-p)/100))\n\n[1] 0.3039818\n\n(Ex1ul90&lt;-p-qnorm(0.025)*sqrt(p*(1-p)/100))\n\n[1] 0.4960182\n\n\n\nThe \\(90\\)% confidence interval is [\\(0.132\\),\\(0.182\\)].The \\(95\\)% confidence interval is [\\(0.128\\),\\(0.186\\)].\n\nThe data can easily be viewed by calling HairEyeColor in R.\n\nHairEyeColor\n\n, , Sex = Male\n\n       Eye\nHair    Brown Blue Hazel Green\n  Black    32   11    10     3\n  Brown    53   50    25    15\n  Red      10   10     7     7\n  Blond     3   30     5     8\n\n, , Sex = Female\n\n       Eye\nHair    Brown Blue Hazel Green\n  Black    36    9     5     2\n  Brown    66   34    29    14\n  Red      16    7     7     7\n  Blond     4   64     5     8\n\n\nNote that there are three dimensions to this table (Hair, Eye, Sex). We can calculate the proportion of Hazel eye colored students with the following command that makes use of indexing:\n\n(p&lt;-(sum(HairEyeColor[,3,1])+sum(HairEyeColor[,3,2]))/sum(HairEyeColor))\n\n[1] 0.1570946\n\n\nNow we can use this proportion to construct the intervals. Recall that for proportions the interval is calculated by \\(\\bar{p} \\pm z_{\\alpha/2} \\sqrt{\\frac{\\bar{p}(1-\\bar{p})}{n}}\\). The \\(90\\)% confidence interval is given by:\n\n(Ex2ll90&lt;-p+qnorm(0.05)*sqrt(p*(1-p)/592))\n\n[1] 0.1324945\n\n(Ex2ul90&lt;-p-qnorm(0.05)*sqrt(p*(1-p)/592))\n\n[1] 0.1816947\n\n\nThe 95% confidence interval is:\n\n(Ex2ll95&lt;-p+qnorm(0.025)*sqrt(p*(1-p)/592))\n\n[1] 0.1277818\n\n(Ex2ul95&lt;-p-qnorm(0.025)*sqrt(p*(1-p)/592))\n\n[1] 0.1864074",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Inference II</span>"
    ]
  },
  {
    "objectID": "InferenceIII.html",
    "href": "InferenceIII.html",
    "title": "13  Inference III",
    "section": "",
    "text": "13.1 Hypothesis Testing\nIn this chapter, you will begin mastering the concepts of null and alternative hypotheses, equipping you with the skills to draw reliable conclusions from sample data, effectively manage risks, and confidently justify strategic decisions.\nThe null hypothesis is a statement about the population parameter. Usually, the status quo. In research, it states no effect or no relationship between variables. The null hypothesis includes some form of the equality sign (i.e., \\(\\geq\\), \\(\\leq\\), or \\(=\\)).\nThe alternative hypothesis directly contradicts the null hypothesis. In research, it states the prediction of the effect or relationship. The alternative includes non-equality signs (i.e., \\(&gt;\\), \\(&lt;\\), or \\(\\ne\\)).\nTo conduct hypothesis testing:",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Inference III</span>"
    ]
  },
  {
    "objectID": "InferenceIII.html#concepts",
    "href": "InferenceIII.html#concepts",
    "title": "13  Inference III",
    "section": "14.1 Concepts",
    "text": "14.1 Concepts\n\nHypothesis Testing\nThe null hypothesis is a statement about the population parameter. Usually, the status quo. In research, it states no effect or no relationship between variables. The null hypothesis includes some form of the equality sign (i.e., \\(\\geq\\), \\(\\leq\\), or \\(=\\)).\nThe alternative hypothesis directly contradicts the null hypothesis. In research, it states the prediction of the effect or relationship. The alternative includes non-equality signs (i.e., \\(&gt;\\), \\(&lt;\\), or \\(\\ne\\)).\nTo conduct hypothesis testing:\n\nSpecify the null and alternate hypothesis.\n\n\nFor means use:\n\n\\(H_o: \\mu \\leq 0\\); \\(Ha: \\mu &gt; \\mu_o\\) right-tail probability\n\\(H_o: \\mu \\geq 0\\); \\(Ha: \\mu &lt; \\mu_o\\) left-tail probability\n\\(H_o: \\mu = 0\\); \\(Ha: \\mu \\ne \\mu_o\\) two-tail probability\n\nFor proportions use:\n\n\\(H_o: P \\leq 0\\); \\(Ha: P &gt; P_o\\) right-tail probability\n\\(H_o: P \\geq 0\\); \\(Ha: P &lt; P_o\\) left-tail probability\n\\(H_o: P = 0\\); \\(Ha: P \\ne P_o\\) two-tail probability\n\n\n\nSpecify the confidence level (i.e., how likely you would be to see non-extreme data, when assuming the null is true. False negative tolerance) and significance level (i.e. how likely you would be to see extreme data, when assuming the null is true. False positive tolerance). Confidence levels are usually set at, \\(0.90\\), \\(0.95\\), or \\(0.99\\), which correspond to \\(10\\)%, \\(5\\)%, and \\(1\\)% significant levels, respectively.\nCalculate the test statistic.\n\nFor a test on means use \\(t_{df}= \\frac {\\bar x-\\mu_o}{s/\\sqrt{n}}\\), where \\(df=n-1\\), \\(\\bar x\\) is the sample mean, \\(\\mu_o\\) is the hypothesized value of \\(\\mu\\), \\(s\\) is the sample standard deviation, and \\(n\\) is the sample size.\nFor a test on proportions use \\(z= \\frac {\\bar p- P_o}{\\sqrt {P_o(1-P_o)/ n}}\\), where \\(\\bar p\\) is the sample proportion, \\(P_o\\) is the hypothesized value of the population proportion \\(P\\), and \\(n\\) is the sample size.\n\nFind the p-value (i.e., the likelihood of getting the observed or more extreme data, assuming the null hypothesis is true). (Substitute \\(t\\) for \\(z\\) if using proportions)\n\nFor a right-tail test, the \\(p\\)-value is \\(P(T\\geq t)\\).\nFor a left-tail test, the \\(p\\)-value is \\(P(T\\leq t)\\).\nFor a two-tail test, the \\(p\\)-value is \\(2P(T\\geq t)\\) if \\(t&gt;0\\) or \\(2P(T\\leq t)\\) if \\(t&lt;0\\).\n\nThe decision rule is to reject the null hypothesis when the \\(p-value&lt;\\alpha\\), and not to reject when \\(p-value \\geq alpha\\).\n\n\n\nUseful R Functions\nt.test() generates a \\(t\\)-test for a vector of values. Use the alternative argument to specify “greater”, “less” or “two.sided” test. The mu argument specifies the hypothesized value for the mean. The conf.level sets the confidence level of the test (0.9,0.95,0.99, etc.).\nprop.test() generates a proportion test when provided the number of successes and sample size.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Inference III</span>"
    ]
  },
  {
    "objectID": "InferenceIII.html#exercises",
    "href": "InferenceIII.html#exercises",
    "title": "13  Inference III",
    "section": "13.3 Exercises",
    "text": "13.3 Exercises\nThe following exercises will help you test your knowledge on Hypothesis Testing. In particular, the exercises work on:\n\nStating Null and Alternate Hypothesis.\nDetermine the statistical validity of the null hypothesis.\nConducting t-tests in R.\n\nTry not to peek at the answers until you have formulated your own answer and double-checked your work for any mistakes.\n\nExercise 1\n\nConsider the following hypothesis: \\(H_{o}: \\mu=50\\), \\(H_{a}: \\mu \\neq 50\\). A sample of \\(16\\) observations yields a mean of \\(46\\) and a standard deviation of \\(10\\). Calculate the value of the test statistic. At a \\(5\\)% significance level, does the population mean differ from \\(50\\)?\n\n\nAnswer\n\nThe sample statistic is -1.6. The null hypothesis can’t be rejected at a 5% significance level since the p-value is 13.04%. We conclude that the population mean is not statistically different from 50.\nIn R we can calculate the t-statistic.\n\nmuEx1 &lt;- 50\nsigmaEx1 &lt;- 10\nn &lt;- 16\n\n(teststat &lt;- (46 - muEx1) / (sigmaEx1 / sqrt(n)))\n\n[1] -1.6\n\n(tcrit &lt;- qt(0.025, n - 1))\n\n[1] -2.13145\n\n\nSince the t-statistic is greater than the critical value of -2.13, we can’t reject the null. We can also estimate the p-value to confirm this finding. Recall that the P-value is the likelihood of obtaining a sample mean at least as extreme as the one derived from the given sample.\n\n2 * pt(teststat, n - 1)\n\n[1] 0.130445\n\n\n\nConsider the following hypothesis: \\(H_{o}: \\mu \\geq 100\\), \\(H_{a}: \\mu &lt; 100\\). You take a sample from a normally distributed population that yields the values in the table below. Conduct a test at a \\(1\\)% significance level to prove the hypothesis.\n\n\n\n96\n102\n93\n87\n92\n82\n\n\n\n\n\n\nAnswer\n\nThe null hypothesis that (H_{o}: ) can’t be rejected since the p-value of 1.9% is greater than the 1% significance level.\nLet’s start by creating an object to store the values of our sample.\n\nsample2 &lt;- c(96, 102, 93, 87, 92, 82)\n\nNow we can construct the t-stat and calculate the critical value.\n\nmean2 &lt;- mean(sample2)\nstandard2 &lt;- sd(sample2)\nn2 &lt;- length(sample2)\n(tstat2 &lt;- (mean2 - 100) / (standard2 / sqrt(n2)))\n\n[1] -2.816715\n\n\nLastly, we can calculate the p-value.\n\npt(tstat2, n2 - 1)\n\n[1] 0.0186262\n\n\nWe can also verify our result using the t.test() function in R.\n\nt.test(sample2, alternative = \"less\", mu = 100, conf.level = 0.99)\n\n\n    One Sample t-test\n\ndata:  sample2\nt = -2.8167, df = 5, p-value = 0.01863\nalternative hypothesis: true mean is less than 100\n99 percent confidence interval:\n    -Inf 101.557\nsample estimates:\nmean of x \n       92 \n\n\n\nConsider the following hypothesis: \\(H_{o}: \\mu \\leq 210\\), \\(H_{a}: \\mu &gt; 210\\). You take a sample from a normally distributed population that yields the values in the table below. Conduct a test at a \\(10\\)% significance level to prove the hypothesis.\n\n\n\n210\n220\n299\n220\n290\n280\n233\n221\n292\n299\n\n\n\n\n\nAnswer\n\nThe null hypothesis that (H_{o}: ) can be rejected since the p-value of 0.2% is less than the 10% significance level.\nLet’s create the object in R with the data.\n\nsample3 &lt;- c(210, 220, 299, 220, 290, 280, 233, 221, 292, 299)\n\nUsing the t.test() function we find:\n\nt.test(sample3, alternative = \"greater\", mu = 210, conf.level = 0.9)\n\n\n    One Sample t-test\n\ndata:  sample3\nt = 3.8333, df = 9, p-value = 0.002004\nalternative hypothesis: true mean is greater than 210\n90 percent confidence interval:\n 239.6593      Inf\nsample estimates:\nmean of x \n    256.4 \n\n\n\n\n\n\nExercise 2\nAccording to www.nps.gov, the period of time between Old Faithful’s eruptions is on average \\(92\\) minutes. Use the built-in faithful R data set and a two-tail test to determine whether this claim is true.\n\n\nAnswer\n\nThe claim that the duration between eruptions is 92 minutes can be rejected at a 10%, 5%, and 1% significance level.\nOnce more calculate the t-test in R with the t.test() function.\n\nt.test(faithful$waiting, alternative = \"two.sided\", mu = 92, conf.level = 0.99)\n\n\n    One Sample t-test\n\ndata:  faithful$waiting\nt = -25.601, df = 271, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 92\n99 percent confidence interval:\n 68.75871 73.03541\nsample estimates:\nmean of x \n 70.89706 \n\n\n\n\n\nExercise 3\n\nTo test if the population proportion differs from \\(0.4\\), a scientist draws a random sample of \\(100\\) observations and obtains a sample proportion of \\(0.48\\). Specify the competing hypothesis. At a \\(5\\)% significance level, does the population proportion differ from \\(0.4\\)?\n\n\nAnswer\n\nThe competing hypotheses are (H_{o}: p = 0.4), (H_{a}: p ). At a 5% significance level, we can’t reject the null hypothesis since the p-value of the test statistic (0.102) is greater than the significance level (0.05). We conclude that the population proportion is not significantly different from 0.4.\nIn R we can calculate the test statistic ().\n\n(pstat &lt;- (0.48 - 0.4) / sqrt(0.4 * (1 - 0.4) / 100))\n\n[1] 1.632993\n\n\nNow we can use the pnorm() function in R to get the p-value. Since it is a two-tailed test, we multiply the probability by 2.\n\n2 * pnorm(pstat, lower.tail = FALSE)\n\n[1] 0.1024704\n\n\n\nWhen taking a sample of \\(320\\) observations, \\(128\\) result in success. Test the following hypothesis \\(H_{o}: p \\geq 0.45\\), \\(H_{a}: p &lt; 0.45\\) at a \\(5\\)% significance level.\n\n\nAnswer\n\nFrom the sample, 40% are labeled as success. Testing the hypothesis reveals that we can reject the null at a 5% significance level. We conclude that the population proportion is less than 0.45.\nWe once again create the test statistic in R.\n\n(pstat2 &lt;- (0.4 - 0.45) / sqrt(0.45 * (1 - 0.45) / 320))\n\n[1] -1.797866\n\n\nWith the statistic, we can now find the p-value:\n\npnorm(pstat2, lower.tail = TRUE)\n\n[1] 0.0360991\n\n\n\nDetermine if more than \\(50\\)% of the observations in a population are below \\(10\\) with the sample data below. Conduct the test at a \\(1\\)% significance level.\n\n\n\n8\n12\n5\n9\n14\n11\n9\n3\n7\n12\n\n\n\n\n\nAnswer\n\nThe competing hypotheses are (H_{o}: p ), (H_{a}: p &gt; 0.5). At a 1% significance level, we can’t reject the null hypothesis since the p-value of the test statistic (0.26) is greater than the significance level (0.01). We conclude that more than 50% of the observations in the population are below 10.\nLet’s create an object to store the values.\n\nvalues &lt;- c(8, 12, 5, 9, 14, 11, 9, 3, 7, 12)\n\nNow, let’s count how many values are below 10 and calculate the proportion.\n\nsum(values &lt; 10) / length(values)\n\n[1] 0.6\n\n\nLastly, we find the test-statistic and p-value:\n\npstat3 &lt;- (0.6 - 0.5) / sqrt(0.5 * (1 - 0.5) / 10)\npnorm(pstat3, lower.tail = FALSE)\n\n[1] 0.2635446\n\n\nWe can also use the prop.test() function in R to confirm our result.\n\nprop.test(6, 10, p = 0.5, alternative = \"greater\", conf.level = 0.99, correct = FALSE)\n\n\n    1-sample proportions test without continuity correction\n\ndata:  6 out of 10, null probability 0.5\nX-squared = 0.4, df = 1, p-value = 0.2635\nalternative hypothesis: true p is greater than 0.5\n99 percent confidence interval:\n 0.2724654 1.0000000\nsample estimates:\n  p \n0.6 \n\n\n\n\n\n\nExercise 4\nAccording to www.worldatlas.com, \\(5\\)% of the population has hazel color eyes. Use the built-in HairEyeColor R data set and a two-tail test to determine whether this claim is true.\n\n\nAnswer\n\nWe reject the null hypothesis that 5% of the population has hazel eyes with our sample.\nThe number of people with hazel eyes is calculated as:\n\n(s &lt;- sum(HairEyeColor[, 3, 1] + HairEyeColor[, 3, 2]))\n\n[1] 93\n\n\nThe total number of people in the survey is given by:\n\n(t &lt;- sum(HairEyeColor))\n\n[1] 592\n\n\nWe can use the prop.test() function once more:\n\nprop.test(93, 592, p = 0.05, alternative = \"two.sided\", conf.level = 0.95, correct = FALSE)\n\n\n    1-sample proportions test without continuity correction\n\ndata:  93 out of 592, null probability 0.05\nX-squared = 142.94, df = 1, p-value &lt; 2.2e-16\nalternative hypothesis: true p is not equal to 0.05\n95 percent confidence interval:\n 0.1300037 0.1886070\nsample estimates:\n        p \n0.1570946",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Inference III</span>"
    ]
  },
  {
    "objectID": "InferenceIII.html#answers",
    "href": "InferenceIII.html#answers",
    "title": "13  Inference III",
    "section": "14.3 Answers",
    "text": "14.3 Answers\n\nExercise 1\n\nThe sample statistic is \\(-1.6\\). The null hypothesis can’t be rejected at a \\(5\\)% significance level since the p-value is \\(13.04\\)%. We conclude that the population mean is not statistically different from \\(50\\).\n\nIn R we can calculate the t-statistic.\n\nmuEx1&lt;-50\nsigmaEx1&lt;-10\nn&lt;-16\n\n(teststat&lt;-(46-muEx1)/(sigmaEx1/sqrt(n)))\n\n[1] -1.6\n\n(tcrit&lt;-qt(0.025,n-1))\n\n[1] -2.13145\n\n\nSince the t-statistic is greater than the critical value of \\(-2.13\\), we can’t reject the null. We can also estimate the p-value to confirm this finding. Recall that the P-value is the likelihood of obtaining a sample mean at least as extreme as the one derived from the given sample.\n\n2*pt(teststat,n-1)\n\n[1] 0.130445\n\n\n\nThe null hypothesis that \\(H_{o}: \\mu \\geq 100\\) can’t be rejected since the p-value of \\(1.9\\)% is greater than the \\(1\\)% significance level.\n\nLet’s start by creating an object to store the values of our sample.\n\nsample2&lt;-c(96,102,93,87,92,82)\n\nNow we can construct the t-stat and calculate the critical value.\n\nmean2&lt;-mean(sample2)\nstandard2&lt;-sd(sample2)\nn2&lt;- length(sample2)\n(tstat2&lt;-(mean2-100)/(standard2/sqrt(n2)))\n\n[1] -2.816715\n\n\nLastly, we can calculate the p-value.\n\npt(tstat2,n2-1)\n\n[1] 0.0186262\n\n\nWe can also verify our result using the t.test() function in R.\n\nt.test(sample2,alternative = \"less\",mu = 100,conf.level = 0.99)\n\n\n    One Sample t-test\n\ndata:  sample2\nt = -2.8167, df = 5, p-value = 0.01863\nalternative hypothesis: true mean is less than 100\n99 percent confidence interval:\n    -Inf 101.557\nsample estimates:\nmean of x \n       92 \n\n\n\nThe null hypothesis that \\(H_{o}: \\mu \\leq 210\\) can be rejected since the p-value of \\(0.2\\)% is less than the \\(10\\)% significance level.\n\nLet’s create the object in R with the data.\n\nsample3&lt;-c(210,220,299,220,290,280,233,221,292,299)\n\nUsing the t.test() function we find:\n\nt.test(sample3,alternative=\"greater\",mu=210,conf.level=0.9)\n\n\n    One Sample t-test\n\ndata:  sample3\nt = 3.8333, df = 9, p-value = 0.002004\nalternative hypothesis: true mean is greater than 210\n90 percent confidence interval:\n 239.6593      Inf\nsample estimates:\nmean of x \n    256.4 \n\n\n\n\nExercise 2\nThe claim that the duration between eruptions is \\(92\\) minutes can be rejected at a \\(10\\)%, \\(5\\)%, and \\(1\\)% significance level.\nOnce more calculate the t-test in R with the t.test() function.\n\nt.test(faithful$waiting,alternative = \"two.sided\",mu=92, conf.level = 0.99)\n\n\n    One Sample t-test\n\ndata:  faithful$waiting\nt = -25.601, df = 271, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 92\n99 percent confidence interval:\n 68.75871 73.03541\nsample estimates:\nmean of x \n 70.89706 \n\n\n\n\nExercise 3\n\nThe competing hypothesis are \\(H_{o}: p = 0.4\\), \\(H_{a}: p \\neq 0.4\\). At a \\(5\\)% significance level we can’t reject the null hypothesis since the p-value of the test statistic (\\(0.102\\)) is greater than the significance level (\\(0.05\\)). We conclude that the population proportion is not significantly different from \\(0.4\\).\n\nIn R we can calculate the test statistic \\(\\frac {\\bar{p}-p_{o}}{\\sqrt {p_{o}(1-p_{o})/n}}\\).\n\n(pstat&lt;-(0.48-0.4)/sqrt(0.4*(1-0.4)/100))\n\n[1] 1.632993\n\n\nNow we can use the pnorm() function in R to get the p-value. Since it is a two-tailed test we multiply the probability by 2.\n\n2*pnorm(pstat,lower.tail = F)\n\n[1] 0.1024704\n\n\n\nFrom the sample \\(40\\)% are labeled as success. Testing the hypothesis reveals that we can reject the null at a \\(5\\)% significance level. We conclude that the population proportion is less than \\(0.45\\).\n\nWe once again create the test statistic in R.\n\n(pstat2&lt;-(0.4-0.45)/sqrt(0.45*(1-0.45)/320))\n\n[1] -1.797866\n\n\nWith the statistic, we can now find the p-value:\n\npnorm(pstat2,lower.tail = T)\n\n[1] 0.0360991\n\n\n\nThe competing hypothesis are \\(H_{o}: p \\leq 0.5\\), \\(H_{a}: p &gt; 0.5\\). At a \\(1\\)% significance level we can’t reject the null hypothesis since the p-value of the test statistic (\\(0.26\\)) is greater than the significance level (\\(0.01\\)). We conclude that more than 50% of the observations in the population are below 10.\n\nLet’s create an object to store the values.\n\nvalues&lt;-c(8,12,5,9,14,11,9,3,7,12)\n\nNow, let’s count how many values are below \\(10\\) and calculate the proportion.\n\nsum(values&lt;10)/length(values)\n\n[1] 0.6\n\n\nLastly, we find the test-statistic and p-value:\n\npstat3&lt;-(0.6-0.5)/sqrt(0.5*(1-0.5)/10)\npnorm(pstat3,lower.tail = F)\n\n[1] 0.2635446\n\n\nWe can also use the prop.test() function in R to confirm our result.\n\nprop.test(6,10,p=0.5,alternative = \"greater\", conf.level = 0.99,\n            correct=F)\n\n\n    1-sample proportions test without continuity correction\n\ndata:  6 out of 10, null probability 0.5\nX-squared = 0.4, df = 1, p-value = 0.2635\nalternative hypothesis: true p is greater than 0.5\n99 percent confidence interval:\n 0.2724654 1.0000000\nsample estimates:\n  p \n0.6 \n\n\n\n\nExercise 4\n\nWe reject the null hypothesis that \\(5\\)% of the population has hazel eyes with our sample.\n\nThe number of people with Hazel eyes is calculated as:\n\n(s&lt;-sum(HairEyeColor[,3,1]+HairEyeColor[,3,2]))\n\n[1] 93\n\n\nThe total number of people in the survey is given by:\n\n(t&lt;-sum(HairEyeColor))\n\n[1] 592\n\n\nWe can use the prop.test() function once more:\n\nprop.test(93,592,p=0.05,alternative = \"two.sided\", conf.level = 0.95, correct=F)\n\n\n    1-sample proportions test without continuity correction\n\ndata:  93 out of 592, null probability 0.05\nX-squared = 142.94, df = 1, p-value &lt; 2.2e-16\nalternative hypothesis: true p is not equal to 0.05\n95 percent confidence interval:\n 0.1300037 0.1886070\nsample estimates:\n        p \n0.1570946",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Inference III</span>"
    ]
  },
  {
    "objectID": "InfReg.html",
    "href": "InfReg.html",
    "title": "14  Regression and Inference",
    "section": "",
    "text": "14.1 Correlation Significance\nIn this chapter, you will begin mastering the concepts of correlation significance, difference of means tests, and regression inference, empowering you with the tools to uncover meaningful relationships and differences in data while assessing their statistical significance. These skills will enable you to make informed business decisions, such as evaluating the strength of market trends, comparing performance across groups, or determining the impact of variables in predictive models, ensuring your conclusions are robust and actionable.\nTo determine the statistical significance of the correlation coefficient we test:\nThe test statistic for the correlation is given by \\(t_{df}= \\frac{r_{xy}\\sqrt{n-2}}{\\sqrt{1-r_{xy}^2}}\\), where \\(df=n-2\\) and \\(r_{xy}\\) is the sample correlation coefficient.\nRun the cor.test() function to perform the test on two vectors. Here is a list of arguments to use:",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Regression and Inference</span>"
    ]
  },
  {
    "objectID": "InfReg.html#concepts",
    "href": "InfReg.html#concepts",
    "title": "14  Regression and Inference",
    "section": "",
    "text": "Correlation Significance\nTo determine the statistical significance of the correlation coefficient we test:\n\n\\(H_o: \\rho \\geq 0\\); \\(H_a: \\rho &lt;0\\) left tail\n\\(H_o: \\rho \\leq 0\\); \\(H_a: \\rho &gt;0\\) right tail\n\\(H_o: \\rho = 0\\); \\(H_a: \\rho \\neq 0\\) two tails\n\nThe test statistic for the correlation is given by \\(t_{df}= \\frac{r_{xy}\\sqrt{n-2}}{\\sqrt{1-r_{xy}^2}}\\), where \\(df=n-2\\) and \\(r_{xy}\\) is the sample correlation coefficient.\nRun the cor.test() function to perform the test on two vectors. Here is a list of arguments to use:\n\nalternative: is a choice between “two.sided”, “less” and “greater”.\nconf.level: sets the confidence level. Enter as a decimal and not percentage.\n\n\n\nDifference of Means Tests\nTests for inference about the difference of two population means.\n\nThe test for unpaired mean differences (not equal variances) is given by \\(t_{df}= \\frac {(\\bar x_1 - \\bar x_2)- \\bar d_o}{\\sqrt {\\frac {s_1^2}{n_1} \\frac{s_2^2}{n_2}}}\\).\nThe test for unpaired mean difference (equal variances) is given by \\(t_{df}= \\frac {(\\bar x_1 - \\bar x_2)- \\bar d_o}{\\sqrt {s_p^2 (\\frac {1}{n_1} + \\frac {1}{n_2})}}\\).\nThe test for paired mean difference is given by \\(t_{df}= \\frac {\\bar d- d_o}{\\frac {s}{\\sqrt{n}}}\\).\n\nRun these test in R by using the t.test() function. Here is a list of arguments to use:\n\npaired: use True for paired, False for independent. The default is False.\nvar.equal: use True for equal variances, False for unequal. The default is False.\nmu: a value that indicate the hypothesized value of the mean or mean difference.\nalternative: is a choice between “two.sided”, “less” and “greater”.\nconf.level: sets the confidence level. Enter as a decimal and not percentage.\n\n\n\nRegression Inference\nWhen running regression a couple of test can be performed on the coefficients to determine significance:\n\nThe first test competing hypothesis are \\(H_o: \\beta_j = 0\\); \\(H_a: \\beta_j \\ne 0\\). The test statistic for the intercept (slope) coefficient is given by \\(t_{df}= \\frac {b_j}{se(b_j)}\\).\nThe second test competing hypothesis are \\(H_o: \\beta_1=\\beta_2=...\\beta_k=0\\); \\(H_a:\\) at least one \\(\\beta_i \\neq 0\\). The joint test of significance is given by \\(F_{df_1,df_2} = \\frac {SSR/k}{SSE/(n-k-1)} = \\frac {MSR}{MSE}\\). The Anova table below shows more detail on this test.\n\n\n\n\n\n\n\n\n\n\n\n\nAnova\ndf\nSS\nMS\nF\nSignificance\n\n\n\n\nRegression\n\\(k\\)\n\\(SSR\\)\n\\(MSR=\\frac{SSR}{k}\\)\n\\(F_{df_1,df_2} = \\frac {MSR}{MSE}\\)\n\\(P(F) \\geq \\frac{MSR}{MSE}\\)\n\n\nResidual\n\\(n-k-1\\)\n\\(SSE\\)\n\\(MSE=\\frac {SSE}{n-k-1}\\)\n\n\n\n\nTotal\n\\(n-1\\)\n\\(SST\\)\n\n\n\n\n\n\nTo conduct these tests, save the lm() model into an object. The summary() function can then be used to retrieve the results of the tests on the model’s parameters. Use the anova() function to obtain the Anova table.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Regression and Inference</span>"
    ]
  },
  {
    "objectID": "InfReg.html#exercises",
    "href": "InfReg.html#exercises",
    "title": "14  Regression and Inference",
    "section": "14.4 Exercises",
    "text": "14.4 Exercises\nThe following exercises will help you test your knowledge on Regression and Inference. In particular, the exercises work on:\n\nDetermining the significance of correlations.\nConduct paired and unpaired test of means and proportions.\nDetermining the significance of the slope and intercept estimates both individually and jointly.\nDeveloping prediction intervals.\n\nTry not to peek at the answers until you have formulated your own answer and double-checked your work for any mistakes.\n\nExercise 1\n\nConsider the following competing hypothesis: \\(H_{o}: \\rho=0\\), \\(H_{a}: \\rho \\neq 0\\). A sample of \\(25\\) observations reveals that the correlation coefficient between two variables is \\(0.15\\). At a \\(5\\)% confidence level, can we reject the null hypothesis?\n\n\nAnswer\n\nAt the 5% significance level, we cannot reject the null since the p-value is 0.47 &gt; 0.05.\nRecall that the t-stat is calculated by (). We can use R as a calculator to calculate this value:\n\nrxy &lt;- 0.15\nn &lt;- 25\n(tstat &lt;- (rxy * sqrt(n - 2)) / (sqrt(1 - rxy^2)))\n\n[1] 0.7276069\n\n\nNow, we can estimate the p-value using the pt() function:\n\n2 * pt(tstat, n - 2, lower.tail = FALSE)\n\n[1] 0.4741966\n\n\n\nInstall the ISLR2 package in R. Use the Hitters data set to look at the relationship between Hits and Salary. Specifically, calculate the correlation coefficient and test the competing hypothesis \\(H_{o}: \\rho=0\\), \\(H_{a}: \\rho \\neq 0\\) at the \\(1\\)% significance level.\n\n\nAnswer\n\nThe estimated correlation of 0.44 and the t-value is 7.89. Since the p-value is approximately 0, we reject the null hypothesis (H_{o}: ).\nOnce the ISLR2 package is downloaded, it can be loaded to R using the library() function. The cor.test() function conducts the appropriate test of significance.\n\nlibrary(ISLR2)\ncor.test(Hitters$Salary, Hitters$Hits, conf.level = 0.95)\n\n\n    Pearson's product-moment correlation\n\ndata:  Hitters$Salary and Hitters$Hits\nt = 7.8863, df = 261, p-value = 8.531e-14\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3355210 0.5314332\nsample estimates:\n      cor \n0.4386747 \n\n\n\n\n\n\nExercise 2\n\nInstall the ISLR2 package in R. Use the Hitters data set to investigate if the average hits were significantly different between the two divisions (American and National). Use the NewLeague and Hits variables to test the hypothesis at the \\(5\\)% significance level. Is there reason to believe that the population variances are different?\n\n\nAnswer\n\nThere is no reason to believe that the population variances are different. Players are recruited from what seems to be a common pool. At a 5% significance level, the difference of the two means is not significantly different from zero. We can’t reject the null hypothesis.\nWe will use the t.test() function in R to test the hypothesis. We note that the test is not paired, two-sided, and assumes equal variances in the population.\n\nt.test(Hitters$Hits[Hitters$NewLeague == \"A\"],\n       Hitters$Hits[Hitters$NewLeague == \"N\"], paired = FALSE, \n       alternative = \"two.sided\", mu = 0, var.equal = TRUE,\n       conf.level = 0.95)\n\n\n    Two Sample t-test\n\ndata:  Hitters$Hits[Hitters$NewLeague == \"A\"] and Hitters$Hits[Hitters$NewLeague == \"N\"]\nt = 1.0862, df = 320, p-value = 0.2782\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -4.581286 15.875028\nsample estimates:\nmean of x mean of y \n103.58523  97.93836 \n\n\n\nUse the ISLR2 package for this question. Particularly, use the BrainCancer data set to test whether males have a higher average survival time than women. Use the sex and time variables to test the hypothesis at the \\(5\\)% significance level. Is there reason to believe that the population variances are different?\n\n\nAnswer\n\nThere might be reason to believe that the population variances are different. Women and men are known to have medical differences. At a 5% significance level, the average survival time of men seems not to be larger than that of women. We can’t reject the null hypothesis (H_{o}: {x}{1} - {x}{2} ).\nOnce more use the t.test() function in R to test the hypothesis. Note that the test is not paired, right-tailed, and assumes different variances in the population.\n\nt.test(BrainCancer$time[BrainCancer$sex == \"Male\"],\n       BrainCancer$time[BrainCancer$sex == \"Female\"], paired = FALSE, \n       alternative = \"greater\", mu = 0, var.equal = FALSE,\n       conf.level = 0.95)\n\n\n    Welch Two Sample t-test\n\ndata:  BrainCancer$time[BrainCancer$sex == \"Male\"] and BrainCancer$time[BrainCancer$sex == \"Female\"]\nt = -0.30524, df = 84.867, p-value = 0.6195\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n -8.504999       Inf\nsample estimates:\nmean of x mean of y \n 26.78302  28.10200 \n\n\n\n\n\n\nExercise 3\n\nUse the sleep data set included in R. At the \\(1\\)% significance level, is there an effect of the drug on the \\(10\\) patients? Assume that the group variable denotes before (\\(1\\)) the drug is administered and after (\\(2\\)) the drug is administered.\n\n\nAnswer\n\nThe drug seems to have an effect as we can reject the null hypothesis (H_{o}: {d} = 0). The difference of means seems to be statistically different from zero.\nUse the t.test() function once more in R. Make sure to note that the test is paired and two-tailed.\n\nt.test(sleep$extra[sleep$group == 1],\n       sleep$extra[sleep$group == 2], paired = TRUE,\n       alternative = \"two.sided\", mu = 0, conf.level = 0.99)\n\n\n    Paired t-test\n\ndata:  sleep$extra[sleep$group == 1] and sleep$extra[sleep$group == 2]\nt = -4.0621, df = 9, p-value = 0.002833\nalternative hypothesis: true mean difference is not equal to 0\n99 percent confidence interval:\n -2.8440519 -0.3159481\nsample estimates:\nmean difference \n          -1.58 \n\n\n\n\n\n\nExercise 4\n\nInstall the ISLR2 package in R. Use the Hitters data set to investigate the effect of HmRun, RBI, and Years on a player’s Salary. Which variables are statistically different from zero? Are the variables jointly significant? Does the \\(R^2\\) suggest a good fit of the data to the model?\n\n\nAnswer\n\nBoth RBI and Years are statistically significant, and the salary of a player increases as they gain more experience and have more RBIs. Home runs do not seem to have an impact on the salary of a player according to the data. The F-statistic reveals that the coefficients are jointly significant since the p-value is approximately zero. Both the Multiple and Adjusted (R^2) suggest that the model only accounts for 32% of the variation in Salary. We might have to include more variables in our model to better explain the salary of a player.\nWe can run a linear regression in R by using the lm() function. We’ll use the summary() function to get more details on the model’s performance.\n\nfit &lt;- lm(Salary ~ HmRun + RBI + Years, data = Hitters)\nsummary(fit)\n\n\nCall:\nlm(formula = Salary ~ HmRun + RBI + Years, data = Hitters)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-752.31 -197.27  -66.80   97.73 2151.78 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -90.086     61.142  -1.473    0.142    \nHmRun         -7.346      4.972  -1.478    0.141    \nRBI            9.156      1.685   5.432 1.28e-07 ***\nYears         32.818      4.838   6.783 7.97e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 372.2 on 259 degrees of freedom\n  (59 observations deleted due to missingness)\nMultiple R-squared:  0.3269,    Adjusted R-squared:  0.3191 \nF-statistic: 41.93 on 3 and 259 DF,  p-value: &lt; 2.2e-16\n\n\n\nJosé Altuve had \\(28\\) home runs, \\(57\\) RBI’s, and has been in the league for \\(12\\) years. What is the model’s predicted salary for him? What is the \\(95\\)% prediction interval? Note: The model predicts his salary if he played in \\(1987\\).\n\n\nAnswer\n\nThe predicted salary is 619.93, and the 95% prediction interval is [-129.89, 1369.7].\n\nnew &lt;- data.frame(HmRun = 28, RBI = 57, Years = 12)\npredict(fit, newdata = new, level = 0.95, interval = \"prediction\")\n\n       fit       lwr      upr\n1 619.9268 -129.8905 1369.744",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Regression and Inference</span>"
    ]
  },
  {
    "objectID": "InfReg.html#answers",
    "href": "InfReg.html#answers",
    "title": "14  Regression and Inference",
    "section": "14.3 Answers",
    "text": "14.3 Answers\n\nExercise 1\n\nAt the \\(5\\)% significance level, we can not reject the null since the p-value is \\(0.47&gt;0.05\\).\n\nRecall that the t-stat is calculated by \\(\\frac {r_{xy}\\sqrt{n-2}}{\\sqrt{1-r_{xy}^2}}\\). We can use R as a calculator to calculate this value:\n\nrxy&lt;-0.15\nn&lt;-25\n(tstat&lt;-(rxy*sqrt(n-2))/(sqrt(1-rxy^2)))\n\n[1] 0.7276069\n\n\nNow, we can estimate the \\(p\\)-value using the pt() function:\n\n2*pt(tstat,n-2,lower.tail = F)\n\n[1] 0.4741966\n\n\n\nThe estimated correlation of \\(0.44\\) and the t-value is \\(7.89\\). Since the \\(p\\)-value is approximately \\(0\\) we reject the null hypothesis \\(H_{o}: \\rho=0\\).\n\nOnce the ISLR2 package is downloaded, it can be loaded to R using the library() function. The cor.test() function conducts the appropriate test of significance.\n\nlibrary(ISLR2)\ncor.test(Hitters$Salary,Hitters$Hits, conf.level = 0.95)\n\n\n    Pearson's product-moment correlation\n\ndata:  Hitters$Salary and Hitters$Hits\nt = 7.8863, df = 261, p-value = 8.531e-14\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3355210 0.5314332\nsample estimates:\n      cor \n0.4386747 \n\n\n\n\nExercise 2\n\nThere is no reason to believe that the population variances are different. Players are recruited from what seems to be a common pool. At a \\(5\\)% significance level, the difference of the two means is not significantly different from zero. We can’t reject the null hypothesis.\n\nWe will use the t.test() function in R to test the hypothesis. We note that the test is not paired, two sided and of equal variances in the population.\n\nt.test(Hitters$Hits[Hitters$NewLeague==\"A\"],\n       Hitters$Hits[Hitters$NewLeague==\"N\"],paired = F, \n       alternative = \"two.sided\",mu = 0,var.equal = T,\n       conf.level = 0.95 )\n\n\n    Two Sample t-test\n\ndata:  Hitters$Hits[Hitters$NewLeague == \"A\"] and Hitters$Hits[Hitters$NewLeague == \"N\"]\nt = 1.0862, df = 320, p-value = 0.2782\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -4.581286 15.875028\nsample estimates:\nmean of x mean of y \n103.58523  97.93836 \n\n\n\nThere might be reason to believe that the population variances are different. Women and men are known to have medical differences. At a \\(5\\)% significance level, the average survival time of men seems not to be larger than that of women. We can’t reject the null hypothesis \\(H_{o}:\\bar {x_{1}}-\\bar {x_{2}} \\leq 0\\).\n\nOnce more use the t.test() function in R to test the hypothesis. Note that the test is not paired, right-tailed and of different variances in the population.\n\nt.test(BrainCancer$time[BrainCancer$sex==\"Male\"],\n       BrainCancer$time[BrainCancer$sex==\"Female\"],paired = F, \n       alternative = \"greater\",mu = 0, var.equal = F,\n       conf.level = 0.95 )\n\n\n    Welch Two Sample t-test\n\ndata:  BrainCancer$time[BrainCancer$sex == \"Male\"] and BrainCancer$time[BrainCancer$sex == \"Female\"]\nt = -0.30524, df = 84.867, p-value = 0.6195\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n -8.504999       Inf\nsample estimates:\nmean of x mean of y \n 26.78302  28.10200 \n\n\n\n\nExercise 3\n\nThere drug seems to have an effect as we can reject the null hypothesis \\(H_{o}:\\bar {d} = 0\\). The difference of means seems to be statistically different from zero.\n\nUse the t.test() function once more in R. Make sure to note that the test is paired, and two-tailed.\n\nt.test(sleep$extra[sleep$group==1],\n       sleep$extra[sleep$group==2], paired=T,\n       alternative = \"two.sided\", mu=0, conf.level = 0.99)\n\n\n    Paired t-test\n\ndata:  sleep$extra[sleep$group == 1] and sleep$extra[sleep$group == 2]\nt = -4.0621, df = 9, p-value = 0.002833\nalternative hypothesis: true mean difference is not equal to 0\n99 percent confidence interval:\n -2.8440519 -0.3159481\nsample estimates:\nmean difference \n          -1.58 \n\n\n\n\nExercise 4\n\nBoth RBI and Years are statistically significant and the salary of a player increases as they gain more experience and have more RBI’s. Home runs do not seem to have an impact on the salary of a player according to the data. The F-Statistics reveals that the coefficients are jointly significant since the p-value is approximately zero. Both the Multiple and Adjusted \\(R^2\\) suggest that the model only accounts for \\(32\\)% of the variation in Salary. We might have to include more variable in our model to better explain the salary of a player.\n\nWe can run a linear regression in R by using the lm() function. We’ll use the summary() function to get more details on the model’s performance.\n\nfit&lt;-lm(Salary~HmRun+RBI+Years,data=Hitters)\nsummary(fit)\n\n\nCall:\nlm(formula = Salary ~ HmRun + RBI + Years, data = Hitters)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-752.31 -197.27  -66.80   97.73 2151.78 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -90.086     61.142  -1.473    0.142    \nHmRun         -7.346      4.972  -1.478    0.141    \nRBI            9.156      1.685   5.432 1.28e-07 ***\nYears         32.818      4.838   6.783 7.97e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 372.2 on 259 degrees of freedom\n  (59 observations deleted due to missingness)\nMultiple R-squared:  0.3269,    Adjusted R-squared:  0.3191 \nF-statistic: 41.93 on 3 and 259 DF,  p-value: &lt; 2.2e-16\n\n\n\nThe predicted salary is \\(619.93\\) and the \\(95\\)% prediction interval is [\\(-129.89\\),\\(1369.7\\)].\n\n\nnew&lt;-data.frame(HmRun=28,RBI=57,Years=12)\npredict(fit,newdata=new,level=0.95,interval=\"prediction\")\n\n       fit       lwr      upr\n1 619.9268 -129.8905 1369.744",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Regression and Inference</span>"
    ]
  },
  {
    "objectID": "R Basics.html",
    "href": "R Basics.html",
    "title": "15  R Basics",
    "section": "",
    "text": "Objects\nBelow you will find a collection of R basic concepts.\nAn object is a data structure that stores a value or a set of values, along with information about the type of data and any associated attributes. Objects are usually created by assigning a value to a variable name. You can assign values by using either = or &lt;-.\nWhen naming objects in R use PascalCase, camelCase, snake_case or dot.case.\nScreenTime&lt;-120",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Grolemund, Garret. 2014. “Hands-on Programming with r.” https://jjallaire.github.io/hopr/.\n\n\nWickham, Hadley. 2017. “R for Data Science.” https://r4ds.hadley.nz.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "R Basics.html#exercises",
    "href": "R Basics.html#exercises",
    "title": "15  R Basics",
    "section": "15.1 Exercises",
    "text": "15.1 Exercises\n\nExercise 1\nUse the airquality data set included in R for this problem.\n\nSort the data by Temp in descending order. What is the day and month of the first observation on the sorted data?\n\n\n\nAnswer\n\nThe day and month of the first observation is August 28th.\nThe easiest way to sort in R is by using the dplyr package. Specifically, the arrange() function within the package. Let’s also use the desc() function to make sure that the data is sorted in descending order. We can use indexing to retrieve the first row of the sorted data set.\n\nlibrary(dplyr)\nSortedAQ&lt;-arrange(airquality,desc(Temp))\nSortedAQ[1,]\n\n  Ozone Solar.R Wind Temp Month Day\n1    76     203  9.7   97     8  28\n\n\n\n\nSort the data only by Temp in descending order. Of the \\(10\\) hottest days, how many of them were in July?\n\n\n\nAnswer\n\nWe can use the arrange() function one more time for this question. Then we can use indexing to retrieve the top \\(10\\) observations.\n\nSortedAQ2&lt;-arrange(airquality,desc(Temp))\nSortedAQ2[1:10,]\n\n   Ozone Solar.R Wind Temp Month Day\n1     76     203  9.7   97     8  28\n2     84     237  6.3   96     8  30\n3    118     225  2.3   94     8  29\n4     85     188  6.3   94     8  31\n5     NA     259 10.9   93     6  11\n6     73     183  2.8   93     9   3\n7     91     189  4.6   93     9   4\n8     NA     250  9.2   92     6  12\n9     97     267  6.3   92     7   8\n10    97     272  5.7   92     7   9\n\n\n\n\nHow many missing values are there in the data set? What rows have missing values for Solar.R?\n\n\n\nAnswer\n\nThere are a total of \\(44\\) missing values. Ozone has \\(37\\) and Solar.R has \\(7\\). Rows \\(5\\), \\(6\\), \\(11\\), \\(27\\), \\(96\\), \\(97\\), \\(98\\) are missing for Solar.R.\nWe can easily identify missing values with the summary() function.\n\nsummary(airquality)\n\n     Ozone           Solar.R           Wind             Temp      \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n NA's   :37       NA's   :7                                       \n     Month            Day      \n Min.   :5.000   Min.   : 1.0  \n 1st Qu.:6.000   1st Qu.: 8.0  \n Median :7.000   Median :16.0  \n Mean   :6.993   Mean   :15.8  \n 3rd Qu.:8.000   3rd Qu.:23.0  \n Max.   :9.000   Max.   :31.0  \n                               \n\n\nTo view the rows that have NA’s in them, we can use the is.na() function and indexing. Below we see that \\(7\\) values are missing for the Solar.R variable in the months \\(5\\) and \\(8\\) combined.\n\nairquality[is.na(airquality$Solar.R),]\n\n   Ozone Solar.R Wind Temp Month Day\n5     NA      NA 14.3   56     5   5\n6     28      NA 14.9   66     5   6\n11     7      NA  6.9   74     5  11\n27    NA      NA  8.0   57     5  27\n96    78      NA  6.9   86     8   4\n97    35      NA  7.4   85     8   5\n98    66      NA  4.6   87     8   6\n\n\n\n\nRemove all observations that have a missing values. Create a new object called CompleteAG.\n\n\n\nAnswer\n\nTo create the new object of complete observations we can use the na.omit() function.\n\nCompleteAQ&lt;-na.omit(airquality)\n\n\n\nWhen using CompleteAG, how many days was the temperature at least \\(60\\) degrees?\n\n\n\nAnswer\n\nThere were \\(107\\) days where the temperature was at least \\(60\\).\nUsing base R we have:\n\nnrow(CompleteAQ[CompleteAQ$Temp&gt;=60,])\n\n[1] 107\n\n\nWe can also use dplyr for this question. Specifically, using the filter() and nrow() functions we get:\n\nnrow(filter(CompleteAQ,Temp&gt;=60))\n\n[1] 107\n\n\n\n\nWhen using CompleteAG, how many days was the temperature within [\\(55\\),\\(75\\)] degrees and an Ozone below \\(20\\)?\n\n\n\nAnswer\n\nThere were \\(24\\) days where the temperature was between \\(55\\) and \\(75\\) and the ozone level was below \\(20\\).\nUsing base R we have:\n\nnrow(CompleteAQ[CompleteAQ$Temp&gt;55 & CompleteAQ$Temp&lt;75 & CompleteAQ$Ozone&lt;20,])\n\n[1] 24\n\n\nUsing the filter() function once more we get:\n\nnrow(filter(CompleteAQ,Temp&gt;55,Temp&lt;75,Ozone&lt;20))\n\n[1] 24\n\n\n\n\n\nExercise 3\nUse the Packers data set for this problem. You can find the data set at https://jagelves.github.io/Data/Packers.csv\n\nRemove the any observation that has a missing value with the na.omit() function. How many observations are left in the data set?\n\n\n\nAnswer\n\nThere are \\(84\\) observations in the complete cases data set.\nLet’s import the data to R by using the read.csv() function.\n\nPackers&lt;-read.csv(\"https://jagelves.github.io/Data/Packers.csv\")\n\nWe can remove any missing observation by using the na.omit() function. We can name this new object Packers2.\n\nPackers2&lt;-na.omit(Packers)\n\nTo find the number of observations we can use the dim() function. It returns the number of observations and variables.\n\ndim(Packers2)\n\n[1] 84  8\n\n\n\n\nDetermine the type of the Experience variable by using the typeof() function. What type is the variable?\n\n\n\nAnswer\n\nThe type is character.\nUse the typeof() function on the Experience variable.\n\ntypeof(Packers2$Experience)\n\n[1] \"character\"\n\n\n\n\nRemove observations that have an “R” and coerce the Experience variable to an integer using the as.integer() function. What is the total sum of years of experience?\n\n\n\nAnswer\n\nThe total sum of experience is \\(288\\).\nFirst, remove any observation with an R by using indexing and logicals.\n\nPackers2&lt;-Packers2[Packers2$Experience!=\"R\",]\n\nNow we can coerce the variable to an integer by using the as.integer() function.\n\nPackers2$Experience&lt;-as.integer(Packers2$Experience)\n\nLastly, calculate the sum using the sum() function.\n\nsum(Packers2$Experience)\n\n[1] 288\n\n\n\n\n\n\n\nWickham, Hadley. 2017. “R for Data Science.” https://r4ds.hadley.nz.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "RegressionII.html#multiple-regression",
    "href": "RegressionII.html#multiple-regression",
    "title": "7  Regression II",
    "section": "7.3 Multiple Regression",
    "text": "7.3 Multiple Regression\nIf we were to predict weight we could use several other variables that are related to get a better prediction. Multiple regression is a technique used predict a variable using more than one independent variable. Mathematically, you would be estimating \\(\\hat{y_i}=\\hat{\\beta_0}+\\hat{\\beta_1}x_1+\\hat{\\beta_2}x_2...+\\hat{\\beta_k}x_k\\), where \\(k\\) is the total number of independent variables included in the regression model.\nExample: Let’s consider an additional variable in our Weight and Exercise example. The table below includes information on Calories (z).\n\n\n\nWeight (y)\nExercise (x)\nCalories (z)\n\n\n\n\n165\n45\n1200\n\n\n170\n10\n1260\n\n\n168\n25\n1220\n\n\n164\n30\n1180\n\n\n165\n40\n1190\n\n\n\nThe regression line can be estimated using computer software and is given by \\(\\hat{y_i}=83.96-0.025x+0.069z\\). A few things can be concluded from the regression line. First, as the amount of exercise increases our weight tends to go down, but as we consume more calories the weight tends to go up. Second, the effectiveness of calories seems to be better that that of exercise. For every minute exercise our weight goes down on average by 0.025 pounds. However, if we reduce our calorie consumption by 1, then our weight goes down by 0.069 pounds.\n\nAnova\nWe can further explore the importance of each variable using the Anova table. This table helps us understand how well our regression model explains the dependent variable. For now, we will use this table to show the \\(SSR\\) generated by each variable as well as to track the errors. Using computer software we get the following table:\n\n\n\nSource\nSum Squares\n\n\n\n\nx\n17.63\n\n\nz\n6.55\n\n\nResiduals\n1.01\n\n\n\nThe table shows that x “closes the gap” with respect to the mean prediction by \\(17.63\\). If we add calories to our model, we further decrease the gap by another \\(6.55\\). In sum, using them together increases the \\(SSR\\) to about \\(24.18\\), which in turn reduced the \\(SSE\\) to only \\(1.01\\). Finally, the \\(R^2\\) also increases from \\(0.7\\) to now \\(0.96\\), \\(R^2= \\frac{17.63+6.55}{25.2}\\).\n\n\nAdjusted \\(R^2\\)\nThe adjusted \\(R^2\\) recognizes that the \\(R^2\\) is a non-decreasing function of the number of independent variables included in the model. This metric penalizes a model with more explanatory variables relative to a simpler model. It is calculated by \\(1-(1-R^2) \\frac {n-1}{n-k-1}\\), where \\(k\\) is the number of explanatory variables used in the model and \\(n\\) is the sample size. For our weight example \\(adjusted R^2=1-(1-0.96) \\frac{5-1}{5-2-1}=0.9196\\).\n\n\nResidual Standard Error\nThe Residual Standard Error estimates the average dispersion of the data points around the regression line. Recall, that we have calculated the squared deviation from the regression line by using \\(SSE\\). Hence if we want to find an average deviation we can divide using the number of observations and then find a square root to go back to linear values. In particular you can use \\(s_e =\\sqrt{\\frac{SSE}{n-k-1}}\\), where \\(k\\) is the number of independent variables used in your regression. For our example with only exercise in our model \\(s_e=\\sqrt{\\frac{7.57}{5-1-1}}=1.59\\). If we include both calories and exercise then \\(s_e=\\sqrt{\\frac{1.01}{5-2-1}}=0.71\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regression II</span>"
    ]
  },
  {
    "objectID": "RegressionII.html#regression-in-r",
    "href": "RegressionII.html#regression-in-r",
    "title": "7  Regression II",
    "section": "7.4 Regression in R",
    "text": "7.4 Regression in R\nBelow we conduct regression analysis in R, using the weight data. Let’s start by creating a tibble to store the data.\n\nlibrary(tidyverse)\nlibrary(ggthemes)\nd&lt;-tibble(y=c(165,170,168,164,165),\n          x=c(45,10,25,30,40),\n          z=c(1200,1260,1220,1180,1190))\n\nWe can first visualize the relationship between x and y using ggplot.\n\nd %&gt;% ggplot() +\n  geom_point(aes(y=y,x=x),\n             pch=21, col=\"black\", bg=\"blue\", alpha=0.5) +\n  theme_clean() +\n  geom_smooth(aes(y=y,x=x),\n              method=\"lm\",se=F,formula=y~x)\n\n\n\n\n\n\n\n\nThe regression line is estimated using the lm() command as shown below:\n\nfit&lt;-lm(y~x, data=d)\n\nWe can retrieve the coefficients by either using the coef() function or the summary() function.\n\nsummary(fit)\n\n\nCall:\nlm(formula = y ~ x, data = d)\n\nResiduals:\n      1       2       3       4       5 \n 0.9000  0.5333  0.8333 -2.4000  0.1333 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 171.00000    1.87913  91.000 2.93e-06 ***\nx            -0.15333    0.05799  -2.644   0.0774 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.588 on 3 degrees of freedom\nMultiple R-squared:  0.6997,    Adjusted R-squared:  0.5996 \nF-statistic: 6.991 on 1 and 3 DF,  p-value: 0.07738\n\n\nThe summary function confirms our results for the \\(R^2\\) and coefficients. If we wanted to make a prediction we can use the predict() function with the fit object as input. Below we predict y when x is equal to 4.\n\npredict(fit,newdata = tibble(x=c(4)))\n\n       1 \n170.3867 \n\n\nNote that the values of \\(x\\) must be entered as a tibble in the function. We can run multiple regression by just adding z to our fit object.\n\nfit&lt;-lm(y~x+z, data=d)\nsummary(fit)\n\n\nCall:\nlm(formula = y ~ x + z, data = d)\n\nResiduals:\n      1       2       3       4       5 \n-0.3375 -0.3375  0.7875 -0.3375  0.2250 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) 83.96250   24.20433   3.469   0.0740 .\nx           -0.02500    0.04413  -0.567   0.6281  \nz            0.06875    0.01911   3.598   0.0693 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7115 on 2 degrees of freedom\nMultiple R-squared:  0.9598,    Adjusted R-squared:  0.9196 \nF-statistic: 23.89 on 2 and 2 DF,  p-value: 0.04018\n\n\nLastly, to obtain the Anova table we can use the anova() command.\n\nanova(fit)\n\nAnalysis of Variance Table\n\nResponse: y\n          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \nx          1 17.6333 17.6333  34.831 0.02753 *\nz          1  6.5542  6.5542  12.947 0.06931 .\nResiduals  2  1.0125  0.5062                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBelow we summarize the R functions: - The lm() function to estimates the linear regression model.\n\nThe predict() function uses the linear model object to predict values. New data is entered as a data frame or tibble.\nThe coef() function returns the model’s coefficients.\nThe summary() function returns the model’s coefficients, and goodness of fit measures.\nThe Anova() function creates the anova table for a regression. you must provide an object that contains the result of the lm() function.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regression II</span>"
    ]
  },
  {
    "objectID": "ProbabilityI.html#experiments-and-sets",
    "href": "ProbabilityI.html#experiments-and-sets",
    "title": "8  Probability I",
    "section": "8.2 Experiments and Sets",
    "text": "8.2 Experiments and Sets\nThe following concepts form the foundation of probability theory. They provide a structure that is useful when analyzing uncertainty and randomness.\n\nAn experiment is a process that leads to one of several outcomes. Ex: Tossing a Die, Tossing a Coin, Drawing a Card, etc.\nAn outcome is the result of an experiment. Ex: A coin landing on heads, drawing the ace of spades.\nThe sample space \\((S)\\) of an experiment contains all possible outcomes of the experiment. Ex: \\(S=\\{1,2,3,4,5,6\\}\\) is the sample space for tossing a die.\nAn event is a subset of the sample space. Ex: \\(A=\\{2,4,6\\}\\) is the event of tossing an even number when rolling a die. \\(B=\\{1A,1D,1S,1H\\}\\) is the event of drawing an ace from a deck of cards.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability I</span>"
    ]
  },
  {
    "objectID": "ProbabilityI.html#basic-probability-concepts",
    "href": "ProbabilityI.html#basic-probability-concepts",
    "title": "8  Probability I",
    "section": "8.3 Basic Probability Concepts",
    "text": "8.3 Basic Probability Concepts\nA probability is a numerical value that measures the likelihood that an event occurs. To calculate probabilities, we find the ratio between favorable outcomes and total outcomes. Hence, the probability of an event is given by:\n\\[\nP(A)=\\frac{Number\\ of\\ outcomes\\ in\\ A}{Number\\ of\\ outocomes\\ in\\ Sample \\ Space}\n\\]\nThe probability of A satisfies \\(0\\leq P(A) \\leq1\\). Since A is a subset of S, the number of elements in A is at most that of the sample space S. Hence, if A=S, then P(A)=1. If A is a null event (i.e., has no elements) P(A)=0 and is impossible. We can conclude that P(A) ranges from 0 to 1, inclusive.\nIn many cases, we analyze how multiple events interact within the sample space, particularly when they either exclude each other or collectively cover all possible outcomes.Mutually exclusive events do not share any common outcomes. The occurrence of one event precludes the occurrence of others. Exhaustive events include all outcomes in the sample space.\nGiven these concepts if follows that the sum of the probabilities of a list of mutually exclusive and exhaustive events equals \\(1\\). Formally, we state: \\(\\sum P(x_i)=1\\), where \\(x_i\\) is outcome \\(i\\).\nExample: Let’s consider a standard deck of 52 playing cards to illustrate the key probability concepts.\nIf we randomly draw one card from a standard deck, the sample space \\(S\\) consists of all 52 cards. Suppose we define event \\(A\\) as drawing a red card. Since there are 26 red cards, the probability is:\n\\[\nP(A) = \\frac{\\text{Number of red cards}}{\\text{Total number of cards}} = \\frac{26}{52} = 0.5\n\\]\nSince \\(( 0 \\leq P(A) \\leq 1)\\), this follows the basic probability rule. Next let’s consider mutually exclusive events. Let event \\(B\\) be drawing a heart and event \\(C\\) be drawing a spade. Since a single drawn card cannot be both a heart and a spade, these events are mutually exclusive.\nThe four suits (hearts, diamonds, clubs, spades) form an exhaustive set, because every card belongs to exactly one suit. The sum of their probabilities equals 1:\n\\[\nP(\\text{hearts}) + P(\\text{diamonds}) + P(\\text{clubs}) + P(\\text{spades}) = \\frac{13}{52} + \\frac{13}{52} + \\frac{13}{52} + \\frac{13}{52} = 1\n\\]\nNote the this also satisfies the rule \\(\\sum P(x_i) = 1\\), where \\(x_i\\) represents each possible outcome (suit).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability I</span>"
    ]
  },
  {
    "objectID": "ProbabilityI.html#probability-rules",
    "href": "ProbabilityI.html#probability-rules",
    "title": "8  Probability I",
    "section": "8.5 Probability Rules",
    "text": "8.5 Probability Rules\nProbability rules help us quantify uncertainty, avoid cognitive biases, and make rational predictions based on available data. These rules allow us to make reliable decisions. Below we introduce some useful rules.\n\nComplement Rule\nThe Complement Rule state that: \\[P(A^c)=1-P(A)\\] where \\(A^c\\) is the complement of \\(A\\). This rule is useful when calculating probabilities for complex scenarios, where finding the probability of the complement is easier than computing the event itself.\nEx: The probability of drawing a spade, diamond or clubs can be estimated by just calculating the probability of drawing a heart. Since the probability of drawing a heart is \\(P(Heart)=\\frac{13}{52}\\), then using the complement rule, \\(P(Not \\text{Heart})=1-\\frac{13}{52}=\\frac{39}{52}\\)\n\n\nAddition Rule\nThe Addition Rule states:\n\\[P(A \\cup B)=P(A)+P(B)-P(A \\cap B)\\] where \\(\\cap\\) is intersection and \\(\\cup\\) is union. This formula prevents double-counting when events overlap by including the intersection of events.\nEx: Let’s calculate the probability of drawing either a heart or a face card. If we let event \\(A\\), be drawing a heart, and event \\(B\\) be drawing a face card, the probability is given by: \\[P(A \\cup B)=\\frac{13}{52}+\\frac{12}{52}-\\frac{3}{52}=\\frac{22}{52}\\]\n\n\nMultiplication Rule\nThe Multiplication Rule states: \\[P(A \\cap B)= P(A|B)P(B)\\] where \\(P(A|B)\\) is the conditional probability. Conditional probability measures the likelihood of an event occurring given that another event has already happened. If we reorganize the formula above we can get:\n\\[P(A|B)=\\frac{P(A \\cap B)}{P(B)}\\] Note that his formula focuses on cases where Event B has already occurred. We can then find the fraction the cases where Event A also happens.\nEx: Suppose we randomly draw two cards from a standard deck of 52 cards, without replacement. The probability that both cards are aces is given by:\n\\[P(Ace \\cap Ace)=P(Ace|Ace) \\times P(Ace)= \\frac {4}{52} \\times \\frac{3}{51}\\] Lastly, if events \\(A\\) and \\(B\\) are independent, we can simplify the multiplication rule to:\n\\[P(A \\cap B)= P(A)P(B)\\] Hence, we can define independence as: events \\(A\\) and \\(B\\) are independent if the occurrence (or non occurrence) of one does not affect the probability of the other occurring.\nEx1: Suppose we randomly draw a card from a standard deck, replace it, and then draw another card. The probability of drawing heart in the first draw is \\(13/52\\), the probability of drawing a spade in the second draw is also \\(13/52\\).\nEx2: Suppose that the weather is sunny, this has no effect on the outcome of a lottery. Hence, the weather and lottery outcomes are independent\n\n\nLaw of Total Probability\nThe Law of Total Probability states: \\[P(A)=P(A \\cap B)+P(A\\cap B^c)\\] The formula mainly states that If an event \\(A\\) can occur in multiple mutually exclusive scenarios (or conditions), then the total probability of \\(A\\) is the sum of its probabilities under each scenario, weighted by how often those scenarios occur. By using the multiplication rule we can alternately show the law of total probability as: \\[P(A)=P(A|B)P(B)+P(A|B^c)P(B^c)\\] The Venn diagram helps us visualize the law of total probability.\n\n\n\n\n\n\n\n\n\nThe image shows how event \\(A\\) can occur. Essentially, when \\(B\\) occurs \\(A\\) is possible in the left semicircle. When \\(B^c\\), then \\(A\\) is possible in the right semicircle. The addition of both semicircles, complete set \\(A\\).\nEx: Consider the sets face cards and hearts. The entire set of hearts can be completed by taking face cards that are hearts, and non face cards (the complement of face cards) and hearts.\n\n\nBayes’ Theorem\nBayes’ Theorem states:\n\\[P(A|B)= \\frac{P(B|A)P(A)}{P(B)}\\] In essence, Bayes’ theorem is a way of updating probabilities in light of new evidence. The theorem tells us that the probability of an event A given that we have observed some evidence is equal to the prior probability (\\(P(A)\\)) times a factor that measures the informativeness of the observed evidence \\(P(B|A)/P(B)\\).\nExample: Assume that a rare form of cancer affects 1% of the population. That is, 1 out of 100 people will likely be affected by it. There is a test for the cancer, but it is imperfect. In particular, if you don’t have cancer, the test says that you do 10% of the time (false positive rate). Also, if you do have cancer, it will tell you that you don’t have it 5% of the time. Let’s say you get tested, and your doctor calls you and says you have tested positive. How likely is it that you have this type of cancer?\nWe can use the Law of Total Probability and Bayes’ theorem to answer this question. Let’s first identify what we want to estimate. Since we need to figure out the likelihood of having cancer, given that we tested positive, we are looking for \\(P(C|+)\\). Now, we can set up Bayes’ rule to see what additional information we need to find our desired value. Bayes’ rule states:\n\\[P(C|+)= \\frac{P(+|C)P(C)}{P(+)}\\] Since the problem does not provide the \\(P(+)\\) we must estimate it with the Law of Total Probability, mainly:\n\\[P(+)=P(+|C)P(C)+P(+|NC)P(NC)=(0.95 \\times 0.01+(0.1 \\times 0.99)=0.1085\\] We can now use this probability to find our new belief that we have cancer:\n\\[P(C|+)= \\frac{(0.95 \\times 0.01) }{0.1085}=0.0875\\] In conclusion, if we before thought we had a \\(1%\\) chance of having the rare form of cancer, once we were told that we tested positive, we should update our belief to about \\(8.75%\\).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability I</span>"
    ]
  },
  {
    "objectID": "ProbabilityI.html#counting-rules",
    "href": "ProbabilityI.html#counting-rules",
    "title": "8  Probability I",
    "section": "8.7 Counting Rules",
    "text": "8.7 Counting Rules\nCounting rules are useful in probability as they allow us to determine the number of ways events can occur without listing out every possibility. Essentially, they facilitate the enumeration of possible outcomes for large sets.\n\nFactorial\nTo illustrate our first counting rule, consider the number of ways that we can organize a group of three people (Ann, Beth, Charlie). The enumeration of all the possible orderings in this case is not that tedious and it can be shown in a table:\n\n\n\nOrdering\nOrder\n\n\n\n\n1\n(Ann, Beth, Charlie)\n\n\n2\n(Ann, Charlie, Beth)\n\n\n3\n(Beth, Ann, Charlie)\n\n\n4\n(Beth, Charlie, Ann)\n\n\n5\n(Charlie, Ann, Beth)\n\n\n6\n(Charlie, Beth, Ann)\n\n\n\nThe table shows that there are six possible ways to organize a group of three people. The Factorial counts how many ways a group of \\(N\\) objects can be organized. Formally, we write:\n\\[Factorial=N!\\]\nwhere the operator \\(!\\) means to take the product of all positive integers from \\(N\\) down to \\(1\\). Additionally, \\(0!\\) is equal to zero. If we then try to find in how many ways we can organize a group of three people, we can avoid enumeration by calculating \\(3!=3 \\times 2 \\times 1=6\\).\nEx: Let’s assume that a manager has locked down the first five batters in a given game, but is undecided on the batting order. The number of ways that he can organize the five batters is given by \\(5!=5 \\times 4 \\times 3 \\times 2 \\times 1=120\\).\n\n\nCombinations\nThe Combination function counts the number of ways to choose \\(x\\) objects from a total of \\(n\\) objects. The order in which the \\(x\\) objects are listed does not matter. If repetition is not allowed use: \\[C(n, x)= \\frac{n!}{(n-x)!x!}\\] Ex: Let’s imaging visiting the Mexican restaurant Chipotle. We are choosing a bowl for lunch and we can choose from the following items: beans, chicken, rice, pico, guac, and beef. If we are only allowed to select four items out of the six, in how many ways can we construct a bowl? Below we use the combination formula to show that there are \\(15\\) possible bowls.\n\\[C(6, 4)= \\frac{6!}{(6-4)!4!}=\\frac{6 \\times 5}{2}=15\\]\nThere are situations when we wish to count all possible combinations inclusive of repetition of elements. If repetition is allowed we use the following formula:\n\\[C(n+x-1,x)=\\frac{(x+n-1)!}{(n-1)!x!}\\]\nEx: Imagine you visit the ice cream store. You want a three scoop dish of ice cream. In how many ways can you choose your dish if the flavors are vanilla, chocolate, smores, brownie, and chips? Below we use the combination formula with repetition to show that the answer is \\(35\\).\n\\[C(7,3)=\\frac{(5+3-1)!}{(5-1)!3!}= 7 \\times 5=35\\]\n\n\nPermutations\nThe Permutation function also counts the number of ways to choose \\(x\\) objects from a total of \\(n\\) objects. However, the order in which the \\(x\\) objects are listed does matter. Note that since order matters, the permutation will always have a higher number of possible outcomes relative to the combination. When repetition is not allowed, permutations can be counted by:\n\\[P(n,x)= \\frac{n!}{(n-x)!}\\]\nEx: The Olympic medal finals for the 100 meters sprint has eight runners competing for three medals. How many medal podiums are possible? There are a total of \\(336\\). Below you can see the calculation.\n\\[P(8,3)= \\frac{8!}{5!}= 8 \\times 7 \\times 6=336\\]\nAs with combinations, there might be situations were we wish to count when repetition is allowed. If this is this case, use: \\[P(n,x)=n^x\\]\nEx: You are trying to guess the combination of a lock that has three possible numbers. Since there are 10 numbers to choose from, there are \\(10^3=1000\\) possible permutations.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability I</span>"
    ]
  },
  {
    "objectID": "ProbabilityI.html#probability-in-r",
    "href": "ProbabilityI.html#probability-in-r",
    "title": "8  Probability I",
    "section": "8.8 Probability in R",
    "text": "8.8 Probability in R\nThe table() function can be used to construct frequency distributions.\nThe factorial() function returns the factorial of a number.\nThe gtools package contains the combinations() and permutations() functions used to calculate combinations and permutations. Use the repeats.allowed argument to specify counting with repetition or no repetition. The v argument allows you to specify a vector of elements.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability I</span>"
    ]
  },
  {
    "objectID": "ProbabilityI.html#venn-diagram",
    "href": "ProbabilityI.html#venn-diagram",
    "title": "8  Probability I",
    "section": "8.4 Venn Diagram",
    "text": "8.4 Venn Diagram\nA Venn diagram is a visual representation of sets and their relationships. It consists of overlapping circles, each representing a set. Below is an example:\n\n\n\n\n\n\n\n\n\nThe rectangle enclosing the circles represents the sample set (i.e., the 52 playing cards). Each circle represents a set or event The heart circle contains all 13 hearts; the same holds for the clubs circle. The area outside the circle that has hearts represents its complement, meaning elements that are clubs, spades, or diamonds. Since the two sets do not overlap, they are mutually exclusive (i.e., if a card is a heart, it can’t be a spade). The total area covered by both sets represents the union denoted by \\(\\cup\\). This is the set that contains all elements that belong to at least one of the sets. Formally, we would represent this set as \\((Hearts \\cup Spades)\\).\n\n\n\n\n\n\n\n\n\nThe overlapping region of two sets contains elements that belong to both sets. This region is called the intersection. The intersection contains elements that are Hearts and Jacks (i.e., the jack of hearts). Formally, we denote the intersection by \\((Hearts \\cap Jacks)\\).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability I</span>"
    ]
  },
  {
    "objectID": "ProbabilityI.html#contingency-tables",
    "href": "ProbabilityI.html#contingency-tables",
    "title": "8  Probability I",
    "section": "8.6 Contingency Tables",
    "text": "8.6 Contingency Tables\nA contingency table (or cross-tabulation) summarizes the relationship between two or events (or variables). It is particularly helpful when analyzing frequencies of events and there relationship to other events.\nThe contingency table below shows the frequency of event A and B occurring/not occurring.\n\n\n\n\n\n\n\n\n\nWe can see that event A occurs 60 times and that event B occurs 40 times. Most importantly, we can also see how the two events are not mutually exclusive, since A and B occur 26 times. The table can be easily translated to probabilities, by divinding each entry by the total amount of outcomes (i.e., 100). We call the table that includes probabilities the joint probability table.\n\n\n\n\n\n\n\n\n\nIn the middle of the table we have the joint probabilities or intersections of events (i.e., \\(A \\cap B\\), \\(A \\cap B^c\\), etc.). The values in the totals columns or rows are the probabilities of each event happening (i.e., \\(P(A)\\), \\(P(B^c)\\), etc.). We call these probabilities marginal probabilities since they reside at the margin of the table.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability I</span>"
    ]
  },
  {
    "objectID": "ProbabilityII.html#expected-value-and-variance",
    "href": "ProbabilityII.html#expected-value-and-variance",
    "title": "9  Probability II",
    "section": "9.2 Expected Value and Variance",
    "text": "9.2 Expected Value and Variance\nWhen summarizing a random variable, we are mostly interested in the variable’s central tendency (Expected Value) and dispersion (Variance).\nThe expected value (mean) is a measure of central location. For a discrete random variable it is given by: \\[E(x)=\\mu=\\sum xf(x)\\] where \\(f(x)\\) is the probability mass function. For a continuous random variable it is given by \\(E(x)= \\int_{-\\infty}^{\\infty} x f(x) dx\\), where \\(f(x)\\) is the probability density function.\nThe variance summarizes the deviation of the values of the random variable from the mean. It is calculated by: \\[var(x)=E[(x-E(x))^2]=E(x^2)-[E(x)]^2\\] Note that this formula can be used for both discrete and continuous random variables.\nExample: Let’s consider an incentive pay scheme for a company. The table below summarizes the scheme:\n\n\n\n\n\n\n\n\n\nIf a worker receives a “Superior” rating, they earn a $10,000 bonus. Based on years of employee data, the company has determined that a “Superior” rating occurs 15% of the time. It also lists all of the possible ratings of its employees and their likelihood on the table.\nTo summarize this distribution we can calculate the mean bonus paid. Using the expected value formula we find that the company pays 4,200 dollars on average in compensation.\n\\[E(x)=(10 \\times 0.15) + (6 \\times 0.25) + (3 \\times 0.4) + (0 \\times 0.2)=4.2\\] The variability of the compensation is given by: \\[var(x)=(100 \\times 0.15) + (36 \\times 0.25) + (9 \\times 0.4) + (0 \\times 0.2)-(4.2)^2=9.96\\] The expected value and the variance has helped us summarize the results of the company’s incentive scheme. Mainly, the company pays on average an incentive of 4,200 dollars give or take 3,156 dollars.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability II</span>"
    ]
  },
  {
    "objectID": "ProbabilityII.html#discrete-uniform-distribution",
    "href": "ProbabilityII.html#discrete-uniform-distribution",
    "title": "9  Probability II",
    "section": "9.3 Discrete Uniform Distribution",
    "text": "9.3 Discrete Uniform Distribution\nThe discrete uniform distribution is a probability distribution that assigns equal probability to each outcome in a finite set of possible outcomes. In other words, each outcome in the set is equally likely to occur.\nThe probability mass function is given by: \\[f(x)=1/n\\] where \\(n\\) is the number of elements in the sample space (all possible outcomes). The expected value is given by: \\[E(x)=\\frac {\\sum x_i}{n}\\] where \\(x_i\\) are the possible values, and \\(n\\) is the number of possible values. The variance is given by: \\[var(x)=\\frac {\\sum (x_i-E(x))^2}{n}\\] Example: Consider tossing a fair die. Since all outcomes are equally likely, the probability of the die landing on 6 is \\(f(x)=1/6\\). The expected value on a roll of a die is \\(E(x)=\\frac {1+2+3+4+5+6}{6}=3.5\\) and the variance is \\(var(x)=\\frac {(-2.5)^2+(-1.5)^2+(-0.5)^2+0.5^2+1.5^2+2.5^2}{6}\\approx 2.92\\).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability II</span>"
    ]
  },
  {
    "objectID": "ProbabilityII.html#binomial-distribution",
    "href": "ProbabilityII.html#binomial-distribution",
    "title": "9  Probability II",
    "section": "9.4 Binomial Distribution",
    "text": "9.4 Binomial Distribution\nThe binomial distribution is a probability distribution that describes the outcome of a sequence of \\(n\\) independent Bernoulli trials. In a Bernoulli trial, there are only two possible outcomes: “success” and “failure”. The probability of success is denoted by \\(p\\), and the probability of failure is denoted by \\(q = 1 - p\\). In a sequence of \\(n\\) independent Bernoulli trials, the number of successes (\\(x\\)) is a random variable that follows a binomial distribution.\nThe probability mass function is given by: \\[f(x)=C_x^n (p^x)(1-p)^{n-x}\\] where \\(n\\) is the number of trials, \\(x\\) is the number of successes, \\(p\\) is the probability of success, and \\(C_x^n\\) is the number of ways there can be \\(x\\) successes in \\(n\\) trials.\nThe expected value of the binomial distribution is\n\\[E(x)=np\\]\nThe variance of the binomial distribution is\n\\[var(x)=np(1-p)\\] Example: Consider the following scenario. Imagine your favorite team in the NBA Finals, trailing by 2 points with seconds left. Your star player, a 90% free-throw shooter, gets three shots. What’s the chance your team takes the lead?\nSince shooting free-throws can be thought as a binomial experiment, we can find the probability using the binomial probability mass function. To take the lead, the star player needs to make all three shots. We can substitute three into the function:\n\\[f(3)=C_3^3 (0.9^3)(0.1)^{0}=0.9^3=0.729\\]\nThe function indicates that there is a 72.9% chance of taking the lead.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability II</span>"
    ]
  },
  {
    "objectID": "ProbabilityII.html#the-hypergeometric-distribution",
    "href": "ProbabilityII.html#the-hypergeometric-distribution",
    "title": "9  Probability II",
    "section": "9.5 The Hypergeometric Distribution",
    "text": "9.5 The Hypergeometric Distribution\nThe hypergeometric distribution is a probability distribution that describes the outcome of drawing a sample from a population without replacement. It is used to calculate the probability of drawing a certain number of successes (\\(x\\)) in a sample of a given size (\\(n\\)), where the success or failure of each individual draw is dependent on the success or failure of other draws.\nThe hypergeometric experiment differs from the binomial since:\n\ntrials are not independent.\nthe probability of success changes from trial to trial.\n\nThe probability mass function is given by:\n\\[f(x)=\\frac {C_x^r C_{n-x}^{N-r}}{C_n^N}\\]\nwhere \\(n\\) is the number of trials, \\(x\\) is the number of successes, \\(r\\) is the number of elements in the population labeled as success, and \\(N\\) is the number of elements in the population.\nThe expected value of the hypergeometric distribution is: \\[E(x)=n \\frac {r}{N}\\]\nThe variance of the hypergeometric distribution is: \\[var(x)= n \\frac {r}{N} (1- \\frac {r}{N}) (\\frac {N-n}{N-1})\\] Example: Picture a box of 12 electric fuses from the manufacturer Ontario Electric. Five fuses are known to be defective. An inspector randomly picks 3 fuses to test. What is the probability that exactly one of the three chosen is defective?\nWe can use the hypergeometric probability mass function to solve this problem. From the problem we know that \\(N=12\\), \\(n=3\\), \\(r=5\\), and \\(x=1\\). Substituting these values into the PMF yields:\n\\[f(1)=\\frac {C_1^5 C_{4}^{7}}{C_3^{12}}=0.4772\\]\nThere is a 47.72% chance that the inspector finds a defective fuse in his sample of three.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability II</span>"
    ]
  },
  {
    "objectID": "ProbabilityII.html#poisson-distribution",
    "href": "ProbabilityII.html#poisson-distribution",
    "title": "9  Probability II",
    "section": "9.6 Poisson Distribution",
    "text": "9.6 Poisson Distribution\nThe Poisson distribution estimates the number of successes (\\(x\\)) over a specified interval of time or space.\nThe probability mass function is given by: \\[f(x)= \\frac {\\mu^{x} e^{-\\mu}}{x!}\\]\nwhere \\(\\mu\\) is the expected number of successes in any given interval and also the variance, and \\(e\\) is Euler’s number (2.71828…).\nAn experiment satisfies a Poisson process if:\n\nThe number of successes with a specified time or space interval equals any integer between zero and infinity.\nThe number of successes counted in non-overlapping intervals are independent.\nThe probability of success in any interval is the same for all intervals of equal size and is proportional to the size of the interval.\n\nExample: Imagine a fast-food restaurant’s drive-thru where, on average, 10 cars arrive in a 15-minute span. What’s the likelihood of exactly 5 cars showing up in 15 minutes?\nWe can use the Poisson PMF function for this problem. The arrival rate is 10 cars every 15 minutes. So there are 10 successes in the 15 minute interval. We can substitute values into the PMF:\n\\[f(5)= \\frac {5^{10} e^{-5}}{5!}=0.0378\\] There is a 3.78% chance of five cars arriving within a 15 minute interval.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability II</span>"
    ]
  },
  {
    "objectID": "ProbabilityII.html#useful-r-functions",
    "href": "ProbabilityII.html#useful-r-functions",
    "title": "9  Probability II",
    "section": "9.7 Useful R Functions",
    "text": "9.7 Useful R Functions\nTo calculate probabilities based on discrete random variables use the dbinom(), dhyper(), and dpois() functions. For the uniform distribution use the extraDistr package and the dunif() function.\nLet’s confirm the results presented earlier in the chapter. We can confirm the result of the basketball example by using the dibinom() function. Below is the code:\n\ndbinom(3,3,0.9)\n\n[1] 0.729\n\n\nThe first argument is x (number of successes), the second argument is n (the number of trials), and the last argument is p (the probability of success). To confirm the result of the fuse box problem we can use the dhyper() function in R:\n\ndhyper(1,5,7,3)\n\n[1] 0.4772727\n\n\nThe first argument is one more time x (number of successes in the sample), the second argument is r (the number of successes in the population), the third argument is N-r (the total number of failures), and the fourth argument is n (the sample size). Finally, for the drive-thru example we use the dpois() function to confirm that the probability of five arrivals is 3.78%.\n\ndpois(5,10)\n\n[1] 0.03783327\n\n\nIn R you can additionally calculate cumulative probabilities by using the pbinom(), phyper(), ppois(), and pdunif() functions. Quantiles can be calculated using the qbinom(), qhyper(), qpois(), and qdunif() functions. Finally, to generate random numbers use the rbinom(), rhyper(), rpois(), and rdunif() functions.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability II</span>"
    ]
  },
  {
    "objectID": "DescriptiveV.html#skewness-in-r",
    "href": "DescriptiveV.html#skewness-in-r",
    "title": "5  Descriptive Stats V",
    "section": "5.11 Skewness in R",
    "text": "5.11 Skewness in R\nUnfortunately, base R does not have a function to calculate the Pearson Coefficient of skewness. Hence, we will have to install the e1071 package. The package contains the function skewness() which makes the estimation of skew simple. Below is the code to calculate the skewness for the eruptions variable.\n\nlibrary(e1071)\nskewness(faithful$eruptions)\n\n[1] -0.4135498\n\n\nThe result indicates that the distribution is mildly skewed to the left.\nHere is a summary of the functions used in this section:\n\nThe quantile() function returns the five point summary when no arguments are specified. For a specific quantile, specify the probs argument.\nThe scale() function calculates the z-scores for a vector of values.\nThe geom_boxplot() command returns a box plot for a vector of values.\nThe skewness() function is found in the e1071 package. It takes as an input a vector of values and returns the Pearson Coefficient of Skew.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Stats V</span>"
    ]
  },
  {
    "objectID": "InferenceI.html#statistical-inference",
    "href": "InferenceI.html#statistical-inference",
    "title": "11  Inference I",
    "section": "",
    "text": "The expected value of the sample means is equal to the population mean (i.e., the sample mean is unbiased). Formally, \\(E(\\bar x_i) = \\mu\\).\nThe standard deviation of the sample means is lower than the population standard deviation. \\(\\sigma_{\\bar x}= \\sigma/\\sqrt{n}\\). We call this measure the standard error.\nIf the population is normally distributed, then the sample means (\\(\\bar x\\)’s) are normally distributed.\nIf the population is not normally distributed, the the sample means are also normally distributed if the sample size is large (i.e., \\(n&gt;30\\)). This is known as the central limit theorem.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Inference I</span>"
    ]
  },
  {
    "objectID": "InferenceI.html#proportions",
    "href": "InferenceI.html#proportions",
    "title": "11  Inference I",
    "section": "11.2 Proportions",
    "text": "11.2 Proportions\nStatistical inference can also be performed with proportions (e.g. the proportion of left handed people, proportion of people with a particular virus, etc.). We can relate this proportions to the outcome of a binomial distribution. Recall that the binomial distribution describes the number of successes \\(x\\) in \\(n\\) trials of a Bernoulli process where \\(p\\) is the probability of success. Here, \\(x/n\\) is the proportion of successes.\n\nTo estimate the population proportion use the sample proportion \\(\\bar p = x/n\\). This estimate is unbiased (i.e., \\(E(\\bar p)=P\\)), where \\(P\\) is the population proportion.\nThe standard error of the estimate is \\(se(\\bar P)= \\sqrt { \\frac {p(1-p)}{n}}\\), where \\(p\\) is the sample proportion, and \\(n\\) is the sample size.\nBy the central limit theorem, the sampling distribution of \\(\\bar p\\) is approximately normal when \\(np \\geq 5\\) and \\(n(1-p)\\geq 5\\).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Inference I</span>"
    ]
  },
  {
    "objectID": "InferenceI.html#useful-r-functions",
    "href": "InferenceI.html#useful-r-functions",
    "title": "11  Inference I",
    "section": "11.3 Useful R Functions",
    "text": "11.3 Useful R Functions\nHere are some functions that are handy when simulating data in R.\nThe pnorm() and punif() functions calculate probabilities for the normal and uniform distributions, respectively.\nThe rnorm() and runif() functions generate random numbers from a normal and uniform distribution, respectively.\nThe for() function creates a loop that repeats a procedure a specified amount of times.\nThe set.seed() function is used to create reproducible results in R when random numbers are used.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Inference I</span>"
    ]
  },
  {
    "objectID": "InferenceII.html#useful-r-functions",
    "href": "InferenceII.html#useful-r-functions",
    "title": "12  Inference II",
    "section": "12.2 Useful R Functions",
    "text": "12.2 Useful R Functions\nThe qnorm() and qt() functions calculate quartiles for the normal and \\(t\\) distributions, respectively.\nThe if() function creates a conditional statement in R.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Inference II</span>"
    ]
  },
  {
    "objectID": "InferenceIII.html#hypothesis-testing",
    "href": "InferenceIII.html#hypothesis-testing",
    "title": "13  Inference III",
    "section": "",
    "text": "Specify the null and alternate hypothesis.\n\n\nFor means use:\n\n\\(H_o: \\mu \\leq 0\\); \\(Ha: \\mu &gt; \\mu_o\\) right-tail probability\n\\(H_o: \\mu \\geq 0\\); \\(Ha: \\mu &lt; \\mu_o\\) left-tail probability\n\\(H_o: \\mu = 0\\); \\(Ha: \\mu \\ne \\mu_o\\) two-tail probability\n\nFor proportions use:\n\n\\(H_o: P \\leq 0\\); \\(Ha: P &gt; P_o\\) right-tail probability\n\\(H_o: P \\geq 0\\); \\(Ha: P &lt; P_o\\) left-tail probability\n\\(H_o: P = 0\\); \\(Ha: P \\ne P_o\\) two-tail probability\n\n\n\nSpecify the confidence level (i.e., how likely you would be to see non-extreme data, when assuming the null is true. False negative tolerance) and significance level (i.e. how likely you would be to see extreme data, when assuming the null is true. False positive tolerance). Confidence levels are usually set at, \\(0.90\\), \\(0.95\\), or \\(0.99\\), which correspond to \\(10\\)%, \\(5\\)%, and \\(1\\)% significant levels, respectively.\nCalculate the test statistic.\n\nFor a test on means use \\(t_{df}= \\frac {\\bar x-\\mu_o}{s/\\sqrt{n}}\\), where \\(df=n-1\\), \\(\\bar x\\) is the sample mean, \\(\\mu_o\\) is the hypothesized value of \\(\\mu\\), \\(s\\) is the sample standard deviation, and \\(n\\) is the sample size.\nFor a test on proportions use \\(z= \\frac {\\bar p- P_o}{\\sqrt {P_o(1-P_o)/ n}}\\), where \\(\\bar p\\) is the sample proportion, \\(P_o\\) is the hypothesized value of the population proportion \\(P\\), and \\(n\\) is the sample size.\n\nFind the p-value (i.e., the likelihood of getting the observed or more extreme data, assuming the null hypothesis is true). (Substitute \\(t\\) for \\(z\\) if using proportions)\n\nFor a right-tail test, the \\(p\\)-value is \\(P(T\\geq t)\\).\nFor a left-tail test, the \\(p\\)-value is \\(P(T\\leq t)\\).\nFor a two-tail test, the \\(p\\)-value is \\(2P(T\\geq t)\\) if \\(t&gt;0\\) or \\(2P(T\\leq t)\\) if \\(t&lt;0\\).\n\nThe decision rule is to reject the null hypothesis when the \\(p-value&lt;\\alpha\\), and not to reject when \\(p-value \\geq alpha\\).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Inference III</span>"
    ]
  },
  {
    "objectID": "InferenceIII.html#useful-r-functions",
    "href": "InferenceIII.html#useful-r-functions",
    "title": "13  Inference III",
    "section": "13.2 Useful R Functions",
    "text": "13.2 Useful R Functions\nt.test() generates a \\(t\\)-test for a vector of values. Use the alternative argument to specify “greater”, “less” or “two.sided” test. The mu argument specifies the hypothesized value for the mean. The conf.level sets the confidence level of the test (0.9,0.95,0.99, etc.).\nprop.test() generates a proportion test when provided the number of successes and sample size.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Inference III</span>"
    ]
  },
  {
    "objectID": "InfReg.html#correlation-significance",
    "href": "InfReg.html#correlation-significance",
    "title": "14  Regression and Inference",
    "section": "",
    "text": "\\(H_o: \\rho \\geq 0\\); \\(H_a: \\rho &lt;0\\) left tail\n\\(H_o: \\rho \\leq 0\\); \\(H_a: \\rho &gt;0\\) right tail\n\\(H_o: \\rho = 0\\); \\(H_a: \\rho \\neq 0\\) two tails\n\n\n\n\nalternative: is a choice between “two.sided”, “less” and “greater”.\nconf.level: sets the confidence level. Enter as a decimal and not percentage.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Regression and Inference</span>"
    ]
  },
  {
    "objectID": "InfReg.html#difference-of-means-tests",
    "href": "InfReg.html#difference-of-means-tests",
    "title": "14  Regression and Inference",
    "section": "14.2 Difference of Means Tests",
    "text": "14.2 Difference of Means Tests\nTests for inference about the difference of two population means.\n\nThe test for unpaired mean differences (not equal variances) is given by \\(t_{df}= \\frac {(\\bar x_1 - \\bar x_2)- \\bar d_o}{\\sqrt {\\frac {s_1^2}{n_1} \\frac{s_2^2}{n_2}}}\\).\nThe test for unpaired mean difference (equal variances) is given by \\(t_{df}= \\frac {(\\bar x_1 - \\bar x_2)- \\bar d_o}{\\sqrt {s_p^2 (\\frac {1}{n_1} + \\frac {1}{n_2})}}\\).\nThe test for paired mean difference is given by \\(t_{df}= \\frac {\\bar d- d_o}{\\frac {s}{\\sqrt{n}}}\\).\n\nRun these test in R by using the t.test() function. Here is a list of arguments to use:\n\npaired: use True for paired, False for independent. The default is False.\nvar.equal: use True for equal variances, False for unequal. The default is False.\nmu: a value that indicate the hypothesized value of the mean or mean difference.\nalternative: is a choice between “two.sided”, “less” and “greater”.\nconf.level: sets the confidence level. Enter as a decimal and not percentage.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Regression and Inference</span>"
    ]
  },
  {
    "objectID": "InfReg.html#regression-inference",
    "href": "InfReg.html#regression-inference",
    "title": "14  Regression and Inference",
    "section": "14.3 Regression Inference",
    "text": "14.3 Regression Inference\nWhen running regression a couple of test can be performed on the coefficients to determine significance:\n\nThe first test competing hypothesis are \\(H_o: \\beta_j = 0\\); \\(H_a: \\beta_j \\ne 0\\). The test statistic for the intercept (slope) coefficient is given by \\(t_{df}= \\frac {b_j}{se(b_j)}\\).\nThe second test competing hypothesis are \\(H_o: \\beta_1=\\beta_2=...\\beta_k=0\\); \\(H_a:\\) at least one \\(\\beta_i \\neq 0\\). The joint test of significance is given by \\(F_{df_1,df_2} = \\frac {SSR/k}{SSE/(n-k-1)} = \\frac {MSR}{MSE}\\). The Anova table below shows more detail on this test.\n\n\n\n\n\n\n\n\n\n\n\n\nAnova\ndf\nSS\nMS\nF\nSignificance\n\n\n\n\nRegression\n\\(k\\)\n\\(SSR\\)\n\\(MSR=\\frac{SSR}{k}\\)\n\\(F_{df_1,df_2} = \\frac {MSR}{MSE}\\)\n\\(P(F) \\geq \\frac{MSR}{MSE}\\)\n\n\nResidual\n\\(n-k-1\\)\n\\(SSE\\)\n\\(MSE=\\frac {SSE}{n-k-1}\\)\n\n\n\n\nTotal\n\\(n-1\\)\n\\(SST\\)\n\n\n\n\n\n\nTo conduct these tests, save the lm() model into an object. The summary() function can then be used to retrieve the results of the tests on the model’s parameters. Use the anova() function to obtain the Anova table.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Regression and Inference</span>"
    ]
  },
  {
    "objectID": "ProbabilityIII.html#uniform-distribution",
    "href": "ProbabilityIII.html#uniform-distribution",
    "title": "10  Probability III",
    "section": "10.2 Uniform Distribution",
    "text": "10.2 Uniform Distribution\nThe uniform probability density function is given by: \\[f(x)= \\frac {1}{b-a}\\] when \\(a \\leq x \\leq b\\) and \\(0\\) otherwise. Here \\(b\\) is the upper limit of the distribution and \\(a\\) is the lower limit. The expected value of the uniform distribution is:\n\\[E(x)= \\frac {a+b}{2}\\]\nThe variance of the uniform distribution is\n\\[var(x)= \\frac {(b-a)^2} {12}\\] Example: Consider the travel time between NY and CHI. Typically, this trips can take anywhere from 120-140 minutes. The specific time of a particular flight is unpredictable, but assumed to be anywhere between this interval. We assume a uniform distribution with upper limit of 140 and lower limit of 120. The probability that the flight take 122 minutes is zero since the interval has an infinite amount of possible values. The probability of the flight taking 130 minutes or less is 0.5. \\[f(x&lt;=130)=\\frac{130-120}{20}=0.5\\] We can confirm this result in R using the punif() function. Below is the code:\n\npunif(130,120,140)\n\n[1] 0.5",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Probability III</span>"
    ]
  },
  {
    "objectID": "ProbabilityIII.html#normal-distribution",
    "href": "ProbabilityIII.html#normal-distribution",
    "title": "10  Probability III",
    "section": "10.3 Normal Distribution",
    "text": "10.3 Normal Distribution\nThe normal PDF is given by\n\\[f(x)= \\frac {1}{\\sigma \\sqrt{2\\pi}} e^{\\frac {-1}{2} (\\frac {x-\\mu}{\\sigma})}\\] where \\(\\mu\\) is the mean, \\(\\sigma\\) is the standard deviation, \\(\\pi\\) is 3.1415… , and \\(e\\) is 2.7282… . The normal distribution has the following properties:\n\nIt is symmetrical about the mean \\(\\mu\\).\nThe mean is at the middle and divides the area of the distribution into halves.\nThe total area under the curve is equal to 1.\nThe distribution is completely determined by its mean and standard deviation.\n\nThe standard normal has a mean of \\(0\\) and a standard deviation of \\(1\\). Otherwise, it has the exact same properties as the normal distribution.\nExample: Dr Tires is planning on offering a mileage guarantee on each set of tires they sell. They are considering a 40,000 mile guarantee. Given that the mean tire mileage is 36,500 miles with a standard deviation of 5,000 miles, we can estimate that the probability that a tire lasts more than 40,000 miles 75.8%. We can confirm this using R.\n\npnorm(40000,36500,5000)\n\n[1] 0.7580363",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Probability III</span>"
    ]
  },
  {
    "objectID": "ProbabilityIII.html#exponential-distribution",
    "href": "ProbabilityIII.html#exponential-distribution",
    "title": "10  Probability III",
    "section": "10.4 Exponential Distribution",
    "text": "10.4 Exponential Distribution\nThe exponential distribution is useful in computing probabilities for the time it takes to complete a task. It describes the time between events in a Poisson process.\nThe probability density function is given by \\[f(x)=1/{\\mu} \\times e^{\\frac{-x}{\\mu}}=\\lambda e^{-{\\lambda} x}\\] Example: Let \\(x\\) represent the loading time for a truck at the Dock Dash loading dock. If the average loading time is 15 minutes, the probability that loading a truck will take 6 minutes or less is 32.97%.\n\npexp(6,1/15)\n\n[1] 0.32968\n\n\nWe can also estimate that the probability that it takes between 6 and 18 minutes is 36.91%.\n\npexp(18,1/15)-pexp(6,1/15)\n\n[1] 0.3691258",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Probability III</span>"
    ]
  },
  {
    "objectID": "ProbabilityIII.html#triangular-distribution",
    "href": "ProbabilityIII.html#triangular-distribution",
    "title": "10  Probability III",
    "section": "10.5 Triangular Distribution",
    "text": "10.5 Triangular Distribution\nThe triangular distribution is characterized by a single mode (the peak of the distribution) and two boundaries. It is often used in situations where the lower and upper bounds of a potential outcome are known, but the exact likelihood of the outcome is uncertain.\nThe probability density function is given by \\[f(x)=\\frac {2(x-a)}{(b-a)(c-a)}\\] for \\(a \\leq x &lt; c\\)\n\\[f(x)=\\frac {2}{(b-a)}\\] for \\(x=c\\) and\n\\[f(x)=\\frac {2(b-x)}{(b-a)(b-c)}\\]\nfor \\(c &lt; x \\leq b\\), and \\(f(x)=0\\) otherwise. The expected value of the distribution is \\[E(x)= \\frac {a+b+c}{3}\\] The variance of the triangular distribution is\n\\[var(x) = \\frac {a^2+b^2+c^2-ab-ac-bc}{18}\\]\nExample: Bite Bliss is planning a new store in Williamsburg. It is estimated that the minimum weekly sales are 1000 and the maximum is 6000. They also estimate that the most likely outcome is around 3000. The probability that future sales will be between 2000 and 2500 is 12.5%.\n\nlibrary(extraDistr)\nptriang(2500,1000,6000,3000)-ptriang(2000,1000,6000,3000)\n\n[1] 0.125",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Probability III</span>"
    ]
  },
  {
    "objectID": "ProbabilityIII.html#useful-r-functions",
    "href": "ProbabilityIII.html#useful-r-functions",
    "title": "10  Probability III",
    "section": "10.6 Useful R Functions",
    "text": "10.6 Useful R Functions\nTo calculate the density of continuous random variables use the dunif(), dnorm(), and dexp() functions. For the triangular distribution use the extraDistr package and the dtriang() function.\nTo calculate probabilities of continuous random variables use the punif(), pnorm(), pexp(), and ptriang() functions.\nTo calculate quartiles of continuous random variables use the qunif(), qnorm(),qexp(), and qtriang() functions.\nTo calculate generate random variables based on continuous random variables use the runif(), rnorm(), rexp(), and rtriang() functions.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Probability III</span>"
    ]
  },
  {
    "objectID": "ProbabilityII.html#finding-probabilities-in-r",
    "href": "ProbabilityII.html#finding-probabilities-in-r",
    "title": "9  Probability II",
    "section": "9.7 Finding Probabilities in R",
    "text": "9.7 Finding Probabilities in R\nTo calculate probabilities based on discrete random variables use the dbinom(), dhyper(), and dpois() functions. For the uniform distribution use the extraDistr package and the dunif() function. For example, to confirm the results of the dice example use the following code:\n\nlibrary(extraDistr)\nddunif(2,1,6)\n\n[1] 0.1666667\n\n\nThe probability of rolling a 2 on a die is 0.167. The function’s first argument is \\(x\\), the second argument is the lower limit (i.e., 1) and the third argument is the upper limit (i.e., 6). To confirm the result of the basketball example, use the dibinom() function. Below is the code:\n\ndbinom(3,3,0.9)\n\n[1] 0.729\n\n\nThe first argument is x (number of successes), the second argument is n (the number of trials), and the last argument is p (the probability of success). To confirm the result of the fuse box problem we can use the dhyper() function in R:\n\ndhyper(1,5,7,3)\n\n[1] 0.4772727\n\n\nThe first argument is one more time x (number of successes in the sample), the second argument is r (the number of successes in the population), the third argument is N-r (the total number of failures), and the fourth argument is n (the sample size). Finally, for the drive-thru example we use the dpois() function to confirm that the probability of five arrivals is 3.78%.\n\ndpois(5,10)\n\n[1] 0.03783327",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability II</span>"
    ]
  },
  {
    "objectID": "ProbabilityII.html#cumulative-probabilities-in-r.",
    "href": "ProbabilityII.html#cumulative-probabilities-in-r.",
    "title": "9  Probability II",
    "section": "9.8 Cumulative Probabilities in R.",
    "text": "9.8 Cumulative Probabilities in R.\nTo calculate cumulative probabilities use the pbinom(), phyper(), ppois(), and pdunif() functions. For example, the probability of tossing a 3 or lower when tossing a die is 1/2. This is confirmed with the code:\n\npdunif(3,1,6)\n\n[1] 0.5\n\n\nLikewise, the probability of making at least 2 shots in our basketball example is:\n\npbinom(1,3,0.9, lower.tail = F)\n\n[1] 0.972\n\n\nNote the use of the lower.tail argument. This allows us to find the cumulative distribution to the right (i.e., upper tail of the distribution).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability II</span>"
    ]
  },
  {
    "objectID": "ProbabilityII.html#finding-quantiles-in-r",
    "href": "ProbabilityII.html#finding-quantiles-in-r",
    "title": "9  Probability II",
    "section": "9.9 Finding quantiles in R",
    "text": "9.9 Finding quantiles in R\nQuantiles can be calculated using the qbinom(), qhyper(), qpois(), and qdunif() functions. If we wanted to find the 70% percentile for the rolling die example we can write:\n\nqdunif(0.7,1,6)\n\n[1] 5\n\n\nThis means that about 70% of the values when rolling a die are less than or equal to 5. Note that 83.33% in fact of rolls are less than or equal to 5. R chooses the closes integer when calculating the quantile.\nLet’s recall the fuse box problem where an inspector samples some boxes and determines whether they are defective or not. The inputs for the problem are \\(N=12\\), \\(n=3\\), and \\(r=5\\). For this problem we can find the 80% percentile with the qhyper() function.\n\nqhyper(0.8,5,7,3)\n\n[1] 2\n\n\nThat is, 80% of the outcomes are at two or less defective items.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability II</span>"
    ]
  },
  {
    "objectID": "ProbabilityII.html#generating-random-numbers-using-r",
    "href": "ProbabilityII.html#generating-random-numbers-using-r",
    "title": "9  Probability II",
    "section": "9.10 Generating Random numbers using R",
    "text": "9.10 Generating Random numbers using R\nTo generate random numbers use the rbinom(), rhyper(), rpois(), and rdunif() functions. Let’s generate 100 random numbers from the basketball example to look at the distribution of outcomes.\n\nshots&lt;-rbinom(100,3,0.9)\n\nThe first input in the function is the number of random numbers we want to generate from the binomial distribution. We can now plot using ggplot.\n\nlibrary(tidyverse)\nlibrary(ggthemes)\nggplot() + geom_bar(aes(shots), col=\"black\",\n                    bg=\"grey\", alpha=0.5) +\n  theme_clean()\n\n\n\n\n\n\n\n\nThe graph shows how the shooter makes all three shots about 72 times out of 100 shots.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability II</span>"
    ]
  }
]