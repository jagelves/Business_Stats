[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Business Statistics",
    "section": "",
    "text": "Introduction\n“Whatever you would make habitual, practice it; and if you would not make a thing habitual, do not practice it, but accustom yourself to something else.” Epictetus\nThis course companion is designed to help you build mastery in statistics and its applications using R. Through practice, you will develop the skills and confidence needed to apply statistical concepts effectively. Each chapter begins with a list of key concepts to guide your learning, and the problems are crafted to reinforce these ideas through hands-on experience. If you need additional support while learning R, I encourage you to explore Grolemund (2014). Take your time, enjoy the process, and make practice a habit!",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#why-r",
    "href": "index.html#why-r",
    "title": "Business Statistics",
    "section": "Why R?",
    "text": "Why R?\nWe will be using R to apply the lessons we learn in BUAD 231. R is a language and environment for statistical computing and graphics. There are several advantages to using the R software for statistical analysis and data science. Some of the main benefits include:\n\nR is a powerful and flexible programming language that allows users to manipulate and analyze data in many different ways.\nR has a large and active community of users, who have developed a wide range of packages and tools for data analysis and visualization.\nR is free and open-source, which makes it accessible to anyone who wants to use it.\nR is widely used in academia and industry, which means that there are many resources and tutorials available to help users learn how to use it.\nR is well-suited for working with large and complex datasets, and it can handle data from many different sources.\nR can be easily integrated with other tools and software, such as databases, visualization tools, and machine learning algorithms.\n\nOverall, R is a powerful and versatile tool for data analysis and data science, and it offers many benefits to users who want to work with data.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#installing-r.",
    "href": "index.html#installing-r.",
    "title": "Business Statistics",
    "section": "Installing R.",
    "text": "Installing R.\nTo install R, visit the R webpage at https://www.r-project.org/. Once in the website, click on the CRAN hyperlink.\n\n\n\n\n\n\n\n\n\nHere you can select the CRAN mirror. Scroll down until you see USA. You are free to choose any mirror you like, I recommend using the Duke University mirror.\n\n\n\n\n\n\n\n\n\nOnce you click on the hyperlink, you will be prompted to choose the download for your operating system. Depending on your operating system, choose either a Windows or Macintosh download.\n\n\n\n\n\n\n\n\n\nFollow all prompts and complete installation.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#installing-rstudio",
    "href": "index.html#installing-rstudio",
    "title": "Business Statistics",
    "section": "Installing RStudio",
    "text": "Installing RStudio\nVisit the Posit website at https://posit.co. Once on the website, hover to the top of the screen and select “Open Source” from the drop down menus.\n\n\n\n\n\n\n\n\n\nNext, choose “R Studio IDE”.\n\n\n\n\n\n\n\n\n\nScroll down until you see the products. You want to download “RStudio Desktop” and make sure it is the free version.\n\n\n\n\n\n\n\n\n\nFinally, select “Download RStudio” and follow the instructions for installation.\n\n\n\n\n\n\n\n\n\nIt is important to note that RStudio will not work if R is not installed. You can think of R as the engine and RStudio as the interface.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#posit-cloud",
    "href": "index.html#posit-cloud",
    "title": "Business Statistics",
    "section": "Posit Cloud",
    "text": "Posit Cloud\nIf you do not wish to install R, you can always use the cloud version. To do this, visit https://posit.cloud/. On the main page click on the “Get Started” button.\n\n\n\n\n\n\n\n\n\nChoose the “Cloud Free” option and log in using your Google credentials (if you have a Google account) or sign up if you want to create a new account.\n\n\n\n\nGrolemund, Garret. 2014. “Hands-on Programming with r.” https://jjallaire.github.io/hopr/.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "DescriptiveI.html",
    "href": "DescriptiveI.html",
    "title": "1  Descriptive Stats I",
    "section": "",
    "text": "1.1 Motivation\nUnderstanding the nature and classification of data is crucial for effective analysis and decision-making. Data are the building blocks of insights, providing a foundation for businesses, researchers, and policymakers to make informed choices. Whether capturing a snapshot of a specific moment, tracking changes over time, or organizing information in structured or unstructured formats, how data is collected and categorized significantly impacts how it is analyzed and interpreted. This overview highlights key types of data and their unique characteristics to help you better understand their application in various contexts.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Descriptive Stats I</span>"
    ]
  },
  {
    "objectID": "DescriptiveI.html#data-and-types-of-data",
    "href": "DescriptiveI.html#data-and-types-of-data",
    "title": "1  Descriptive Stats I",
    "section": "1.2 Data and Types of Data",
    "text": "1.2 Data and Types of Data\nData are facts and figures collected, analyzed and summarized for presentation and interpretation. Data can be classified as:\n\nCross Sectional Data refers to data collected at the same (or approximately the same) point in time. Ex: NFL standings in 1980 or Country GDP in 2015.\nTime Series Data refers to data collected over several time periods. Ex: U.S. inflation rate from 2000-2010 or Tesla deliveries from 2016-2022.\nStructured Data resides in a predefined row-column format (tidy). Ex: spreadsheet data.\nUnstructured Data do not conform to a pre-defined row-column format. Ex: Text, video, and other multimedia.\n\nExample: Consider a retail store analyzing its sales performance. If the store collects data on the total revenue generated by each location on Black Friday, it is cross-sectional data. On the other hand, if the store tracks weekly sales for the past year to observe trends, it is time series data. Structured data, like sales figures stored in spreadsheets, allows for easy comparison and analysis. Meanwhile, customer feedback gathered from social media posts and video reviews represents unstructured data, requiring advanced tools to extract meaningful insights.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Descriptive Stats I</span>"
    ]
  },
  {
    "objectID": "DescriptiveI.html#data-sets",
    "href": "DescriptiveI.html#data-sets",
    "title": "1  Descriptive Stats I",
    "section": "1.3 Data Sets",
    "text": "1.3 Data Sets\nA data set contains all data collected for a particular study. Data sets are composed of:\n\nElements are the entities on which data are collected. Ex: Football teams, countries, and individuals.\nVariables are a set of characteristics collected for each element. Ex: Goals scored, GDP, weight.\nObservations are the set of measurements obtained for a particular element. Ex: Salah, 20 (goals), 15 (assists). US, 2.3 (inflation), 4.5% (federal interest rate).\n\n\n\n\nElements\nVariable 1\nVariable 2\n\n\n\n\nElement 1\n#\n#\n\n\nElement 2\n#\n#\n\n\nElement 3\n#\n#\n\n\n…\n…\n…\n\n\n\nExample: Consider the dataset on electric vehicles (EV’s) displayed below:\n\n\n\n\n\n\n\n\n\nIn this dataset, each row represents an electric vehicle model, making the elements the specific EV models rather than the manufacturers. The variables collected for each model include:\n\nMake: The manufacturer of the EV.\nModel: The specific name of the EV model.\nRange_km: Driving range in kilometers on a full charge.\nTopSpeed_kmh: Maximum speed in km/h.\nPrice_pounds: Price in pounds (£).\nCharge_kmh: Charging speed in kilometers per hour.\n\nAn example observation is “Tesla Model 3,” with the following data: Make: Tesla, Model: Model 3, Range_km: 415, TopSpeed_kmh: 201, Price_pounds: 39,990, Charge_kmh: 690.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Descriptive Stats I</span>"
    ]
  },
  {
    "objectID": "DescriptiveI.html#scales-of-measurement",
    "href": "DescriptiveI.html#scales-of-measurement",
    "title": "1  Descriptive Stats I",
    "section": "1.4 Scales of Measurement",
    "text": "1.4 Scales of Measurement\nUnderstanding scales of measurement is crucial for analyzing and interpreting data effectively in business. By distinguishing between categorical (e.g., marital status, satisfaction ratings) and numerical data (e.g., profits, prices), you’ll know what methods to use for analysis. Knowing whether data is nominal, ordinal, interval, or ratio ensures your analysis and conclusions are accurate and relevant.\nThe scales of measurements determine the amount and type of information contained in each variable. In general, variables can be classified as categorical or numerical.\n\nCategorical (qualitative) data includes labels or names to identify an attribute of each element. Categorical data can be nominal or ordinal.\n\nWith nominal data, the order of the categories is arbitrary. Ex: Marital Status, Race/Ethnicity, or NFL division.\nWith ordinal data, the order or rank of the categories is meaningful. Ex: Rating, Difficulty Level, or Spice Level.\n\nNumerical (quantitative) include numerical values that indicate how many (discrete) or how much (continuous). The data can be either interval or ratio.\n\nWith interval data, the distance between values is expressed in terms of a fixed unit of measure. The zero value is arbitrary and does not represent the absence of the characteristic. Ratios are not meaningful. Ex: Temperature or Dates.\nWith ratio data, the ratio between values is meaningful. The zero value is not arbitrary and represents the absence of the characteristic. Ex: Prices, Profits, Wins.\n\n\nExample: Let’s keep using the EV example. Consider the new data set below:\n\n\n\n\n\n\n\n\n\nThe variables can be classified as follows: Car (Categorical - Nominal), consists of names of cars, which are labels used to identify each row. The order of these names does not matter, making it nominal data. Brand (Categorical - Nominal) represents the manufacturer of the car (e.g., Ford, Audi). These are labels with no inherent order, making it nominal data. Range (Numerical - Ratio), refers to the car’s driving range in miles. It is numerical and ratio because it has a meaningful zero (a car with zero range cannot move), and ratios are meaningful (e.g., a car with 250 miles range has double the range of one with 125 miles). Rating (Categorical - Ordinal) represents a rank or score (e.g., 4, 3, 2). The order matters, as higher ratings indicate better performance. However, the intervals between ratings are not consistent, so it is ordinal data. Year (Numerical - Interval) represents a point in time. While numerical, it is interval data because the zero point is arbitrary (e.g., year 0 does not indicate the “absence” of time), and ratios are not meaningful (e.g., 2020 is not “twice as late” as 1010).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Descriptive Stats I</span>"
    ]
  },
  {
    "objectID": "DescriptiveI.html#useful-base-r-functions",
    "href": "DescriptiveI.html#useful-base-r-functions",
    "title": "1  Descriptive Stats I",
    "section": "1.5 Useful Base R Functions",
    "text": "1.5 Useful Base R Functions\nUnderstanding and using Base R functions is essential for efficiently managing and analyzing data. Functions like na.omit() help clean datasets by removing rows with missing values, ensuring your analyses are accurate and complete. nrow() and ncol() quickly provide insights into the size of your dataset, while is.na() allows you to identify and address missing data. The summary() function is a powerful way to generate descriptive statistics and assess the overall structure of your data at a glance. Additionally, coercion functions like as.integer(), as.factor(), and as.double() enable you to convert variables to appropriate data types, ensuring compatibility with different analysis methods.\n\nThe na.omit() function removes any observations that have a missing value (NA). The resulting data frame has only complete cases. Input: A data frame (tibble) or vector.\nThe nrow() and ncol() functions return the number of rows and columns respectively from a data frame. Input: A data frame (tibble).\nThe is.na() function returns a vector of True and False that specify if an entry is missing (NA) or not. Input: A data frame (tibble) or vector.\nThe summary() function returns a collection of descriptive statistics from a data frame (or vector). The function also returns whether there are any missing values (NA) in a variable. Input: A data frame (tibble) or vector.\nThe as.integer(), as.factor(), as.double(), are functions used to coerce your data into a different scale of measurement. Input: A vector or column of a data frame (tibble).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Descriptive Stats I</span>"
    ]
  },
  {
    "objectID": "DescriptiveI.html#useful-dplyr-functions",
    "href": "DescriptiveI.html#useful-dplyr-functions",
    "title": "1  Descriptive Stats I",
    "section": "1.6 Useful DPLYR Functions",
    "text": "1.6 Useful DPLYR Functions\nThe dplyr package has a collection of functions that are useful for data manipulation and transformation. If you are interested in this package you can refer to Wickham (2017). To install, run the following command in the console install.packages(\"dplyr\").\n\nThe arrange() function allows you to sort data frames in ascending order. Pair with the desc() function to sort the data in descending order.\nThe filter() function allows you to subset the rows of your data based on a condition.\nThe select() function allows you to select a subset of variables from your data frame.\nThe mutate() function allows you to create a new variable.\nThe group_by() function allows you to group your data frame by categories present in a given variable.\nThe summarise() function allows you to summarise your data, based on groupings generated by the goup_by() function.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Descriptive Stats I</span>"
    ]
  },
  {
    "objectID": "DescriptiveI.html#exercises",
    "href": "DescriptiveI.html#exercises",
    "title": "1  Descriptive Stats I",
    "section": "1.7 Exercises",
    "text": "1.7 Exercises\nThe following exercises will help you test your knowledge on the Scales of Measurement. They will also allow you to practice some basic data “wrangling” in R. In these exercises you will:\n\nIdentify numerical and categorical data.\nClassify data according to their scale of measurement.\nSort and filter data in R.\nHandle missing values (NA’s) in R.\n\nAnswers are provided below. Try not to peak until you have a formulated your own answer and double checked your work for any mistakes.\n\nExercise 1\nA bookstore has compiled data set on their current inventory. A portion of the data is shown below:\n\n\n\nTitle\nPrice\nYear Published\nRating\n\n\n\n\nFrankenstein\n5.49\n1818\n4.2\n\n\nDracula\n7.60\n1897\n4.0\n\n\n…\n…\n…\n…\n\n\nSleepy Hollow\n6.95\n1820\n3.8\n\n\n\n\nWhich of the above variables are categorical and which are numerical?\n\n\n\nAnswer\n\nThe “Title” variable represents the names of books. Therefore, this is a categorical variable. “Price” represents the cost of each book in a numeric format, making it a numerical variable. “Year Published” indicates the publication year of each book. It is numerical. If “Rating” represents a numerical score based on a continuous scale (e.g., average user ratings on a platform like Goodreads), it is numerical because arithmetic operations like averaging or comparing differences are meaningful. If “Rating” represents predefined categories (e.g., “Excellent,” “Good,” “Fair,” “Poor”) or is interpreted as ranks without meaningful differences between values, it would be categorical.\n\n\nWhat is the measurement scale of each of the above variable?\n\n\n\nAnswer\n\nThe measurement scale is nominal for Title since these are labels used to identify each book and do not have a numerical meaning or order. If Rating represents a score (e.g., 4.2, 4.0) given to each book, it is numerical and could be considered interval data because the scale represents a meaningful difference, but it may not have an absolute zero or meaningful ratios (e.g., a book rated 4.0 is not “twice as good” as one rated 2.0). Price is a measurable quantity with a meaningful zero (e.g., a book priced at $0 means it is free), making it ratio data. Year is interval data because the zero point is arbitrary (year 0 does not represent the absence of time) and differences between years are meaningful (e.g., 1897 - 1818 = 79 years).\n\n\n\nExercise 2\nA car company tracks the number of deliveries every quarter. A portion of the data is shown below:\n\n\n\nYear\nQuarter\nDeliveries\n\n\n\n\n2016\n1\n14800\n\n\n2016\n2\n14400\n\n\n…\n…\n…\n\n\n2022\n3\n343840\n\n\n\n\nWhat is the measurement scale of the Year variable? What are the strengths and weaknesses of this type of measurement scale?\n\n\n\nAnswer\n\nThe variable Year is measured on the interval scale because the observations can be ranked, categorized and measured when using this kind of scale. However, there is no true zero point so we cannot calculate meaningful ratios between years.\n\n\nWhat is the measurement scale for the Quarter variable? What is the weakness of this type of measurement scale?\n\n\n\nAnswer\n\nThe variable Quarter is measured on the ordinal scale, even though it contains numbers. It is the least sophisticated level of measurement because if we are presented with nominal data, all we can do is categorize or group the data.\n\n\nWhat is the measurement scale for the Deliveries variable? What are the strengths of this type of measurement scale?\n\n\n\nAnswer\n\nThe variable Deliveries is measured on the ratio scale. It is the strongest level of measurement because it allows us to categorize and rank the data as well as find meaningful differences between observations. Also, with a true zero point, we can interpret the ratios between observations.\n\n\n\nExercise 3\nUse the airquality data set included in R for this problem.\n\nSort the data by Temp in descending order. What is the day and month of the first observation on the sorted data?\n\n\n\nAnswer\n\nThe day and month of the first observation is August 28th.\nThe easiest way to sort in R is by using the dplyr package. Specifically, the arrange() function within the package. Let’s also use the desc() function to make sure that the data is sorted in descending order. We can use indexing to retrieve the first row of the sorted data set.\n\nlibrary(dplyr)\nSortedAQ&lt;-arrange(airquality,desc(Temp))\nSortedAQ[1,]\n\n  Ozone Solar.R Wind Temp Month Day\n1    76     203  9.7   97     8  28\n\n\n\n\nSort the data only by Temp in descending order. Of the \\(10\\) hottest days, how many of them were in July?\n\n\n\nAnswer\n\nWe can use the arrange() function one more time for this question. Then we can use indexing to retrieve the top \\(10\\) observations.\n\nSortedAQ2&lt;-arrange(airquality,desc(Temp))\nSortedAQ2[1:10,]\n\n   Ozone Solar.R Wind Temp Month Day\n1     76     203  9.7   97     8  28\n2     84     237  6.3   96     8  30\n3    118     225  2.3   94     8  29\n4     85     188  6.3   94     8  31\n5     NA     259 10.9   93     6  11\n6     73     183  2.8   93     9   3\n7     91     189  4.6   93     9   4\n8     NA     250  9.2   92     6  12\n9     97     267  6.3   92     7   8\n10    97     272  5.7   92     7   9\n\n\n\n\nHow many missing values are there in the data set? What rows have missing values for Solar.R?\n\n\n\nAnswer\n\nThere are a total of \\(44\\) missing values. Ozone has \\(37\\) and Solar.R has \\(7\\). Rows \\(5\\), \\(6\\), \\(11\\), \\(27\\), \\(96\\), \\(97\\), \\(98\\) are missing for Solar.R.\nWe can easily identify missing values with the summary() function.\n\nsummary(airquality)\n\n     Ozone           Solar.R           Wind             Temp      \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n NA's   :37       NA's   :7                                       \n     Month            Day      \n Min.   :5.000   Min.   : 1.0  \n 1st Qu.:6.000   1st Qu.: 8.0  \n Median :7.000   Median :16.0  \n Mean   :6.993   Mean   :15.8  \n 3rd Qu.:8.000   3rd Qu.:23.0  \n Max.   :9.000   Max.   :31.0  \n                               \n\n\nTo view the rows that have NA’s in them, we can use the is.na() function and indexing. Below we see that \\(7\\) values are missing for the Solar.R variable in the months \\(5\\) and \\(8\\) combined.\n\nairquality[is.na(airquality$Solar.R),]\n\n   Ozone Solar.R Wind Temp Month Day\n5     NA      NA 14.3   56     5   5\n6     28      NA 14.9   66     5   6\n11     7      NA  6.9   74     5  11\n27    NA      NA  8.0   57     5  27\n96    78      NA  6.9   86     8   4\n97    35      NA  7.4   85     8   5\n98    66      NA  4.6   87     8   6\n\n\n\n\nRemove all observations that have a missing values. Create a new object called CompleteAG.\n\n\n\nAnswer\n\nTo create the new object of complete observations we can use the na.omit() function.\n\nCompleteAQ&lt;-na.omit(airquality)\n\n\n\nWhen using CompleteAG, how many days was the temperature at least \\(60\\) degrees?\n\n\n\nAnswer\n\nThere were \\(107\\) days where the temperature was at least \\(60\\).\nUsing base R we have:\n\nnrow(CompleteAQ[CompleteAQ$Temp&gt;=60,])\n\n[1] 107\n\n\nWe can also use dplyr for this question. Specifically, using the filter() and nrow() functions we get:\n\nnrow(filter(CompleteAQ,Temp&gt;=60))\n\n[1] 107\n\n\n\n\nWhen using CompleteAG, how many days was the temperature within [\\(55\\),\\(75\\)] degrees and an Ozone below \\(20\\)?\n\n\n\nAnswer\n\nThere were \\(24\\) days where the temperature was between \\(55\\) and \\(75\\) and the ozone level was below \\(20\\).\nUsing base R we have:\n\nnrow(CompleteAQ[CompleteAQ$Temp&gt;55 & CompleteAQ$Temp&lt;75 & CompleteAQ$Ozone&lt;20,])\n\n[1] 24\n\n\nUsing the filter() function once more we get:\n\nnrow(filter(CompleteAQ,Temp&gt;55,Temp&lt;75,Ozone&lt;20))\n\n[1] 24\n\n\n\n\n\nExercise 4\nUse the Packers data set for this problem. You can find the data set at https://jagelves.github.io/Data/Packers.csv\n\nRemove the any observation that has a missing value with the na.omit() function. How many observations are left in the data set?\n\n\n\nAnswer\n\nThere are \\(84\\) observations in the complete cases data set.\nLet’s import the data to R by using the read.csv() function.\n\nPackers&lt;-read.csv(\"https://jagelves.github.io/Data/Packers.csv\")\n\nWe can remove any missing observation by using the na.omit() function. We can name this new object Packers2.\n\nPackers2&lt;-na.omit(Packers)\n\nTo find the number of observations we can use the dim() function. It returns the number of observations and variables.\n\ndim(Packers2)\n\n[1] 84  8\n\n\n\n\nDetermine the type of the Experience variable by using the typeof() function. What type is the variable?\n\n\n\nAnswer\n\nThe type is character.\nUse the typeof() function on the Experience variable.\n\ntypeof(Packers2$Experience)\n\n[1] \"character\"\n\n\n\n\nRemove observations that have an “R” and coerce the Experience variable to an integer using the as.integer() function. What is the total sum of years of experience?\n\n\n\nAnswer\n\nThe total sum of experience is \\(288\\).\nFirst, remove any observation with an R by using indexing and logicals.\n\nPackers2&lt;-Packers2[Packers2$Experience!=\"R\",]\n\nNow we can coerce the variable to an integer by using the as.integer() function.\n\nPackers2$Experience&lt;-as.integer(Packers2$Experience)\n\nLastly, calculate the sum using the sum() function.\n\nsum(Packers2$Experience)\n\n[1] 288\n\n\n\n\n\n\n\nWickham, Hadley. 2017. “R for Data Science.” https://r4ds.hadley.nz.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Descriptive Stats I</span>"
    ]
  },
  {
    "objectID": "DescriptiveII.html",
    "href": "DescriptiveII.html",
    "title": "2  Descriptive Stats II",
    "section": "",
    "text": "2.1 Frequency Distributions (Categorical)\nUnderstanding and visualizing data distribution is a fundamental step in data analysis. A frequency distribution organizes a structured data summary into non-overlapping classes, allowing for insights into patterns and trends. Complementary to this, relative frequency, cumulative frequency, and cumulative relative frequency offer deeper perspectives on the proportions and accumulation of data within these classes. Visualization techniques, including bar plots and histograms, play a crucial role in representing these distributions, with bar plots suited for qualitative data and histograms tailored for quantitative data. The R package ggplot2, has functions like geom_bar() and geom_hist() to plot distributions efficiently. By leveraging these methods, data can be transformed into clear and meaningful insights.\nA frequency distribution is perhaps the most valuable tool for summarizing categorical data. It illustrates with a table the number of items within distinct, non-overlapping categories. Presenting the data in a tabular format makes it easier to identify patterns and trends within the categories. A key component of this analysis is the relative frequency, which quantifies the proportion of items in each category relative to the total number of observations. You can calculate it by taking the frequency of a particular class (\\(f_{i}/n\\)), and dividing it by the total frequency \\(n\\). Relative frequency helps contextualize the data by highlighting the significance of each category compared to the whole.\nExample: Consider data on students’ answers to the question, what is your favorite food? You can see the data below:\nSimply observing raw data can make identifying the most and least popular items challenging. A frequency distribution organizes this information into a clear table, showcasing the popularity of each item. The frequency distribution of the table is displayed below:\nFood\nFrequency\nRelative\n\n\n\n\nChicken\n5\n0.20\n\n\nPasta\n4\n0.16\n\n\nPizza\n6\n0.24\n\n\nSushi\n10\n0.40\nEach food item is tallied up, and the result is shown in the Frequency column. We can also show the tally result as a ratio of the total food items recorded in the data (i.e., 25). For example, five students liked chicken; out of the 25 students surveyed, this represents 0.2 or 20%. This approach makes it much easier to pinpoint the most popular and the least popular items. Below, you can see the bar graph showing the frequency distribution of the food items data. Note that the visualization is constructed by showing each food item as a bar with a height equal to the frequency.\nIn sum, the bar plot illustrates the frequency distribution of categorical data. It includes the classes in the horizontal axis and frequencies or relative frequencies in the vertical axis and has gaps between each bar.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Stats II</span>"
    ]
  },
  {
    "objectID": "DescriptiveII.html#concepts",
    "href": "DescriptiveII.html#concepts",
    "title": "2  Descriptive Stats II",
    "section": "",
    "text": "Frequency\nA frequency distribution is a tabular summary of data showing the number of items in each of several non-overlapping classes.\n\nThe relative frequency is calculated by \\(f_{i}/n\\), where \\(f_{i}\\) is the frequency of class \\(i\\) and \\(n\\) is the total frequency.\nThe cumulative frequency shows the number of data items with values less than or equal to the upper class limit of each class.\nThe cumulative relative frequency is given by \\(cf_{i}/n\\), where \\(cf_{i}\\) is the cumulative frequency of class \\(i\\).\n\n\n\nPlots\nA bar plot illustrates the frequency distribution of qualitative data.\n\nIs an illustration for qualitative data.\nIncludes the classes in the horizontal axis and frequencies or relative frequencies in the vertical axis.\nHas gaps between each bar.\n\nA histogram illustrates the frequency distribution of quantitative data.\n\nIs an illustration for quantitative data.\nThere are no gaps between the bars.\nThe number, width and limits of each class must be determined.\n\nThe number of classes can be determined by the \\(2^k\\) rule: select \\(k\\) such that \\(2^k\\) is greater than the number of observations by the smallest amount.\nThe width of the class is approximately range/(# of Classes). The value should be rounded up.\nThe limits should be chosen so that each point belongs to only one class.\n\n\n\n\nUseful R Functions\nThe table() command generates frequency distributions or contingency tables if two variables are used.\nThe prop.table() command generates relative frequency distributions from an object that contains a table.\nThe cut() function generates class limits and bins used in frequency distributions (and histograms) for quantitative data.\nBase R has the barplot() function for categorical variable, histogram() function for numerical data, and the plot() function for line charts or scatter plots. Below are some arguments that are helpful when plotting.\n\nmain: used to set the plot’s title. The title should be entered as a character.\ncol: used to set the color of the plot. Hex and RGB values are allowed as inputs. The color should be entered as a character.\nxlab and ylab: are used to set the labels for the \\(x\\) and \\(y\\) axis respectively. The labels should be entered as characters.\nlegend() is a function to customize the legend of a graph. This argument may be used with the plot(), barplot() or histogram() functions.\n\nx: used to set the location of the legend in the plotting area. Ex: “bottomleft”.\nlegend: a vector specifying the legend names to be included.\ncol: a vector specifying the color of each item in the legend.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Stats II</span>"
    ]
  },
  {
    "objectID": "DescriptiveII.html#exercises",
    "href": "DescriptiveII.html#exercises",
    "title": "2  Descriptive Stats II",
    "section": "2.4 Exercises",
    "text": "2.4 Exercises\nThe following exercises will help you practice summarizing data with tables and simple graphs. In particular, the exercises work on:\n\nDeveloping frequency distributions for both categorical and numerical data.\nConstructing bar charts, histograms, and line charts.\nCreating contingency tables.\n\nAnswers are provided below. Try not to peak until you have a formulated your own answer and double checked your work for any mistakes.\n\nExercise 1\nInstall the ISLR2 package in R. You will need the BrainCancer data set to answer this question.\n\nConstruct a frequency and relative frequency table of the Diagnosis variable. What was the most common diagnosis? What percentage of the sample had this diagnosis?\n\n\n\nAnswer\n\nThe most common diagnosis is Meningioma, a slow-growing tumor that forms from the membranous layers surrounding the brain and spinal cord. The diagnosis represents about \\(48.28\\)% of the sample.\nStart by loading the ISLR2 package. To construct the frequency distribution table, use the table() function.\n\nlibrary(ISLR2)\ntable(BrainCancer$diagnosis)\n\n\nMeningioma  LG glioma  HG glioma      Other \n        42          9         22         14 \n\n\nThe relative frequency distribution can be easily retrieved by saving the frequency table in an object and then using the prop.table() function.\n\nfreq&lt;-table(BrainCancer$diagnosis)\nprop.table(freq)\n\n\nMeningioma  LG glioma  HG glioma      Other \n 0.4827586  0.1034483  0.2528736  0.1609195 \n\n\n\n\nConstruct a bar chart. Summarize the findings.\n\n\n\nAnswer\n\nThe majority of diagnosis are Meningioma. Low grade glioma is the least common of diagnosis. High grade glioma and other diagnosis have about the same frequency.\nTo construct the bar chart use the geom_bar() function from tidyverse.\n\nlibrary(tidyverse)\nlibrary(ggthemes)\nggplot(data=BrainCancer) + \n  geom_bar(aes(diagnosis), alpha=0.5, col=\"black\") + \n  theme_clean()\n\n\n\n\n\n\n\n\n\n\nConstruct a contingency table that shows the Diagnosis along with the Status. Which diagnosis had the highest number of non-survivals (0)? What was the survival rate of this diagnosis?\n\n\n\nAnswer\n\n\\(33\\) people did not survive Meningioma. The survival rate of Meningioma is only \\(21.43\\)%.\nUse the table() function one more time to create the contingency table for the two variables.\n\n(freq2&lt;-table(BrainCancer$status,BrainCancer$diagnosis))\n\n   \n    Meningioma LG glioma HG glioma Other\n  0         33         5         5     9\n  1          9         4        17     5\n\n\nTo get the survival rates, we can use the prop.table() function once again.\n\nprop.table(freq2,margin = 2)\n\n   \n    Meningioma LG glioma HG glioma     Other\n  0  0.7857143 0.5555556 0.2272727 0.6428571\n  1  0.2142857 0.4444444 0.7727273 0.3571429\n\n\n\n\n\nExercise 2\nYou will need the airquality data set (in base R) to answer this question.\n\nConstruct a frequency distribution for Temp. Use five intervals with widths of \\(50&lt;x\\le60\\); \\(60&lt;x\\le70\\); etc. Which interval had the highest frequency? How many times was the temperature between \\(50\\) and \\(60\\) degrees?\nConstruct a relative frequency, cumulative frequency and the relative cumulative frequency distributions. What proportion of the time was Temp between \\(50\\) and \\(60\\) degrees? How many times was the Temp \\(70\\) degrees or less? What proportion of the time was Temp more than \\(70\\) degrees?\nConstruct the histogram. Is the distribution symmetric? If not, is it skewed to the left or right?\n\n\n\nExercise 3\nYou will need the Portfolio data set from the ISLR2 package to answer this question.\n\nConstruct a line chart that shows the returns over time for each portfolio (X and Y) by using two lines each with a unique color. Assume the data is for the period \\(1901\\) to \\(2000\\). Include also a legend that matches colors to portfolios.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Stats II</span>"
    ]
  },
  {
    "objectID": "DescriptiveII.html#answers",
    "href": "DescriptiveII.html#answers",
    "title": "2  Descriptive Stats II",
    "section": "2.5 Answers",
    "text": "2.5 Answers\n\nExercise 1\n\nThe most common diagnosis is Meningioma, a slow-growing tumor that forms from the membranous layers surrounding the brain and spinal cord. The diagnosis represents about \\(48.28\\)% of the sample.\n\nStart by loading the ISLR2 package. To construct the frequency distribution table, use the table() function.\n\nlibrary(ISLR2)\ntable(BrainCancer$diagnosis)\n\n\nMeningioma  LG glioma  HG glioma      Other \n        42          9         22         14 \n\n\nThe relative frequency distribution can be easily retrieved by saving the frequency table in an object and then using the prop.table() function.\n\nfreq&lt;-table(BrainCancer$diagnosis)\nprop.table(freq)\n\n\nMeningioma  LG glioma  HG glioma      Other \n 0.4827586  0.1034483  0.2528736  0.1609195 \n\n\n\nThe majority of diagnosis are Meningioma. Low grade glioma is the least common of diagnosis. High grade glioma and other diagnosis have about the same frequency.\n\nTo construct the bar chart use the barplot() function in R.\n\nbarplot(freq, col = \"#F5F5F5\", ylim=c(0,50))\n\n\n\n\n\n\n\n\n\n\\(33\\) people did not survive Meningioma. The survival rate of Meningioma is only \\(21.43\\)%.\n\nUse the table() function one more time to create the contingency table for the two variables.\n\n(freq2&lt;-table(BrainCancer$status,BrainCancer$diagnosis))\n\n   \n    Meningioma LG glioma HG glioma Other\n  0         33         5         5     9\n  1          9         4        17     5\n\n\nTo get the survival rates, we can use the prop.table() function once again.\n\nprop.table(freq2,margin = 2)\n\n   \n    Meningioma LG glioma HG glioma     Other\n  0  0.7857143 0.5555556 0.2272727 0.6428571\n  1  0.2142857 0.4444444 0.7727273 0.3571429\n\n\n\nMeningioma and not surviving is the most common with \\(33\\) occurrences. High grade glioma and surviving is the the second most common.\n\nUse the barplot() function one more time to construct the stacked column chart.\n\nbarplot(table(BrainCancer$status,BrainCancer$diagnosis),\n        legend.text = c(\"Not Survived\",\"Survived\"), ylim=c(0,50))\n\n\n\n\n\n\n\n\n\n\nExercise 2\n\nThe highest frequency is in the \\(80 &lt; x ≤ 90\\) bin. \\(8\\) temperatures were between \\(50 &lt; x ≤ 60\\) degrees.\n\nCreate a vector containing the intervals desired by using the seq() function.\n\nintervals &lt;- seq(50, 100, by=10)\n\nNext use the cut() function to create the cuts for the histogram.\n\nintervals.cut &lt;- cut(airquality$Temp, intervals, left=FALSE, right=TRUE)\n\nThe frequency distribution can be obtained by using the table() function on the interval.cut object created above.\n\ntable(intervals.cut)\n\nintervals.cut\n (50,60]  (60,70]  (70,80]  (80,90] (90,100] \n       8       25       52       54       14 \n\n\n\nThe temperature was \\(5.22\\)% of the time between \\(50\\) and \\(60\\); The temperature was \\(70\\) or less \\(33\\) times; The temperature was above \\(70\\), \\(78.43\\)% of the time.\n\nTo get the relative frequency table, start by saving the proportion table into an object.Then you can use the prop.table() function.\n\nfreq&lt;-table(intervals.cut) \nprop.table(freq)\n\nintervals.cut\n   (50,60]    (60,70]    (70,80]    (80,90]   (90,100] \n0.05228758 0.16339869 0.33986928 0.35294118 0.09150327 \n\n\nFor the cumulative distribution you can use the cumsum() function on the frequency distribution.\n\ncumulfreq&lt;-cumsum(freq)\ncumulfreq\n\n (50,60]  (60,70]  (70,80]  (80,90] (90,100] \n       8       33       85      139      153 \n\n\nLastly, for the relative cumulative distribution table, you can use the cumsum() function on the relative frequency table.\n\ncumsum(prop.table(freq))\n\n   (50,60]    (60,70]    (70,80]    (80,90]   (90,100] \n0.05228758 0.21568627 0.55555556 0.90849673 1.00000000 \n\n\n\nThe distribution is not perfectly symmetric. It is skewed slightly to the left (see histogram.)\n\nUse the hist() function to create the histogram.\n\nhist(airquality$Temp, breaks=intervals, \n     right=TRUE,col=\"#F5F5F5\", main=\"Temperature in NY\", xlab=\"\")\n\n\n\n\n\n\n\n\n\n\nExercise 3\n\nFrom \\(1901\\) through \\(2000\\), both portfolios have behaved very similarly. Returns are between \\(-3\\)% and \\(3\\)%, there is no trend, and positive (negative) returns for X seem to match with positive (negative) returns of Y.\n\nYou can use the plot() function to create a plot of Portfolio Y. The line for Portfolio X can be added with the lines() function.\n\nplot(Portfolio$Y, \n     x=seq(1901,2000), type=\"l\", \n     col=\"black\", xlab=\"\", ylab=\"% Return\", ylim=c(-3,3), \n     xlim=c(1901,2000), lwd=2, axes = F)\naxis(side=1, labels=TRUE, font=1,las=1)\naxis(side=2, labels=TRUE, font=1,las=1)\nlines(Portfolio$X, x=seq(1901,2000), type=\"l\", \n      col=\"darkgrey\", lwd=2)\nlegend(x = \"bottomleft\",          \n       legend = c(\"Port Y\", \"Port X\"),  \n       lty = c(1, 1),           \n       col = c(\"black\", \"darkgrey\"),         \n       lwd = 2,\n       bty=\"n\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Stats II</span>"
    ]
  },
  {
    "objectID": "DescriptiveIII.html",
    "href": "DescriptiveIII.html",
    "title": "3  Descriptive Statistics III",
    "section": "",
    "text": "3.1 Concepts",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics III</span>"
    ]
  },
  {
    "objectID": "DescriptiveIII.html#concepts",
    "href": "DescriptiveIII.html#concepts",
    "title": "3  Descriptive Statistics III",
    "section": "",
    "text": "Measures of Central Location\nMeasures of Central Location determine where the center of a distribution lies.\n\nThe mean is the average value for a numerical variable. The sample statistic is estimated by \\(\\bar{x}=\\sum x_{i}/n\\), where \\(x_i\\) is observation \\(i\\), and \\(n\\) is the number of observations. The population parameter is defined as \\(\\mu=\\sum x_{i}/N\\).\nThe median is the value in the middle when data is organized in ascending order. When \\(n\\) is even, the median is the average between the two middle values.\nThe mode is the value with highest frequency from a set of observations.\nThe weighted mean uses weights to determine the importance of each data point of a variable. It is calculated by \\(\\frac{\\sum w_{i}x_{i}}{\\sum w_{i}}\\), where \\(w_{i}\\) are the weights associated to the values \\(x_{i}\\).\nThe geometric mean is a multiplicative average that is less sensitive to outliers. It is used to average growth rates or rated of return. It is calculated by \\(\\sqrt[n]{(1+r_1)*(1+r_2)...(1+r_n)}-1\\), where \\(\\sqrt[n]{}\\) is the \\(n_{th}\\) root, and \\(r_i\\) are the returns or growth rates.\n\n\n\nUseful R functions\nBase R has a collection of functions that calculate measures of central location.\n\nThe mean() function calculates the average of a vector of values.\nThe median() function returns the median of a vector of values.\nThe table() function provides us with a frequency distribution. We can then identify the mode(s) of the vector provided.\nThe summary() function returns a collection of descriptive statistics for a vector or data frame.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics III</span>"
    ]
  },
  {
    "objectID": "DescriptiveIII.html#exercises",
    "href": "DescriptiveIII.html#exercises",
    "title": "3  Descriptive Statistics III",
    "section": "3.2 Exercises",
    "text": "3.2 Exercises\nThe following exercises will help you practice the measures of central location. In particular, the exercises work on:\n\nCalculating the mean, median, and the mode.\nCalculating the weighted average.\nApplying the geometric mean for growth rates and returns.\n\nAnswers are provided below. Try not to peak until you have a formulated your own answer and double checked your work for any mistakes.\n\nExercise 1\nFor the following exercises, make your calculations by hand and verify results using R functions when possible.\n\nUse the following observations to calculate the mean, the median, and the mode.\n\n\n\n8\n10\n9\n12\n12\n\n\n\nUse following observations to calculate the mean, the median, and the mode.\n\n\n\n-4\n0\n-6\n1\n-3\n-4\n\n\n\nUse the following observations, calculate the mean, the median, and the mode.\n\n\n\n20\n15\n25\n20\n10\n15\n25\n20\n15\n\n\n\n\n\n\nExercise 2\nDownload the ISLR2 package. You will need the OJ data set to answer this question.\n\nFind the mean price for Country Hill (PriceCH) and Minute Maid (PriceMM).\nFind the mean price of Country Hill (PriceCH) in store 1 and store 2 (StoreID). Which store had the better price?\nFind the mean price paid by Country Hill (PriceCH) purchasers (Purchase) in store 1 (StoreID)? How about store 2? Which store had the better price?\n\n\n\nExercise 3\n\nOver the past year an investor bought TSLA. She made these purchases on three occasions at the prices shown in the table below. Calculate the average price per share.\n\n\n\n\nDate\nPrice Per Share\nNumber of Shares\n\n\n\n\nFebruary\n250.34\n80\n\n\nApril\n234.59\n120\n\n\nAug\n270.45\n50\n\n\n\n\nWhat would have been the average price per share if the investor would have bought equal amounts of shares each month?\n\n\n\nExercise 4\n\nConsider the following observations for the consumer price index (CPI). Calculate the inflation rate (Growth Rate of the CPI) for each period.\n\n\n\n1.0\n1.3\n1.6\n1.8\n2.1\n\n\n\nSuppose that you want to invest $1000 dollars in a stock that is predicted to yield the following returns in the next four years. Calculate both the arithmetic mean and the geometric mean. Use the geometric mean to estimate how much money you would have by the end of year 4.\n\n\n\nYear\nAnnual Return\n\n\n\n\n1\n17.3\n\n\n2\n19.6\n\n\n3\n6.8\n\n\n4\n8.2",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics III</span>"
    ]
  },
  {
    "objectID": "DescriptiveIII.html#answers",
    "href": "DescriptiveIII.html#answers",
    "title": "3  Descriptive Statistics III",
    "section": "3.3 Answers",
    "text": "3.3 Answers\n\nExercise 1\n\nTo find the mean we will use the following formula \\(( \\frac{1}{n} \\sum_{i=i}^{n} x_{i})\\). The summation of the values is \\(51\\) and the number of observations is \\(5\\). The mean is \\(51/5=10.2\\).\nThe median is found by locating the middle value when data is sorted in ascending order. The median in this example is \\(10\\).\nThe mode is the value with the highest frequency. In this example the mode is \\(12\\) since it is repeated twice and all other numbers appear only once.\n\nThe mean can be easily verified in R by using the mean() function:\n\nmean(c(8,10,9,12,12))\n\n[1] 10.2\n\n\nSimilarly, the median is easily verified by using the median() function:\n\nmedian(c(8,10,9,12,12))\n\n[1] 10\n\n\nWe can use the table() function to calculate frequencies and easily identify the mode.\n\ntable(c(8,10,9,12,12))\n\n\n 8  9 10 12 \n 1  1  1  2 \n\n\n\nThe mean is \\(-2.67\\), the median is \\(-3.5\\), the mode is \\(-4\\).\n\nThese mean is verified in R:\n\nmean(c(-4,0,-6,1,-3,-4))\n\n[1] -2.666667\n\n\nThe median in R:\n\nmedian(c(-4,0,-6,1,-3,-4))\n\n[1] -3.5\n\n\nFinally, the mode in R:\n\ntable(c(-4,0,-6,1,-3,-4))\n\n\n-6 -4 -3  0  1 \n 1  2  1  1  1 \n\n\n\nThe mean is \\(18.33\\), the median is \\(20\\), the data is bimodal with both \\(15\\) and \\(20\\) being modes.\n\nThese mean is verified in R:\n\nmean(c(20,15,25,20,10,15,25,20,15))\n\n[1] 18.33333\n\n\nThe median in R:\n\nmedian(c(20,15,25,20,10,15,25,20,15))\n\n[1] 20\n\n\nThe frequency distribution identifies the modes:\n\ntable(c(20,15,25,20,10,15,25,20,15))\n\n\n10 15 20 25 \n 1  3  3  2 \n\n\n\n\nExercise 2\n\nThe mean price for Country Hill is \\(1.87\\). The mean price for Minute Maid is \\(2.09\\).\n\nThe means can be easily found with the mean() function:\n\nlibrary(ISLR2)\nmean(OJ$PriceCH)\n\n[1] 1.867421\n\nmean(OJ$PriceMM)\n\n[1] 2.085411\n\n\n\nThe mean price at store 1 for Country Hill is \\(1.80\\) vs. \\(1.84\\) for store 2. The juice is cheaper at store 1.\n\nThe means for each store can be found by using indexing and a logical statement. The Country Hill mean price at store 1 is given by:\n\nmean(OJ$PriceCH[OJ$StoreID==1])\n\n[1] 1.803758\n\n\nThe Country Hill mean price at store 2 is given by:\n\nmean(OJ$PriceCH[OJ$StoreID==2])\n\n[1] 1.841216\n\n\n\nPurchasers of Country Hill at store 1 paid and average of \\(1.80\\) for Country Hill juice. At store 2 they paid \\(1.86\\). Once again the average price was lower at store 1.\n\nThe mean for Country Hill purchasers at store 1 is given by:\n\nmean(OJ$PriceCH[OJ$StoreID==1 & OJ$Purchase==\"CH\"])\n\n[1] 1.797176\n\n\nThe mean for Country Hill purchasers at store 2 is:\n\nmean(OJ$PriceCH[OJ$StoreID==2 & OJ$Purchase==\"CH\"])\n\n[1] 1.857383\n\n\n\n\nExercise 3\n\nThe average price of sale is found by using the weighted average formula. \\(\\frac{\\sum w_{i}x_{i}}{\\sum w_{i}}\\) The weights (\\(w_{i}\\)) are given by the number of shares bought and the values (\\(x_{i}\\)) are the prices. The weighted average is \\(246.802\\).\n\nIn R you can create two vectors. One holds the share price and the other one the number of shares bought.\n\nPricePerShare&lt;-c(250.34,234.59,270.45)\nNumberOfShares&lt;-c(80,120,50)\n\nNext, you can multiply the PricePerShare and NumberOfShares vectors to find the numerator and then use sum() function to find the denominator. The weighted average is:\n\n(WeightedAverage&lt;-\n  sum(PricePerShare*NumberOfShares)/sum(NumberOfShares))\n\n[1] 246.802\n\n\n\nThe average if equal shares were bought would be \\(251.7933\\).\n\nIn R you can use the mean() function on the PricePerShare vector.\n\n(Average&lt;-mean(PricePerShare))\n\n[1] 251.7933\n\n\n\n\nExercise 4\n\nThe inflation rate for each period is shown in the table below:\n\n\n\n\n30%\n23.08%\n12.5%\n16.67%\n\n\n\nIn R create an object to store the values of the CPI:\n\nCPI&lt;-c(1,1.3,1.6,1.8,2.1)\n\nNext use the diff() function to find the difference between the end value and start value. Divide the result by a vector of starting value and multiply times 100.\n\n(Inflation&lt;-100*diff(CPI)/CPI[1:4])\n\n[1] 30.00000 23.07692 12.50000 16.66667\n\n\n\nAt the end of 4 years it is predicted that you would have \\(1621.17\\) dollars. Each year you would have gained \\(12.84\\)% on average.\n\nIn R include the annual rates in a vector:\n\ngrowth&lt;-c(0.173,0.196,0.068,0.082)\n\nThe arithmetic mean is:\n\n100*mean(growth)\n\n[1] 12.975\n\n\nThe geometric mean is:\n\n(geom&lt;-((prod(1+growth))^(1/4)-1)*100)\n\n[1] 12.8384\n\n\nAt the end of the four years we would have:\n\n1000*(1+geom/100)^4\n\n[1] 1621.167",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics III</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Grolemund, Garret. 2014. “Hands-on Programming with r.” https://jjallaire.github.io/hopr/.\n\n\nWickham, Hadley. 2017. “R for Data Science.” https://r4ds.hadley.nz.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "DescriptiveII.html#plots-using-ggplot2",
    "href": "DescriptiveII.html#plots-using-ggplot2",
    "title": "2  Descriptive Stats II",
    "section": "2.3 Plots Using ggplot2",
    "text": "2.3 Plots Using ggplot2\nA bar plot illustrates the frequency distribution of qualitative data.\n\nIs an illustration for qualitative data.\nIncludes the classes in the horizontal axis and frequencies or relative frequencies in the vertical axis.\nHas gaps between each bar.\n\nA histogram illustrates the frequency distribution of quantitative data.\n\nIs an illustration for quantitative data.\nThere are no gaps between the bars.\nThe number, width and limits of each class must be determined.\n\nThe number of classes can be determined by the \\(2^k\\) rule: select \\(k\\) such that \\(2^k\\) is greater than the number of observations by the smallest amount.\nThe width of the class is approximately range/(# of Classes). The value should be rounded up.\nThe limits should be chosen so that each point belongs to only one class.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Stats II</span>"
    ]
  },
  {
    "objectID": "DescriptiveII.html#frequency-distribution",
    "href": "DescriptiveII.html#frequency-distribution",
    "title": "2  Descriptive Stats II",
    "section": "",
    "text": "The relative frequency is calculated by \\(f_{i}/n\\), where \\(f_{i}\\) is the frequency of class \\(i\\) and \\(n\\) is the total frequency.\nThe cumulative frequency shows the number of data items with values less than or equal to the upper class limit of each class.\nThe cumulative relative frequency is given by \\(cf_{i}/n\\), where \\(cf_{i}\\) is the cumulative frequency of class \\(i\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Stats II</span>"
    ]
  },
  {
    "objectID": "DescriptiveII.html#useful-r-functions",
    "href": "DescriptiveII.html#useful-r-functions",
    "title": "2  Descriptive Stats II",
    "section": "Useful R Functions",
    "text": "Useful R Functions\nThe table() command generates frequency distributions or contingency tables if two variables are used.\nThe prop.table() command generates relative frequency distributions from an object that contains a table.\nThe cut() function generates class limits and bins used in frequency distributions (and histograms) for quantitative data.\nBase R has the barplot() function for categorical variable, histogram() function for numerical data, and the plot() function for line charts or scatter plots. Below are some arguments that are helpful when plotting.\n\nmain: used to set the plot’s title. The title should be entered as a character.\ncol: used to set the color of the plot. Hex and RGB values are allowed as inputs. The color should be entered as a character.\nxlab and ylab: are used to set the labels for the \\(x\\) and \\(y\\) axis respectively. The labels should be entered as characters.\nlegend() is a function to customize the legend of a graph. This argument may be used with the plot(), barplot() or histogram() functions.\n\nx: used to set the location of the legend in the plotting area. Ex: “bottomleft”.\nlegend: a vector specifying the legend names to be included.\ncol: a vector specifying the color of each item in the legend.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Stats II</span>"
    ]
  },
  {
    "objectID": "DescriptiveII.html#frequency-distributions-numerical",
    "href": "DescriptiveII.html#frequency-distributions-numerical",
    "title": "2  Descriptive Stats II",
    "section": "2.2 Frequency Distributions (Numerical)",
    "text": "2.2 Frequency Distributions (Numerical)\n\nThe cumulative frequency shows the number of data items with values less than or equal to the upper class limit of each class.\nThe cumulative relative frequency is given by \\(cf_{i}/n\\), where \\(cf_{i}\\) is the cumulative frequency of class \\(i\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Stats II</span>"
    ]
  }
]