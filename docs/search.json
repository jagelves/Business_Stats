[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Business Statistics",
    "section": "",
    "text": "Introduction\n“Whatever you would make habitual, practice it; and if you would not make a thing habitual, do not practice it, but accustom yourself to something else.” Epictetus\nThis course companion is designed to help you build mastery in statistics and its applications using R. Through practice, you will develop the skills and confidence needed to apply statistical concepts effectively. Each chapter begins with a list of key concepts to guide your learning, and the problems are crafted to reinforce these ideas through hands-on experience. If you need additional support while learning R, I encourage you to explore Grolemund (2014). Take your time, enjoy the process, and make practice a habit!",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#why-r",
    "href": "index.html#why-r",
    "title": "Business Statistics",
    "section": "Why R?",
    "text": "Why R?\nWe will be using R to apply the lessons we learn in BUAD 231. R is a language and environment for statistical computing and graphics. There are several advantages to using the R software for statistical analysis and data science. Some of the main benefits include:\n\nR is a powerful and flexible programming language that allows users to manipulate and analyze data in many different ways.\nR has a large and active community of users, who have developed a wide range of packages and tools for data analysis and visualization.\nR is free and open-source, which makes it accessible to anyone who wants to use it.\nR is widely used in academia and industry, which means that there are many resources and tutorials available to help users learn how to use it.\nR is well-suited for working with large and complex datasets, and it can handle data from many different sources.\nR can be easily integrated with other tools and software, such as databases, visualization tools, and machine learning algorithms.\n\nOverall, R is a powerful and versatile tool for data analysis and data science, and it offers many benefits to users who want to work with data.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#installing-r.",
    "href": "index.html#installing-r.",
    "title": "Business Statistics",
    "section": "Installing R.",
    "text": "Installing R.\nTo install R, visit the R webpage at https://www.r-project.org/. Once in the website, click on the CRAN hyperlink.\n\n\n\n\n\n\n\n\n\nHere you can select the CRAN mirror. Scroll down until you see USA. You are free to choose any mirror you like, I recommend using the Duke University mirror.\n\n\n\n\n\n\n\n\n\nOnce you click on the hyperlink, you will be prompted to choose the download for your operating system. Depending on your operating system, choose either a Windows or Macintosh download.\n\n\n\n\n\n\n\n\n\nFollow all prompts and complete installation.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#installing-rstudio",
    "href": "index.html#installing-rstudio",
    "title": "Business Statistics",
    "section": "Installing RStudio",
    "text": "Installing RStudio\nVisit the Posit website at https://posit.co. Once on the website, hover to the top of the screen and select “Open Source” from the drop down menus.\n\n\n\n\n\n\n\n\n\nNext, choose “R Studio IDE”.\n\n\n\n\n\n\n\n\n\nScroll down until you see the products. You want to download “RStudio Desktop” and make sure it is the free version.\n\n\n\n\n\n\n\n\n\nFinally, select “Download RStudio” and follow the instructions for installation.\n\n\n\n\n\n\n\n\n\nIt is important to note that RStudio will not work if R is not installed. You can think of R as the engine and RStudio as the interface.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#posit-cloud",
    "href": "index.html#posit-cloud",
    "title": "Business Statistics",
    "section": "Posit Cloud",
    "text": "Posit Cloud\nIf you do not wish to install R, you can always use the cloud version. To do this, visit https://posit.cloud/. On the main page click on the “Get Started” button.\n\n\n\n\n\n\n\n\n\nChoose the “Cloud Free” option and log in using your Google credentials (if you have a Google account) or sign up if you want to create a new account.\n\n\n\n\nGrolemund, Garret. 2014. “Hands-on Programming with r.” https://jjallaire.github.io/hopr/.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "DescriptiveI.html",
    "href": "DescriptiveI.html",
    "title": "1  Descriptive Stats I",
    "section": "",
    "text": "1.1 Data and Types of Data\nUnderstanding the nature and classification of data is crucial for effective analysis and decision-making. Data are the building blocks of insights, providing a foundation for businesses, researchers, and policymakers to make informed choices. Whether capturing a snapshot of a specific moment, tracking changes over time, or organizing information in structured or unstructured formats, how data is collected and categorized significantly impacts how it is analyzed and interpreted. This overview highlights key types of data and their unique characteristics to help you better understand their application in various contexts.\nData are facts and figures collected, analyzed and summarized for presentation and interpretation. Data can be classified as:\nExample: Consider a retail store analyzing its sales performance. If the store collects data on the total revenue generated by each location on Black Friday, it is cross-sectional data. On the other hand, if the store tracks weekly sales for the past year to observe trends, it is time series data. Structured data, like sales figures stored in spreadsheets, allows for easy comparison and analysis. Meanwhile, customer feedback gathered from social media posts and video reviews represents unstructured data, requiring advanced tools to extract meaningful insights.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Descriptive Stats I</span>"
    ]
  },
  {
    "objectID": "DescriptiveI.html#data-and-types-of-data",
    "href": "DescriptiveI.html#data-and-types-of-data",
    "title": "1  Descriptive Stats I",
    "section": "",
    "text": "Cross Sectional Data refers to data collected at the same (or approximately the same) point in time. Ex: NFL standings in 1980 or Country GDP in 2015.\nTime Series Data refers to data collected over several time periods. Ex: U.S. inflation rate from 2000-2010 or Tesla deliveries from 2016-2022.\nStructured Data resides in a predefined row-column format (tidy). Ex: spreadsheet data.\nUnstructured Data do not conform to a pre-defined row-column format. Ex: Text, video, and other multimedia.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Descriptive Stats I</span>"
    ]
  },
  {
    "objectID": "DescriptiveI.html#data-sets",
    "href": "DescriptiveI.html#data-sets",
    "title": "1  Descriptive Stats I",
    "section": "1.2 Data Sets",
    "text": "1.2 Data Sets\nA data set contains all data collected for a particular study. Data sets are composed of:\n\nElements are the entities on which data are collected. Ex: Football teams, countries, and individuals.\nVariables are a set of characteristics collected for each element. Ex: Goals scored, GDP, weight.\nObservations are the set of measurements obtained for a particular element. Ex: Salah, 20 (goals), 15 (assists). US, 2.3 (inflation), 4.5% (federal interest rate).\n\n\n\n\nElements\nVariable 1\nVariable 2\n\n\n\n\nElement 1\n#\n#\n\n\nElement 2\n#\n#\n\n\nElement 3\n#\n#\n\n\n…\n…\n…\n\n\n\nExample: Consider the dataset on electric vehicles (EV’s) displayed below:\n\n\n\n\n\n\n\n\n\nIn this dataset, each row represents an electric vehicle model, making the elements the specific EV models rather than the manufacturers. The variables collected for each model include:\n\nMake: The manufacturer of the EV.\nModel: The specific name of the EV model.\nRange_km: Driving range in kilometers on a full charge.\nTopSpeed_kmh: Maximum speed in km/h.\nPrice_pounds: Price in pounds (£).\nCharge_kmh: Charging speed in kilometers per hour.\n\nAn example observation is “Tesla Model 3,” with the following data: Make: Tesla, Model: Model 3, Range_km: 415, TopSpeed_kmh: 201, Price_pounds: 39,990, Charge_kmh: 690.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Descriptive Stats I</span>"
    ]
  },
  {
    "objectID": "DescriptiveI.html#scales-of-measurement",
    "href": "DescriptiveI.html#scales-of-measurement",
    "title": "1  Descriptive Stats I",
    "section": "1.3 Scales of Measurement",
    "text": "1.3 Scales of Measurement\nUnderstanding scales of measurement is crucial for analyzing and interpreting data effectively in business. By distinguishing between categorical (e.g., marital status, satisfaction ratings) and numerical data (e.g., profits, prices), you’ll know what methods to use for analysis. Knowing whether data is nominal, ordinal, interval, or ratio ensures your analysis and conclusions are accurate and relevant.\nThe scales of measurements determine the amount and type of information contained in each variable. In general, variables can be classified as categorical or numerical.\n\nCategorical (qualitative) data includes labels or names to identify an attribute of each element. Categorical data can be nominal or ordinal.\n\nWith nominal data, the order of the categories is arbitrary. Ex: Marital Status, Race/Ethnicity, or NFL division.\nWith ordinal data, the order or rank of the categories is meaningful. Ex: Rating, Difficulty Level, or Spice Level.\n\nNumerical (quantitative) include numerical values that indicate how many (discrete) or how much (continuous). The data can be either interval or ratio.\n\nWith interval data, the distance between values is expressed in terms of a fixed unit of measure. The zero value is arbitrary and does not represent the absence of the characteristic. Ratios are not meaningful. Ex: Temperature or Dates.\nWith ratio data, the ratio between values is meaningful. The zero value is not arbitrary and represents the absence of the characteristic. Ex: Prices, Profits, Wins.\n\n\nExample: Let’s keep using the EV example. Consider the new data set below:\n\n\n\n\n\n\n\n\n\nThe variables can be classified as follows: Car (Categorical - Nominal), consists of names of cars, which are labels used to identify each row. The order of these names does not matter, making it nominal data. Brand (Categorical - Nominal) represents the manufacturer of the car (e.g., Ford, Audi). These are labels with no inherent order, making it nominal data. Range (Numerical - Ratio), refers to the car’s driving range in miles. It is numerical and ratio because it has a meaningful zero (a car with zero range cannot move), and ratios are meaningful (e.g., a car with 250 miles range has double the range of one with 125 miles). Rating (Categorical - Ordinal) represents a rank or score (e.g., 4, 3, 2). The order matters, as higher ratings indicate better performance. However, the intervals between ratings are not consistent, so it is ordinal data. Year (Numerical - Interval) represents a point in time. While numerical, it is interval data because the zero point is arbitrary (e.g., year 0 does not indicate the “absence” of time), and ratios are not meaningful (e.g., 2020 is not “twice as late” as 1010).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Descriptive Stats I</span>"
    ]
  },
  {
    "objectID": "DescriptiveI.html#useful-base-r-functions",
    "href": "DescriptiveI.html#useful-base-r-functions",
    "title": "1  Descriptive Stats I",
    "section": "1.4 Useful Base R Functions",
    "text": "1.4 Useful Base R Functions\nUnderstanding and using Base R functions is essential for efficiently managing and analyzing data. Functions like na.omit() help clean datasets by removing rows with missing values, ensuring your analyses are accurate and complete. nrow() and ncol() quickly provide insights into the size of your dataset, while is.na() allows you to identify and address missing data. The summary() function is a powerful way to generate descriptive statistics and assess the overall structure of your data at a glance. Additionally, coercion functions like as.integer(), as.factor(), and as.double() enable you to convert variables to appropriate data types, ensuring compatibility with different analysis methods.\n\nThe na.omit() function removes any observations that have a missing value (NA). The resulting data frame has only complete cases. Input: A data frame (tibble) or vector.\nThe nrow() and ncol() functions return the number of rows and columns respectively from a data frame. Input: A data frame (tibble).\nThe is.na() function returns a vector of True and False that specify if an entry is missing (NA) or not. Input: A data frame (tibble) or vector.\nThe summary() function returns a collection of descriptive statistics from a data frame (or vector). The function also returns whether there are any missing values (NA) in a variable. Input: A data frame (tibble) or vector.\nThe as.integer(), as.factor(), as.double(), are functions used to coerce your data into a different scale of measurement. Input: A vector or column of a data frame (tibble).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Descriptive Stats I</span>"
    ]
  },
  {
    "objectID": "DescriptiveI.html#useful-dplyr-functions",
    "href": "DescriptiveI.html#useful-dplyr-functions",
    "title": "1  Descriptive Stats I",
    "section": "1.5 Useful dplyr Functions",
    "text": "1.5 Useful dplyr Functions\nThe dplyr package has a collection of functions that are useful for data manipulation and transformation. If you are interested in this package you can refer to Wickham (2017). To install, run the following command in the console install.packages(\"dplyr\").\n\nThe arrange() function allows you to sort data frames in ascending order. Pair with the desc() function to sort the data in descending order.\nThe filter() function allows you to subset the rows of your data based on a condition.\nThe select() function allows you to select a subset of variables from your data frame.\nThe mutate() function allows you to create a new variable.\nThe group_by() function allows you to group your data frame by categories present in a given variable.\nThe summarise() function allows you to summarise your data, based on groupings generated by the goup_by() function.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Descriptive Stats I</span>"
    ]
  },
  {
    "objectID": "DescriptiveI.html#exercises",
    "href": "DescriptiveI.html#exercises",
    "title": "1  Descriptive Stats I",
    "section": "1.6 Exercises",
    "text": "1.6 Exercises\nThe following exercises will help you test your knowledge on the Scales of Measurement. They will also allow you to practice some basic data “wrangling” in R. In these exercises you will:\n\nIdentify numerical and categorical data.\nClassify data according to their scale of measurement.\nSort and filter data in R.\nHandle missing values (NA’s) in R.\n\nAnswers are provided below. Try not to peak until you have a formulated your own answer and double checked your work for any mistakes.\n\nExercise 1\nA bookstore has compiled data set on their current inventory. A portion of the data is shown below:\n\n\n\nTitle\nPrice\nYear Published\nRating\n\n\n\n\nFrankenstein\n5.49\n1818\n4.2\n\n\nDracula\n7.60\n1897\n4.0\n\n\n…\n…\n…\n…\n\n\nSleepy Hollow\n6.95\n1820\n3.8\n\n\n\n\nWhich of the above variables are categorical and which are numerical?\n\n\n\nAnswer\n\nThe “Title” variable represents the names of books. Therefore, this is a categorical variable. “Price” represents the cost of each book in a numeric format, making it a numerical variable. “Year Published” indicates the publication year of each book. It is numerical. If “Rating” represents a numerical score based on a continuous scale (e.g., average user ratings on a platform like Goodreads), it is numerical because arithmetic operations like averaging or comparing differences are meaningful. If “Rating” represents predefined categories (e.g., “Excellent,” “Good,” “Fair,” “Poor”) or is interpreted as ranks without meaningful differences between values, it would be categorical.\n\n\nWhat is the measurement scale of each of the above variable?\n\n\n\nAnswer\n\nThe measurement scale is nominal for Title since these are labels used to identify each book and do not have a numerical meaning or order. If Rating represents a score (e.g., 4.2, 4.0) given to each book, it is numerical and could be considered interval data because the scale represents a meaningful difference, but it may not have an absolute zero or meaningful ratios (e.g., a book rated 4.0 is not “twice as good” as one rated 2.0). Price is a measurable quantity with a meaningful zero (e.g., a book priced at $0 means it is free), making it ratio data. Year is interval data because the zero point is arbitrary (year 0 does not represent the absence of time) and differences between years are meaningful (e.g., 1897 - 1818 = 79 years).\n\n\n\nExercise 2\nA car company tracks the number of deliveries every quarter. A portion of the data is shown below:\n\n\n\nYear\nQuarter\nDeliveries\n\n\n\n\n2016\n1\n14800\n\n\n2016\n2\n14400\n\n\n…\n…\n…\n\n\n2022\n3\n343840\n\n\n\n\nWhat is the measurement scale of the Year variable? What are the strengths and weaknesses of this type of measurement scale?\n\n\n\nAnswer\n\nThe variable Year is measured on the interval scale because the observations can be ranked, categorized and measured when using this kind of scale. However, there is no true zero point so we cannot calculate meaningful ratios between years.\n\n\nWhat is the measurement scale for the Quarter variable? What is the weakness of this type of measurement scale?\n\n\n\nAnswer\n\nThe variable Quarter is measured on the ordinal scale, even though it contains numbers. It is the least sophisticated level of measurement because if we are presented with nominal data, all we can do is categorize or group the data.\n\n\nWhat is the measurement scale for the Deliveries variable? What are the strengths of this type of measurement scale?\n\n\n\nAnswer\n\nThe variable Deliveries is measured on the ratio scale. It is the strongest level of measurement because it allows us to categorize and rank the data as well as find meaningful differences between observations. Also, with a true zero point, we can interpret the ratios between observations.\n\n\n\nExercise 3\nUse the airquality data set included in R for this problem.\n\nSort the data by Temp in descending order. What is the day and month of the first observation on the sorted data?\n\n\n\nAnswer\n\nThe day and month of the first observation is August 28th.\nThe easiest way to sort in R is by using the dplyr package. Specifically, the arrange() function within the package. Let’s also use the desc() function to make sure that the data is sorted in descending order. We can use indexing to retrieve the first row of the sorted data set.\n\nlibrary(dplyr)\nSortedAQ&lt;-arrange(airquality,desc(Temp))\nSortedAQ[1,]\n\n  Ozone Solar.R Wind Temp Month Day\n1    76     203  9.7   97     8  28\n\n\n\n\nSort the data only by Temp in descending order. Of the \\(10\\) hottest days, how many of them were in July?\n\n\n\nAnswer\n\nWe can use the arrange() function one more time for this question. Then we can use indexing to retrieve the top \\(10\\) observations.\n\nSortedAQ2&lt;-arrange(airquality,desc(Temp))\nSortedAQ2[1:10,]\n\n   Ozone Solar.R Wind Temp Month Day\n1     76     203  9.7   97     8  28\n2     84     237  6.3   96     8  30\n3    118     225  2.3   94     8  29\n4     85     188  6.3   94     8  31\n5     NA     259 10.9   93     6  11\n6     73     183  2.8   93     9   3\n7     91     189  4.6   93     9   4\n8     NA     250  9.2   92     6  12\n9     97     267  6.3   92     7   8\n10    97     272  5.7   92     7   9\n\n\n\n\nHow many missing values are there in the data set? What rows have missing values for Solar.R?\n\n\n\nAnswer\n\nThere are a total of \\(44\\) missing values. Ozone has \\(37\\) and Solar.R has \\(7\\). Rows \\(5\\), \\(6\\), \\(11\\), \\(27\\), \\(96\\), \\(97\\), \\(98\\) are missing for Solar.R.\nWe can easily identify missing values with the summary() function.\n\nsummary(airquality)\n\n     Ozone           Solar.R           Wind             Temp      \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n NA's   :37       NA's   :7                                       \n     Month            Day      \n Min.   :5.000   Min.   : 1.0  \n 1st Qu.:6.000   1st Qu.: 8.0  \n Median :7.000   Median :16.0  \n Mean   :6.993   Mean   :15.8  \n 3rd Qu.:8.000   3rd Qu.:23.0  \n Max.   :9.000   Max.   :31.0  \n                               \n\n\nTo view the rows that have NA’s in them, we can use the is.na() function and indexing. Below we see that \\(7\\) values are missing for the Solar.R variable in the months \\(5\\) and \\(8\\) combined.\n\nairquality[is.na(airquality$Solar.R),]\n\n   Ozone Solar.R Wind Temp Month Day\n5     NA      NA 14.3   56     5   5\n6     28      NA 14.9   66     5   6\n11     7      NA  6.9   74     5  11\n27    NA      NA  8.0   57     5  27\n96    78      NA  6.9   86     8   4\n97    35      NA  7.4   85     8   5\n98    66      NA  4.6   87     8   6\n\n\n\n\nRemove all observations that have a missing values. Create a new object called CompleteAG.\n\n\n\nAnswer\n\nTo create the new object of complete observations we can use the na.omit() function.\n\nCompleteAQ&lt;-na.omit(airquality)\n\n\n\nWhen using CompleteAG, how many days was the temperature at least \\(60\\) degrees?\n\n\n\nAnswer\n\nThere were \\(107\\) days where the temperature was at least \\(60\\).\nUsing base R we have:\n\nnrow(CompleteAQ[CompleteAQ$Temp&gt;=60,])\n\n[1] 107\n\n\nWe can also use dplyr for this question. Specifically, using the filter() and nrow() functions we get:\n\nnrow(filter(CompleteAQ,Temp&gt;=60))\n\n[1] 107\n\n\n\n\nWhen using CompleteAG, how many days was the temperature within [\\(55\\),\\(75\\)] degrees and an Ozone below \\(20\\)?\n\n\n\nAnswer\n\nThere were \\(24\\) days where the temperature was between \\(55\\) and \\(75\\) and the ozone level was below \\(20\\).\nUsing base R we have:\n\nnrow(CompleteAQ[CompleteAQ$Temp&gt;55 & CompleteAQ$Temp&lt;75 & CompleteAQ$Ozone&lt;20,])\n\n[1] 24\n\n\nUsing the filter() function once more we get:\n\nnrow(filter(CompleteAQ,Temp&gt;55,Temp&lt;75,Ozone&lt;20))\n\n[1] 24\n\n\n\n\n\nExercise 4\nUse the Packers data set for this problem. You can find the data set at https://jagelves.github.io/Data/Packers.csv\n\nRemove the any observation that has a missing value with the na.omit() function. How many observations are left in the data set?\n\n\n\nAnswer\n\nThere are \\(84\\) observations in the complete cases data set.\nLet’s import the data to R by using the read.csv() function.\n\nPackers&lt;-read.csv(\"https://jagelves.github.io/Data/Packers.csv\")\n\nWe can remove any missing observation by using the na.omit() function. We can name this new object Packers2.\n\nPackers2&lt;-na.omit(Packers)\n\nTo find the number of observations we can use the dim() function. It returns the number of observations and variables.\n\ndim(Packers2)\n\n[1] 84  8\n\n\n\n\nDetermine the type of the Experience variable by using the typeof() function. What type is the variable?\n\n\n\nAnswer\n\nThe type is character.\nUse the typeof() function on the Experience variable.\n\ntypeof(Packers2$Experience)\n\n[1] \"character\"\n\n\n\n\nRemove observations that have an “R” and coerce the Experience variable to an integer using the as.integer() function. What is the total sum of years of experience?\n\n\n\nAnswer\n\nThe total sum of experience is \\(288\\).\nFirst, remove any observation with an R by using indexing and logicals.\n\nPackers2&lt;-Packers2[Packers2$Experience!=\"R\",]\n\nNow we can coerce the variable to an integer by using the as.integer() function.\n\nPackers2$Experience&lt;-as.integer(Packers2$Experience)\n\nLastly, calculate the sum using the sum() function.\n\nsum(Packers2$Experience)\n\n[1] 288\n\n\n\n\n\n\n\nWickham, Hadley. 2017. “R for Data Science.” https://r4ds.hadley.nz.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Descriptive Stats I</span>"
    ]
  },
  {
    "objectID": "DescriptiveII.html",
    "href": "DescriptiveII.html",
    "title": "2  Descriptive Stats II",
    "section": "",
    "text": "2.1 Frequency Distributions (Categorical)\nUnderstanding and visualizing data distributions is a fundamental step in data analysis. A frequency distribution organizes data into non-overlapping classes, allowing for insights into patterns and trends. Complementary to this, relative frequency, cumulative frequency, and cumulative relative frequency offer deeper perspectives on the proportions and accumulation of data within these classes. Visualization techniques play a crucial role in representing these distributions, with bar plots suited for qualitative data and histograms tailored for quantitative data. The R package ggplot2, has functions like geom_bar() and geom_hist() to plot distributions efficiently. By leveraging these methods, data can be transformed into clear and meaningful insights.\nA frequency distribution is perhaps the most valuable tool for summarizing categorical data. It illustrates with a table the number of items within distinct, non-overlapping categories. The relative frequency, which quantifies the proportion of items in each category relative to the total number of observations is often used as an alternative to showing raw frequencies. You can calculate it by taking the frequency of a particular class (\\(f_{i}\\)), and dividing it by the total frequency \\(n\\). Relative frequency helps contextualize the data by highlighting the significance of each category compared to the whole.\nExample: Consider data on students’ answers to the question, what is your favorite food? You can see the data below:\nSimply observing raw data can make identifying the most and least popular items challenging. A frequency distribution organizes this information into a clear table, showcasing the popularity of each item. The frequency distribution of the table is displayed below:\nFood\nFrequency\nRelative\n\n\n\n\nChicken\n5\n0.20\n\n\nPasta\n4\n0.16\n\n\nPizza\n6\n0.24\n\n\nSushi\n10\n0.40\nEach food item is tallied up, and the result is shown in the frequency column. Alternately, we can show the tally as a proportion of the total (i.e., 25). For example, five students liked chicken; out of the 25 students surveyed, this represents 0.2 or 20%. This calculation is shown for each food item in the relative frequency column.\nBelow, you can see the bar graph showing the frequency distribution of the food items data. Note that the visualization is constructed by showing each food item as a bar with a height equal to the frequency.\nIn sum, the bar plot illustrates the frequency distribution of categorical data. It includes the classes in the horizontal axis and frequencies or relative frequencies in the vertical axis and has gaps between each bar.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Stats II</span>"
    ]
  },
  {
    "objectID": "DescriptiveII.html#frequency-distributions-numerical",
    "href": "DescriptiveII.html#frequency-distributions-numerical",
    "title": "2  Descriptive Stats II",
    "section": "2.2 Frequency Distributions (Numerical)",
    "text": "2.2 Frequency Distributions (Numerical)\nWhen working with numerical data, building a frequency distributions requires additional steps compared to categorical data. The challenge lies in the absence of predefined categories or classes. To construct a frequency distribution for numerical data, it is essential to determine the number, width, and limits of the classes. Here are the steps to create a frequency distribution when data is numerical:\n1. Determine the Number of Classes: The number of classes can be estimated using the \\(2^k\\) rule, where \\(k\\) is the smallest integer such that \\(2^k\\) exceeds the total number of observations by the least amount. This ensures the chosen number of classes provides a reasonable level of granularity for summarizing the data.\nEx: If a data set has 50 observations, we would choose six classes since \\(2^6=64\\) is greater than \\(50\\) by the least amount.\n2. Calculate the Width of Each Class: The width of a class is determined using the formula: range/(# of Classes).\nEx: If the data set has 50 observations and the minimum value 20 and the maximum is 78, then the width of each class is \\(58/6 \\approx 9.7\\). Hence, we can round up and use a class width of 10.\n3. Establish Class Limits: The class limits define the range of values in each class. These limits should be chosen such that each data point belongs to only one class.\nEx: Consider a data set of 50 observations where each class has a width of 10. Set the class limits of the first class to [20,30). Note that the square bracket indicates that the point should be included in the class, whereas ) indicates that the point should no be included in the class. The six classes would be [20,30), [30,40), [40,50), [50,60), [60,70), and [70,80).\nExample: Let’s look at a snapshot of the Dow Jones Industrial 30 stock prices. Below you can see the data:\n\n\n\n\n\n\n\n\n\nLet’s follow the steps to build the frequency distribution.\n1. Determine the Number of Classes: Here we choose five classes since \\(2^5=32\\) is greater than \\(30\\) by the least amount.\n2. Calculate the Width of Each Class: The smallest values in the data set is \\(23\\) and the maximum is \\(501\\). This gives us a range of \\(478\\). Now we can just take the range and divide by five to get \\(95.6\\). To make things simple we can round to \\(100\\) and use a class width of \\(100\\).\n3. Establish Class Limits: Since we have rounded up we can be flexible with our class limits. The following class limits are suggested [20,120), [120,220), [220,320), [320,420), and [420,520). Note that each class has a width of \\(100\\), and that each data point belongs to one single class.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Stats II</span>"
    ]
  },
  {
    "objectID": "DescriptiveII.html#plots-using-ggplot2",
    "href": "DescriptiveII.html#plots-using-ggplot2",
    "title": "2  Descriptive Stats II",
    "section": "2.3 Plots Using ggplot2",
    "text": "2.3 Plots Using ggplot2\nA bar plot illustrates the frequency distribution of qualitative data.\n\nIs an illustration for qualitative data.\nIncludes the classes in the horizontal axis and frequencies or relative frequencies in the vertical axis.\nHas gaps between each bar.\n\nA histogram illustrates the frequency distribution of quantitative data.\n\nIs an illustration for quantitative data.\nThere are no gaps between the bars.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Stats II</span>"
    ]
  },
  {
    "objectID": "DescriptiveII.html#useful-r-functions",
    "href": "DescriptiveII.html#useful-r-functions",
    "title": "2  Descriptive Stats II",
    "section": "2.6 Useful R Functions",
    "text": "2.6 Useful R Functions\nThe table() command generates frequency distributions or contingency tables if two variables are used.\nThe prop.table() command generates relative frequency distributions from an object that contains a table.\nThe cut() function generates class limits and bins used in frequency distributions (and histograms) for quantitative data.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Stats II</span>"
    ]
  },
  {
    "objectID": "DescriptiveII.html#exercises",
    "href": "DescriptiveII.html#exercises",
    "title": "2  Descriptive Stats II",
    "section": "2.7 Exercises",
    "text": "2.7 Exercises\nThe following exercises will help you practice summarizing data with tables and simple graphs. In particular, the exercises work on:\n\nDeveloping frequency distributions for both categorical and numerical data.\nConstructing bar charts, histograms, and line charts.\nCreating contingency tables.\n\nAnswers are provided below. Try not to peak until you have a formulated your own answer and double checked your work for any mistakes.\n\nExercise 1\nInstall the ISLR2 package in R. You will need the BrainCancer data set to answer this question.\n\nConstruct a frequency and relative frequency table of the Diagnosis variable. What was the most common diagnosis? What percentage of the sample had this diagnosis?\n\n\n\nAnswer\n\nThe most common diagnosis is Meningioma, a slow-growing tumor that forms from the membranous layers surrounding the brain and spinal cord. The diagnosis represents about \\(48.28\\)% of the sample.\nStart by loading the ISLR2 package. To construct the frequency distribution table, use the table() function.\n\nlibrary(ISLR2)\ntable(BrainCancer$diagnosis)\n\n\nMeningioma  LG glioma  HG glioma      Other \n        42          9         22         14 \n\n\nThe relative frequency distribution can be easily retrieved by saving the frequency table in an object and then using the prop.table() function.\n\nfreq&lt;-table(BrainCancer$diagnosis)\nprop.table(freq)\n\n\nMeningioma  LG glioma  HG glioma      Other \n 0.4827586  0.1034483  0.2528736  0.1609195 \n\n\n\n\nConstruct a bar chart. Summarize the findings.\n\n\n\nAnswer\n\nThe majority of diagnosis are Meningioma. Low grade glioma is the least common of diagnosis. High grade glioma and other diagnosis have about the same frequency.\nTo construct the bar chart use the geom_bar() function from tidyverse.\n\nlibrary(tidyverse)\nlibrary(ggthemes)\nggplot(data=BrainCancer) + \n  geom_bar(aes(diagnosis), alpha=0.5, col=\"black\") + \n  theme_clean()\n\n\n\n\n\n\n\n\n\n\nConstruct a contingency table that shows the Diagnosis along with the Status. Which diagnosis had the highest number of non-survivals (0)? What was the survival rate of this diagnosis?\n\n\n\nAnswer\n\n\\(33\\) people did not survive Meningioma. The survival rate of Meningioma is only \\(21.43\\)%.\nUse the table() function one more time to create the contingency table for the two variables.\n\n(freq2&lt;-table(BrainCancer$status,BrainCancer$diagnosis))\n\n   \n    Meningioma LG glioma HG glioma Other\n  0         33         5         5     9\n  1          9         4        17     5\n\n\nTo get the survival rates, we can use the prop.table() function once again.\n\nprop.table(freq2,margin = 2)\n\n   \n    Meningioma LG glioma HG glioma     Other\n  0  0.7857143 0.5555556 0.2272727 0.6428571\n  1  0.2142857 0.4444444 0.7727273 0.3571429\n\n\n\n\n\nExercise 2\nYou will need the airquality data set (in base R) to answer this question.\n\nConstruct a frequency distribution for Temp. Use five classes with widths of \\(50&lt;x\\le60\\); \\(60&lt;x\\le70\\); etc. Which interval had the highest frequency? How many times was the temperature between \\(50\\) and \\(60\\) degrees?\n\n\n\nAnswer\n\nThe highest frequency is in the \\(80 &lt; x ≤ 90\\) bin. \\(8\\) temperatures were between \\(50 &lt; x ≤ 60\\) degrees.\nCreate a vector containing the intervals desired by using the seq() function.\n\nintervals &lt;- seq(50, 100, by=10)\n\nNext use the cut() function to create the cuts for the histogram.\n\nintervals.cut &lt;- cut(airquality$Temp, intervals, left=FALSE, right=TRUE)\n\nThe frequency distribution can be obtained by using the table() function on the interval.cut object created above.\n\ntable(intervals.cut)\n\nintervals.cut\n (50,60]  (60,70]  (70,80]  (80,90] (90,100] \n       8       25       52       54       14 \n\n\n\n\nConstruct a relative frequency, cumulative frequency and the relative cumulative frequency distributions. What proportion of the time was Temp between \\(50\\) and \\(60\\) degrees? How many times was the Temp \\(70\\) degrees or less? What proportion of the time was Temp more than \\(70\\) degrees?\n\n\n\nAnswer\n\nThe temperature was \\(5.22\\)% of the time between \\(50\\) and \\(60\\); The temperature was \\(70\\) or less \\(33\\) times; The temperature was above \\(70\\), \\(78.43\\)% of the time.\nTo get the relative frequency table, start by saving the proportion table into an object.Then you can use the prop.table() function.\n\nfreq&lt;-table(intervals.cut) \nprop.table(freq)\n\nintervals.cut\n   (50,60]    (60,70]    (70,80]    (80,90]   (90,100] \n0.05228758 0.16339869 0.33986928 0.35294118 0.09150327 \n\n\nFor the cumulative distribution you can use the cumsum() function on the frequency distribution.\n\ncumulfreq&lt;-cumsum(freq)\ncumulfreq\n\n (50,60]  (60,70]  (70,80]  (80,90] (90,100] \n       8       33       85      139      153 \n\n\nLastly, for the relative cumulative distribution table, you can use the cumsum() function on the relative frequency table.\n\ncumsum(prop.table(freq))\n\n   (50,60]    (60,70]    (70,80]    (80,90]   (90,100] \n0.05228758 0.21568627 0.55555556 0.90849673 1.00000000 \n\n\n\n\nConstruct the histogram. Is the distribution symmetric? If not, is it skewed to the left or right?\n\n\n\nAnswer\n\nThe distribution is not perfectly symmetric. It is skewed slightly to the left (see histogram.)\nUse the geom_histogram() function to create the histogram.\n\nggplot() + \n  geom_histogram(aes(airquality$Temp), col=\"black\", \n                 bg=\"darkgreen\",alpha=0.2,\n                 bins=5,\n                 binwidth = 10,\n                 boundary=0) + theme_clean() +\n  labs(x=\"Temperature\", y=\"Frequency\")\n\n\n\n\n\n\n\n\n\n\n\nExercise 3\nYou will need the Portfolio data set from the ISLR2 package to answer this question.\n\nConstruct a line chart that shows the returns over time for each portfolio (X and Y) by using two lines each with a unique color. Assume the data is for the period \\(1901\\) to \\(2000\\). Include also a legend that matches colors to portfolios.\n\n\n\nAnswer\n\nFrom \\(1901\\) through \\(2000\\), both portfolios have behaved very similarly. Returns are between \\(-3\\)% and \\(3\\)%, there is no trend, and positive (negative) returns for X seem to match with positive (negative) returns of Y.\nYou can use the geom_line() function to create a line for each portfolio.\n\nggplot() +\n  geom_line(aes(y=Portfolio$Y,x=seq(1901,2000)), col=\"blue\") +\n  geom_line(aes(y=Portfolio$X,x=seq(1901,2000)), col=\"grey\") +\n  theme_clean() +\n  labs(x=\"Year\", y=\"Return\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Stats II</span>"
    ]
  },
  {
    "objectID": "DescriptiveII.html#answers",
    "href": "DescriptiveII.html#answers",
    "title": "2  Descriptive Stats II",
    "section": "2.8 Answers",
    "text": "2.8 Answers\n\nExercise 1\n\nThe most common diagnosis is Meningioma, a slow-growing tumor that forms from the membranous layers surrounding the brain and spinal cord. The diagnosis represents about \\(48.28\\)% of the sample.\n\nStart by loading the ISLR2 package. To construct the frequency distribution table, use the table() function.\n\nlibrary(ISLR2)\ntable(BrainCancer$diagnosis)\n\n\nMeningioma  LG glioma  HG glioma      Other \n        42          9         22         14 \n\n\nThe relative frequency distribution can be easily retrieved by saving the frequency table in an object and then using the prop.table() function.\n\nfreq&lt;-table(BrainCancer$diagnosis)\nprop.table(freq)\n\n\nMeningioma  LG glioma  HG glioma      Other \n 0.4827586  0.1034483  0.2528736  0.1609195 \n\n\n\nThe majority of diagnosis are Meningioma. Low grade glioma is the least common of diagnosis. High grade glioma and other diagnosis have about the same frequency.\n\nTo construct the bar chart use the barplot() function in R.\n\nbarplot(freq, col = \"#F5F5F5\", ylim=c(0,50))\n\n\n\n\n\n\n\n\n\n\\(33\\) people did not survive Meningioma. The survival rate of Meningioma is only \\(21.43\\)%.\n\nUse the table() function one more time to create the contingency table for the two variables.\n\n(freq2&lt;-table(BrainCancer$status,BrainCancer$diagnosis))\n\n   \n    Meningioma LG glioma HG glioma Other\n  0         33         5         5     9\n  1          9         4        17     5\n\n\nTo get the survival rates, we can use the prop.table() function once again.\n\nprop.table(freq2,margin = 2)\n\n   \n    Meningioma LG glioma HG glioma     Other\n  0  0.7857143 0.5555556 0.2272727 0.6428571\n  1  0.2142857 0.4444444 0.7727273 0.3571429\n\n\n\nMeningioma and not surviving is the most common with \\(33\\) occurrences. High grade glioma and surviving is the the second most common.\n\nUse the barplot() function one more time to construct the stacked column chart.\n\nbarplot(table(BrainCancer$status,BrainCancer$diagnosis),\n        legend.text = c(\"Not Survived\",\"Survived\"), ylim=c(0,50))\n\n\n\n\n\n\n\n\n\n\nExercise 2\n\nThe highest frequency is in the \\(80 &lt; x ≤ 90\\) bin. \\(8\\) temperatures were between \\(50 &lt; x ≤ 60\\) degrees.\n\nCreate a vector containing the intervals desired by using the seq() function.\n\nintervals &lt;- seq(50, 100, by=10)\n\nNext use the cut() function to create the cuts for the histogram.\n\nintervals.cut &lt;- cut(airquality$Temp, intervals, left=FALSE, right=TRUE)\n\nThe frequency distribution can be obtained by using the table() function on the interval.cut object created above.\n\ntable(intervals.cut)\n\nintervals.cut\n (50,60]  (60,70]  (70,80]  (80,90] (90,100] \n       8       25       52       54       14 \n\n\n\nThe temperature was \\(5.22\\)% of the time between \\(50\\) and \\(60\\); The temperature was \\(70\\) or less \\(33\\) times; The temperature was above \\(70\\), \\(78.43\\)% of the time.\n\nTo get the relative frequency table, start by saving the proportion table into an object.Then you can use the prop.table() function.\n\nfreq&lt;-table(intervals.cut) \nprop.table(freq)\n\nintervals.cut\n   (50,60]    (60,70]    (70,80]    (80,90]   (90,100] \n0.05228758 0.16339869 0.33986928 0.35294118 0.09150327 \n\n\nFor the cumulative distribution you can use the cumsum() function on the frequency distribution.\n\ncumulfreq&lt;-cumsum(freq)\ncumulfreq\n\n (50,60]  (60,70]  (70,80]  (80,90] (90,100] \n       8       33       85      139      153 \n\n\nLastly, for the relative cumulative distribution table, you can use the cumsum() function on the relative frequency table.\n\ncumsum(prop.table(freq))\n\n   (50,60]    (60,70]    (70,80]    (80,90]   (90,100] \n0.05228758 0.21568627 0.55555556 0.90849673 1.00000000 \n\n\n\nThe distribution is not perfectly symmetric. It is skewed slightly to the left (see histogram.)\n\nUse the hist() function to create the histogram.\n\nhist(airquality$Temp, breaks=intervals, \n     right=TRUE,col=\"#F5F5F5\", main=\"Temperature in NY\", xlab=\"\")\n\n\n\n\n\n\n\n\n\n\nExercise 3\n\nFrom \\(1901\\) through \\(2000\\), both portfolios have behaved very similarly. Returns are between \\(-3\\)% and \\(3\\)%, there is no trend, and positive (negative) returns for X seem to match with positive (negative) returns of Y.\n\nYou can use the plot() function to create a plot of Portfolio Y. The line for Portfolio X can be added with the lines() function.\n\nplot(Portfolio$Y, \n     x=seq(1901,2000), type=\"l\", \n     col=\"black\", xlab=\"\", ylab=\"% Return\", ylim=c(-3,3), \n     xlim=c(1901,2000), lwd=2, axes = F)\naxis(side=1, labels=TRUE, font=1,las=1)\naxis(side=2, labels=TRUE, font=1,las=1)\nlines(Portfolio$X, x=seq(1901,2000), type=\"l\", \n      col=\"darkgrey\", lwd=2)\nlegend(x = \"bottomleft\",          \n       legend = c(\"Port Y\", \"Port X\"),  \n       lty = c(1, 1),           \n       col = c(\"black\", \"darkgrey\"),         \n       lwd = 2,\n       bty=\"n\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Stats II</span>"
    ]
  },
  {
    "objectID": "DescriptiveIII.html",
    "href": "DescriptiveIII.html",
    "title": "3  Descriptive Statistics III",
    "section": "",
    "text": "3.1 The Mean\nUnderstanding where the “center” of a dataset lies is popular step used to interpret and summarize data effectively. Measures of central location provide different ways to identify the “typical” value in a dataset, each with its unique strengths and limitations.\nThe mean is the average value for a numerical variable. It is a widely understood and straightforward measure to calculate. It incorporates all data points, providing a comprehensive representation of the data set. However, its reliance on every value also makes it sensitive to outliers or skewed distributions, which may cause it to not accurately reflect the true center of the data.\nThe sample statistic is estimated by \\(\\bar{x}=\\sum x_{i}/n\\), where \\(x_i\\) is observation \\(i\\), and \\(n\\) is the number of observations. The population parameter is defined as \\(\\mu=\\sum x_{i}/N\\).\nEx: Consider the following numbers \\(x={1,4,2,1}\\). The mean would be equal to \\(\\bar{x}=\\frac{1+4+2+1}{4}\\) or \\(\\bar{x}=2\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics III</span>"
    ]
  },
  {
    "objectID": "DescriptiveIII.html#concepts",
    "href": "DescriptiveIII.html#concepts",
    "title": "3  Descriptive Statistics III",
    "section": "",
    "text": "Measures of Central Location\nMeasures of Central Location determine where the center of a distribution lies.\n\nThe mean is the average value for a numerical variable. The sample statistic is estimated by \\(\\bar{x}=\\sum x_{i}/n\\), where \\(x_i\\) is observation \\(i\\), and \\(n\\) is the number of observations. The population parameter is defined as \\(\\mu=\\sum x_{i}/N\\).\nThe median is the value in the middle when data is organized in ascending order. When \\(n\\) is even, the median is the average between the two middle values.\nThe mode is the value with highest frequency from a set of observations.\nThe weighted mean uses weights to determine the importance of each data point of a variable. It is calculated by \\(\\frac{\\sum w_{i}x_{i}}{\\sum w_{i}}\\), where \\(w_{i}\\) are the weights associated to the values \\(x_{i}\\).\nThe geometric mean is a multiplicative average that is less sensitive to outliers. It is used to average growth rates or rated of return. It is calculated by \\(\\sqrt[n]{(1+r_1)*(1+r_2)...(1+r_n)}-1\\), where \\(\\sqrt[n]{}\\) is the \\(n_{th}\\) root, and \\(r_i\\) are the returns or growth rates.\n\n\n\nUseful R functions\nBase R has a collection of functions that calculate measures of central location.\n\nThe mean() function calculates the average of a vector of values.\nThe median() function returns the median of a vector of values.\nThe table() function provides us with a frequency distribution. We can then identify the mode(s) of the vector provided.\nThe summary() function returns a collection of descriptive statistics for a vector or data frame.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics III</span>"
    ]
  },
  {
    "objectID": "DescriptiveIII.html#exercises",
    "href": "DescriptiveIII.html#exercises",
    "title": "3  Descriptive Statistics III",
    "section": "3.7 Exercises",
    "text": "3.7 Exercises\nThe following exercises will help you practice the measures of central location. In particular, the exercises work on:\n\nCalculating the mean, median, and the mode.\nCalculating the weighted average.\nApplying the geometric mean for growth rates and returns.\n\nAnswers are provided below. Try not to peak until you have a formulated your own answer and double checked your work for any mistakes.\n\nExercise 1\nFor the following exercises, make your calculations by hand and verify results using R functions when possible.\n\nUse the following observations to calculate the mean, the median, and the mode.\n\n\n\n8\n10\n9\n12\n12\n\n\n\n\n\n\nAnswer\n\nTo find the mean we will use the following formula \\(( \\frac{1}{n} \\sum_{i=i}^{n} x_{i})\\). The summation of the values is \\(51\\) and the number of observations is \\(5\\). The mean is \\(51/5=10.2\\).\nThe median is found by locating the middle value when data is sorted in ascending order. The median in this example is \\(10\\).\nThe mode is the value with the highest frequency. In this example the mode is \\(12\\) since it is repeated twice and all other numbers appear only once.\nThe mean can be easily verified in R by using the mean() function:\n\nmean(c(8,10,9,12,12))\n\n[1] 10.2\n\n\nSimilarly, the median is easily verified by using the median() function:\n\nmedian(c(8,10,9,12,12))\n\n[1] 10\n\n\nWe can use the table() function to calculate frequencies and easily identify the mode.\n\ntable(c(8,10,9,12,12))\n\n\n 8  9 10 12 \n 1  1  1  2 \n\n\n\n\nUse following observations to calculate the mean, the median, and the mode.\n\n\n\n-4\n0\n-6\n1\n-3\n-4\n\n\n\n\n\n\nAnswer\n\nThe mean is \\(-2.67\\), the median is \\(-3.5\\), the mode is \\(-4\\).\nThe mean is verified in R:\n\nmean(c(-4,0,-6,1,-3,-4))\n\n[1] -2.666667\n\n\nThe median in R:\n\nmedian(c(-4,0,-6,1,-3,-4))\n\n[1] -3.5\n\n\nFinally, the mode in R:\n\ntable(c(-4,0,-6,1,-3,-4))\n\n\n-6 -4 -3  0  1 \n 1  2  1  1  1 \n\n\n\n\nUse the following observations, calculate the mean, the median, and the mode.\n\n\n\n20\n15\n25\n20\n10\n15\n25\n20\n15\n\n\n\n\n\n\nAnswer\n\nThe mean is \\(18.33\\), the median is \\(20\\), the data is bimodal with both \\(15\\) and \\(20\\) being modes.\nThe mean is verified in R:\n\nmean(c(20,15,25,20,10,15,25,20,15))\n\n[1] 18.33333\n\n\nThe median in R:\n\nmedian(c(20,15,25,20,10,15,25,20,15))\n\n[1] 20\n\n\nThe frequency distribution identifies the modes:\n\ntable(c(20,15,25,20,10,15,25,20,15))\n\n\n10 15 20 25 \n 1  3  3  2 \n\n\n\n\n\nExercise 2\nDownload the ISLR2 package. You will need the OJ data set to answer this question.\n\nFind the mean price for Country Hill (PriceCH) and Minute Maid (PriceMM).\n\n\n\nAnswer\n\nThe mean price for Country Hill is \\(1.87\\). The mean price for Minute Maid is \\(2.09\\).\nThe means can be easily found with the mean() function:\n\nlibrary(ISLR2)\nOJ=OJ\nmean(OJ$PriceCH)\n\n[1] 1.867421\n\nmean(OJ$PriceMM)\n\n[1] 2.085411\n\n\n\n\nFind the mean price of Country Hill (PriceCH) at each store (StoreID). Which store provides the better price?\n\n\n\nAnswer\n\nThe mean price at store 1 for Country Hill is \\(1.80\\). The juice is cheaper at store 1.\nThe means for each store can be found by using group_by() and summarise().The mean price at each store is:\n\nOJ %&gt;% group_by(StoreID) %&gt;% summarise(MeanCH=mean(PriceCH))\n\n# A tibble: 5 × 2\n  StoreID MeanCH\n    &lt;dbl&gt;  &lt;dbl&gt;\n1       1   1.80\n2       2   1.84\n3       3   1.93\n4       4   1.95\n5       7   1.84\n\n\n\n\nFind the median price paid by Country Hill (PriceCH) purchasers (Purchase) in all stores? Which store had the better median price?\n\n\n\nAnswer\n\nPurchasers of Country Hill at store 1 paid a median price of \\(1.76\\) for Country Hill juice. This once again was the lowest price.\nThe median price for Country Hill purchasers at each store is given by:\n\nOJ %&gt;% filter(Purchase==\"CH\") %&gt;% group_by(StoreID) %&gt;% summarise(MedianCH=median(PriceCH))\n\n# A tibble: 5 × 2\n  StoreID MedianCH\n    &lt;dbl&gt;    &lt;dbl&gt;\n1       1     1.76\n2       2     1.86\n3       3     1.99\n4       4     1.99\n5       7     1.86\n\n\n\n\n\nExercise 3\n\nOver the past year an investor bought TSLA. She made these purchases on three occasions at the prices shown in the table below. Calculate the average price per share.\n\n\n\n\nDate\nPrice Per Share\nNumber of Shares\n\n\n\n\nFebruary\n250.34\n80\n\n\nApril\n234.59\n120\n\n\nAug\n270.45\n50\n\n\n\n\n\nAnswer\n\nThe average price of sale is found by using the weighted average formula. \\(\\frac{\\sum w_{i}x_{i}}{\\sum w_{i}}\\) The weights (\\(w_{i}\\)) are given by the number of shares bought and the values (\\(x_{i}\\)) are the prices. The weighted average is \\(246.802\\).\nIn R you can create two vectors. One holds the share price and the other one the number of shares bought.\n\nPricePerShare&lt;-c(250.34,234.59,270.45)\nNumberOfShares&lt;-c(80,120,50)\n\nNext, can use the weighted.mean() function in R, with PricePerShare as the value and NumberOfShares as the weights. The weighted average is:\n\n(WeightedAverage&lt;-weighted.mean(PricePerShare,NumberOfShares))\n\n[1] 246.802\n\n\n\n\nWhat would have been the average price per share if the investor would have bought equal amounts of shares each month?\n\n\n\nAnswer\n\nThe average if equal shares were bought would be \\(251.7933\\).\nIn R you can use the mean() function on the PricePerShare vector.\n\n(Average&lt;-mean(PricePerShare))\n\n[1] 251.7933\n\n\n\n\n\nExercise 4\n\nConsider the following observations for the consumer price index (CPI). Calculate the inflation rate (Growth Rate of the CPI) for each period.\n\n\n\n1.0\n1.3\n1.6\n1.8\n2.1\n\n\n\n\n\n\nAnswer\n\nThe inflation rate is the percentage change in the CPI. The inflation rate for each period is shown in the table below:\n\n\n\n30%\n23.08%\n12.5%\n16.67%\n\n\n\nIn R create an object to store the values of the CPI:\n\nCPI&lt;-c(1,1.3,1.6,1.8,2.1)\n\nNext use the diff() function to find the difference between the end value and start value. Divide the result by a vector of starting value and multiply times 100.\n\n(Inflation&lt;-100*diff(CPI)/CPI[1:4])\n\n[1] 30.00000 23.07692 12.50000 16.66667\n\n\n\n\nSuppose that you want to invest $1000 dollars in a stock that is predicted to yield the following returns in the next four years. Calculate both the arithmetic mean and the geometric mean. Use the geometric mean to estimate how much money you would have by the end of year 4.\n\n\n\nYear\nAnnual Return\n\n\n\n\n1\n17.3\n\n\n2\n19.6\n\n\n3\n6.8\n\n\n4\n8.2\n\n\n\n\n\n\nAnswer\n\nAt the end of 4 years it is predicted that you would have \\(1621.17\\) dollars. Each year you would have gained \\(12.84\\)% on average.\nIn R include the annual rates in a vector:\n\ngrowth&lt;-c(0.173,0.196,0.068,0.082)\n\nThe arithmetic mean is:\n\n100*mean(growth)\n\n[1] 12.975\n\n\nThe geometric mean is:\n\n(geom&lt;-((prod(1+growth))^(1/length(growth))-1)*100)\n\n[1] 12.8384\n\n\nAt the end of the four years we would have:\n\n1000*(1+geom/100)^4\n\n[1] 1621.167",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics III</span>"
    ]
  },
  {
    "objectID": "DescriptiveIII.html#answers",
    "href": "DescriptiveIII.html#answers",
    "title": "3  Descriptive Statistics III",
    "section": "3.8 Answers",
    "text": "3.8 Answers\n\nExercise 1\n\nTo find the mean we will use the following formula \\(( \\frac{1}{n} \\sum_{i=i}^{n} x_{i})\\). The summation of the values is \\(51\\) and the number of observations is \\(5\\). The mean is \\(51/5=10.2\\).\nThe median is found by locating the middle value when data is sorted in ascending order. The median in this example is \\(10\\).\nThe mode is the value with the highest frequency. In this example the mode is \\(12\\) since it is repeated twice and all other numbers appear only once.\n\nThe mean can be easily verified in R by using the mean() function:\n\nmean(c(8,10,9,12,12))\n\n[1] 10.2\n\n\nSimilarly, the median is easily verified by using the median() function:\n\nmedian(c(8,10,9,12,12))\n\n[1] 10\n\n\nWe can use the table() function to calculate frequencies and easily identify the mode.\n\ntable(c(8,10,9,12,12))\n\n\n 8  9 10 12 \n 1  1  1  2 \n\n\n\nThe mean is \\(-2.67\\), the median is \\(-3.5\\), the mode is \\(-4\\).\n\nThese mean is verified in R:\n\nmean(c(-4,0,-6,1,-3,-4))\n\n[1] -2.666667\n\n\nThe median in R:\n\nmedian(c(-4,0,-6,1,-3,-4))\n\n[1] -3.5\n\n\nFinally, the mode in R:\n\ntable(c(-4,0,-6,1,-3,-4))\n\n\n-6 -4 -3  0  1 \n 1  2  1  1  1 \n\n\n\nThe mean is \\(18.33\\), the median is \\(20\\), the data is bimodal with both \\(15\\) and \\(20\\) being modes.\n\nThese mean is verified in R:\n\nmean(c(20,15,25,20,10,15,25,20,15))\n\n[1] 18.33333\n\n\nThe median in R:\n\nmedian(c(20,15,25,20,10,15,25,20,15))\n\n[1] 20\n\n\nThe frequency distribution identifies the modes:\n\ntable(c(20,15,25,20,10,15,25,20,15))\n\n\n10 15 20 25 \n 1  3  3  2 \n\n\n\n\nExercise 2\n\nThe mean price for Country Hill is \\(1.87\\). The mean price for Minute Maid is \\(2.09\\).\n\nThe means can be easily found with the mean() function:\n\nlibrary(ISLR2)\nmean(OJ$PriceCH)\n\n[1] 1.867421\n\nmean(OJ$PriceMM)\n\n[1] 2.085411\n\n\n\nThe mean price at store 1 for Country Hill is \\(1.80\\) vs. \\(1.84\\) for store 2. The juice is cheaper at store 1.\n\nThe means for each store can be found by using indexing and a logical statement. The Country Hill mean price at store 1 is given by:\n\nmean(OJ$PriceCH[OJ$StoreID==1])\n\n[1] 1.803758\n\n\nThe Country Hill mean price at store 2 is given by:\n\nmean(OJ$PriceCH[OJ$StoreID==2])\n\n[1] 1.841216\n\n\n\nPurchasers of Country Hill at store 1 paid and average of \\(1.80\\) for Country Hill juice. At store 2 they paid \\(1.86\\). Once again the average price was lower at store 1.\n\nThe mean for Country Hill purchasers at store 1 is given by:\n\nmean(OJ$PriceCH[OJ$StoreID==1 & OJ$Purchase==\"CH\"])\n\n[1] 1.797176\n\n\nThe mean for Country Hill purchasers at store 2 is:\n\nmean(OJ$PriceCH[OJ$StoreID==2 & OJ$Purchase==\"CH\"])\n\n[1] 1.857383\n\n\n\n\nExercise 3\n\nThe average price of sale is found by using the weighted average formula. \\(\\frac{\\sum w_{i}x_{i}}{\\sum w_{i}}\\) The weights (\\(w_{i}\\)) are given by the number of shares bought and the values (\\(x_{i}\\)) are the prices. The weighted average is \\(246.802\\).\n\nIn R you can create two vectors. One holds the share price and the other one the number of shares bought.\n\nPricePerShare&lt;-c(250.34,234.59,270.45)\nNumberOfShares&lt;-c(80,120,50)\n\nNext, you can multiply the PricePerShare and NumberOfShares vectors to find the numerator and then use sum() function to find the denominator. The weighted average is:\n\n(WeightedAverage&lt;-\n  sum(PricePerShare*NumberOfShares)/sum(NumberOfShares))\n\n[1] 246.802\n\n\n\nThe average if equal shares were bought would be \\(251.7933\\).\n\nIn R you can use the mean() function on the PricePerShare vector.\n\n(Average&lt;-mean(PricePerShare))\n\n[1] 251.7933\n\n\n\n\nExercise 4\n\nThe inflation rate for each period is shown in the table below:\n\n\n\n\n30%\n23.08%\n12.5%\n16.67%\n\n\n\nIn R create an object to store the values of the CPI:\n\nCPI&lt;-c(1,1.3,1.6,1.8,2.1)\n\nNext use the diff() function to find the difference between the end value and start value. Divide the result by a vector of starting value and multiply times 100.\n\n(Inflation&lt;-100*diff(CPI)/CPI[1:4])\n\n[1] 30.00000 23.07692 12.50000 16.66667\n\n\n\nAt the end of 4 years it is predicted that you would have \\(1621.17\\) dollars. Each year you would have gained \\(12.84\\)% on average.\n\nIn R include the annual rates in a vector:\n\ngrowth&lt;-c(0.173,0.196,0.068,0.082)\n\nThe arithmetic mean is:\n\n100*mean(growth)\n\n[1] 12.975\n\n\nThe geometric mean is:\n\n(geom&lt;-((prod(1+growth))^(1/4)-1)*100)\n\n[1] 12.8384\n\n\nAt the end of the four years we would have:\n\n1000*(1+geom/100)^4\n\n[1] 1621.167",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics III</span>"
    ]
  },
  {
    "objectID": "DescriptiveIV.html",
    "href": "DescriptiveIV.html",
    "title": "4  Descriptive Stats IV",
    "section": "",
    "text": "4.1 The Range\nUnderstanding the spread or variability of data is important when making informed decisions with data. Measures of dispersion provide insights into how much the values in a data set deviate from the central tendency Below, we explore the main statistics that help us quantify variability:\nThis is the simplest measure of dispersion, defined as the difference between the largest and smallest values in a variable. While straightforward, the range only accounts for the extremes and does not reflect the variability within the variable. The range is calculated by \\(Range=largest-smallest\\).\nEx: Consider the data \\(x=\\{480, 1050, 1400, 2500, 3200\\}\\). The range is given by \\(Range=3200-480=2720\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Stats IV</span>"
    ]
  },
  {
    "objectID": "DescriptiveIV.html#concepts",
    "href": "DescriptiveIV.html#concepts",
    "title": "4  Descriptive Stats IV",
    "section": "",
    "text": "Measures of Dispersion\nMeasures of dispersion are used to determine the spread (variability) of the data.\n\nThe range is calculated by \\(largest-smallest\\). It ignores the variability of the data between the largest and smallest values.\nThe variance calculates the dispersion around the mean. It uses squared deviations. The population parameter is given by \\(\\sigma^2= \\frac{\\sum (x_i-\\mu)^2}{N}\\), while the sample statistic is \\(s^2=\\frac{\\sum (x_i-\\bar{x})^2}{n-1}\\).\nThe standard deviation measures the average deviation around the mean. It is calculated as the square root of the variance. For the population parameter use \\(\\sigma=\\sqrt{\\sigma^2}\\) and \\(s=\\sqrt{s^2}\\) for the sample statistic.\nThe Mean Absolute Deviation (\\(MAD\\)) measures the average deviation from the mean. This measure uses absolute deviations. It is calculated by \\(MAD=\\frac{\\sum |x_i-\\mu|}{N}\\) for the population and \\(mad=\\frac{\\sum |x_i-\\bar{x}|}{n}\\) for the sample.\nThe coefficient of variation \\(CV=s/\\bar{x}\\) adjusts the standard deviation for differences in units of measure or scale.\n\n\n\nPortfolio Assesment\nTo asses the performance of a portfolio calculate:\n\nThe mean return of the portfolio \\(\\alpha\\bar{R}_1+(1-\\alpha)\\bar{R}_2\\), where \\(\\alpha\\) is the weight of investment 1 in the portfolio and \\(\\bar{R}_i\\) is the average return of investment \\(i \\in\\){\\(1\\),\\(2\\)}.\nThe variance of the portfolio is given by \\(\\begin{bmatrix} \\alpha \\\\ 1-\\alpha \\end{bmatrix}^T \\begin{bmatrix} s_x^2 & s_{xy} \\\\ s_{xy} & s_y^2 \\end{bmatrix} \\begin{bmatrix} \\alpha \\\\ 1-\\alpha \\end{bmatrix}\\)\nThe Sharpe ratio quantifies the excess return of an investment over the risk free return. It is calculated by \\(\\frac{\\bar{R_p}-R_f}{s}\\), where \\(\\bar{R_p}\\) is the mean return of the portfolio, \\(R_f\\) is the risk free return, and \\(s\\) is the standard deviation.\n\n\n\nUseful R Functions\nThe range() function returns the maximum and minimum of a vector of values.\nThe diff() function finds the first difference of a vector.\nThe var() function calculates the sample variance for a vector of values. To calculate the population variance, adjust the result by a factor of \\((n-1)/n\\).\nThe sd() function calculates the sample standard deviation.\nThe matrix() function defines a matrix.\nWhen dealing with matrices, the t() function transposes a vector or matrix, and the operator %*% performs matrix multiplication.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Stats IV</span>"
    ]
  },
  {
    "objectID": "DescriptiveIV.html#exercises",
    "href": "DescriptiveIV.html#exercises",
    "title": "4  Descriptive Stats IV",
    "section": "4.8 Exercises",
    "text": "4.8 Exercises\nThe following exercises will help you practice the measures of dispersion. In particular, the exercises work on:\n\nCalculating the range, MAD, variance, and the standard deviation.\nUsing R to calculate measures of dispersion.\nCalculating and using the Sharpe ratio to select investments.\n\nAnswers are provided below. Try not to peak until you have a formulated your own answer and double checked your work for any mistakes.\n\nExercise 1\nFor the following exercises, make your calculations by hand and verify results using R functions when possible. Make sure to calculate the deviations from the mean.\n\nUse the following observations to calculate the Range, MAD, Variance and Standard Deviation. Assume that the data below is the entire population.\n\n\n\n70\n68\n4\n98\n\n\n\n\n\n\nAnswer\n\nThe mean is \\(60\\), the Range is \\(94\\), the MAD is \\(28\\), the variance is \\(1186\\) and the variance is \\(34.44\\).\nStart by crating a vector to hold the values:\n\nEx1&lt;-c(70,68,4,98)\n\nThe range can be calculated by using the range() and diff() functions in R.\n\n(Range&lt;-diff(range(Ex1)))\n\n[1] 94\n\n\nNext, we can create a table by hand that captures the deviations from the mean. Let’s calculate the mean first:\n\n(Average1&lt;-mean(Ex1))\n\n[1] 60\n\n\nNow we can use the mean to fill out a table of deviations:\n\n\n\n\\(x_{i}\\)\n\\(x_{i}-\\bar{x}\\)\n\\((x_{i}-\\bar{x})^2\\)\n\\(|x_{i}-\\bar{x}|\\)\n\n\n\n\n70\n10\n100\n10\n\n\n68\n8\n64\n8\n\n\n4\n-56\n3136\n56\n\n\n98\n38\n1444\n38\n\n\n\nThe variance averages out the squared deviations \\((x_{i}-\\bar{x})^2\\), the MAD averages out the absolute deviations \\(|x_{i}-\\bar{x}|\\), and the standard deviation is the square root of the variance.\nLet’s verify the variance in R:\n\nSquaredDeviations1&lt;-(Ex1-Average1)^2\nAverageDeviations1&lt;-mean(SquaredDeviations1)\nvar(Ex1)*3/4\n\n[1] 1186\n\n\nNote that R calculates the sample variance. Hence, we must multiply the result by \\(3/4\\) to get the population variance. The standard deviation is just the square root of the variance:\n\nsqrt(AverageDeviations1)\n\n[1] 34.43835\n\n\nLastly, the MAD is calculated by averaging the absolute deviations \\(|x_{i}-\\bar{x}|\\).\n\nAbsoluteDeviations1&lt;-abs(Ex1-Average1)\nmean(AbsoluteDeviations1)\n\n[1] 28\n\n\n\n\nUse the following observations to calculate the Range, MAD, Variance and Standard Deviation. Assume that the data below is a sample from the population.\n\n\n\n-4\n0\n-6\n1\n-3\n0\n\n\n\n\n\n\nAnswer\n\nThe mean is \\(-2\\), Range is \\(7\\), the MAD is \\(2.33\\), the variance is \\(7.6\\) and the standard deviation is \\(2.76\\).\nHere is the table of deviations from the mean:\n\n\n\n\\(x_{i}\\)\n\\(x_{i}-\\bar{x}\\)\n\\((x_{i}-\\bar{x})^2\\)\n\\(|x_{i}-\\bar{x}|\\)\n\n\n\n\n-4\n-2\n4\n2\n\n\n0\n2\n4\n2\n\n\n-6\n-4\n16\n4\n\n\n1\n3\n9\n3\n\n\n-3\n-1\n1\n1\n\n\n0\n2\n4\n2\n\n\n\nWe can check the results in R. Let’s start with the variance:\n\nEx2&lt;-c(-4,0,-6,1,-3,0)\nvar(Ex2)\n\n[1] 7.6\n\n\nThe standard deviation can be found with the sd() function:\n\nsd(Ex2)\n\n[1] 2.75681\n\n\nThe MAD is given by:\n\n(MAD&lt;-mean(abs(Ex2-mean(Ex2))))\n\n[1] 2.333333\n\n\nLastly, the range:\n\ndiff(range(Ex2))\n\n[1] 7\n\n\n\n\n\nExercise 2\nYou will need the Stocks data set to answer this question. You can find this data at https://jagelves.github.io/Data/Stocks.csv The data is a sample of daily stock prices for ticker symbols TSLA (Tesla), VTI (S&P 500) and GBTC (Bitcoin).\n\nCalculate the standard deviations for each stock. Which stock had the lowest standard deviation?\n\n\n\nAnswer\n\nFor the sample taken, GBTC has the less variation. The standard deviation of GBTC is \\(9.43\\), which is less than \\(16.57\\) for VTI or \\(50.38\\) for TSLA.\nStart by loading the data set from the website. Since the file is in csv format, we will use the read.csv() function.\n\nStockPrices&lt;-read.csv(\"https://jagelves.github.io/Data/Stocks.csv\")\n\nLet’s start with the standard deviation of the Tesla stock. The standard deviation is given by:\n\nsd(StockPrices$TSLA)\n\n[1] 50.38092\n\n\nNext, let’s find the standard deviation for the S&P 500 or VTI. The standard deviation is given by:\n\nsd(StockPrices$VTI)\n\n[1] 16.5731\n\n\nFinally, let’s calculate the standard deviation for GBTC or Bitcoin.\n\nsd(StockPrices$GBTC)\n\n[1] 9.434213\n\n\n\n\nCalculate the MAD. Does your answer in 1. remain the same?\n\n\n\nAnswer\n\nThe answer is the same, since the MAD for GBTC is \\(8.46\\) which is lower than \\(14.27\\) for VTI or \\(41.67\\) for TSLA.\nTo calculate the MAD for TSLA we can use the following command:\n\n(MADTSLA&lt;-mean(abs(StockPrices$TSLA-mean(StockPrices$TSLA))))\n\n[1] 41.67163\n\n\nThe MAD for VTI is:\n\n(MADVTI&lt;-mean(abs(StockPrices$VTI-mean(StockPrices$VTI))))\n\n[1] 14.27169\n\n\nThe MAD for GBTC is:\n\n(MADGBTC&lt;-mean(abs(StockPrices$GBTC-mean(StockPrices$GBTC))))\n\n[1] 8.458029\n\n\n\n\nFinally, calculate the coefficient of variation. Any changes to your conclusions?\n\n\n\nAnswer\n\nBy considering the magnitudes of the stock prices, it seems like VTI is the less volatile stock. VTI has a CV of \\(0.08\\) which is lower than \\(0.44\\) for GBTC or \\(0.18\\) for TSLA. In fact, by CV Bitcoin seems to be the most risky asset.\nThe coefficients of variations are as follows. For TSLA the CV is:\n\n(CVTSLA&lt;-sd(StockPrices$TSLA)/mean(StockPrices$TSLA))\n\n[1] 0.1793755\n\n\nFor VTI the CV is:\n\n(CVVTI&lt;-sd(StockPrices$VTI)/mean(StockPrices$VTI))\n\n[1] 0.07970004\n\n\nFor GBTC we get:\n\n(CVGBTC&lt;-sd(StockPrices$GBTC)/mean(StockPrices$GBTC))\n\n[1] 0.4442497\n\n\n\n\n\nExercise 3\nInstall the ISLR2 package. You will need the Portfolio data set to answer this question. The data has 100 records of the returns of two stocks.\n\nCalculate the mean and standard deviation for each stock. Which investment has higher returns on average? Which investment is safest as measured by the standard deviation?\n\n\n\nAnswer\n\nThe best performing stock on average is stock X. It has an average return of \\(-0.078\\)% vs. \\(0.097\\)% for stock Y. The safest stock is stock X as well, since the standard deviation is \\(1.062\\) percentage points vs. \\(1.14\\) percentage points for stock Y.\nStart by loading the ISLR2 package:\n\nlibrary(ISLR2)\n\nNext, calculate the mean for stock X:\n\nmean(Portfolio$X)\n\n[1] -0.07713211\n\n\nand stock Y.\n\nmean(Portfolio$Y)\n\n[1] -0.09694472\n\n\nThen, calculate the standard deviation for stock X\n\nsd(Portfolio$X)\n\n[1] 1.062376\n\n\nand stock Y.\n\nsd(Portfolio$Y)\n\n[1] 1.143782\n\n\n\n\nUse a Risk Free rate of return of 3.5% to calculate the Sharpe ratio for each stock. Which stock would you recommend?\n\n\n\nAnswer\n\nThe Sharpe Ratio measures the excess return per unit of risk taken. Stock X has the better Sharpe Ratio. \\(-0.106\\) vs. \\(-0.115\\). Stock X is recommended since it provides a higher excess return per unit of risk taken.\nTo calculate Sharpe Ratios use both the average return, and the standard deviation. For stock X, the Sharpe Ratio is:\n\n(mean(Portfolio$X)-0.035)/sd(Portfolio$X)\n\n[1] -0.1055484\n\n\nThe Sharpe Ratio for stock Y:\n\n(mean(Portfolio$Y)-0.035)/sd(Portfolio$Y)\n\n[1] -0.1153583\n\n\n\n\nCalculate the average return for a portfolio that has 30% of stock X and 70% of stock Y. What is the standard deviation of the portfolio?\n\n\n\nAnswer\n\nThe portfolio has an average return of \\(-0.091\\) which is worse than stock X but better than stock Y. The standard deviation is \\(1.00\\). This is better than stock X and Y separately. The Sharpe ratio of \\(-0.091\\) is also better for the portfolio than for each stock individually.\nThe mean of the portfolio is given by:\n\n(mean_return=0.3*mean(Portfolio$X)+0.7*mean(Portfolio$Y))\n\n[1] -0.09100094\n\n\nThe covariance matrix is given by:\n\n(risk&lt;-cov(Portfolio))\n\n          X         Y\nX 1.1286424 0.6263583\nY 0.6263583 1.3082375\n\n\nUsing the matrix we can now calculate the standard deviation:\n\n(standard&lt;-sqrt(t(c(0.3,0.7)) %*% (risk %*% c(0.3,0.7))))\n\n         [,1]\n[1,] 1.002838\n\n\nFinally, the Sharpe ration for the portfolio is:\n\nmean_return/standard[1]\n\n[1] -0.09074338",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Stats IV</span>"
    ]
  },
  {
    "objectID": "DescriptiveIV.html#answers",
    "href": "DescriptiveIV.html#answers",
    "title": "4  Descriptive Stats IV",
    "section": "4.3 Answers",
    "text": "4.3 Answers\n\nExercise 1\n\nThe mean is \\(60\\), the Range is \\(94\\), the MAD is \\(28\\), the variance is \\(1186\\) and the variance is \\(34.44\\).\n\nStart by crating a vector to hold the values:\n\nEx1&lt;-c(70,68,4,98)\n\nThe range can be calculated by using the range() and diff() functions in R.\n\n(Range&lt;-diff(range(Ex1)))\n\n[1] 94\n\n\nNext, we can create a table by hand that captures the deviations from the mean. Let’s calculate the mean first:\n\n(Average1&lt;-mean(Ex1))\n\n[1] 60\n\n\nNow we can use the mean to fill out a table of deviations:\n\n\n\n\\(x_{i}\\)\n\\(x_{i}-\\bar{x}\\)\n\\((x_{i}-\\bar{x})^2\\)\n\\(|x_{i}-\\bar{x}|\\)\n\n\n\n\n70\n10\n100\n10\n\n\n68\n8\n64\n8\n\n\n4\n-56\n3136\n56\n\n\n98\n38\n1444\n38\n\n\n\nThe variance averages out the squared deviations \\((x_{i}-\\bar{x})^2\\), the MAD averages out the absolute deviations \\(|x_{i}-\\bar{x}|\\), and the standard deviation is the square root of the variance.\nLet’s verify the variance in R:\n\nSquaredDeviations1&lt;-(Ex1-Average1)^2\nAverageDeviations1&lt;-mean(SquaredDeviations1)\nvar(Ex1)*3/4\n\n[1] 1186\n\n\nNote that R calculates the sample variance. Hence, we must multiply the result by \\(3/4\\) to get the population variance. The standard deviation is just the square root of the variance:\n\nsqrt(AverageDeviations1)\n\n[1] 34.43835\n\n\nLastly, the MAD is calculated by averaging the absolute deviations \\(|x_{i}-\\bar{x}|\\).\n\nAbsoluteDeviations1&lt;-abs(Ex1-Average1)\nmean(AbsoluteDeviations1)\n\n[1] 28\n\n\n\nThe mean is \\(-2\\), Range is \\(7\\), the MAD is \\(2.33\\), the variance is \\(7.6\\) and the standard deviation is \\(2.76\\).\n\nHere is the table of deviations from the mean:\n\n\n\n\\(x_{i}\\)\n\\(x_{i}-\\bar{x}\\)\n\\((x_{i}-\\bar{x})^2\\)\n\\(|x_{i}-\\bar{x}|\\)\n\n\n\n\n-4\n-2\n4\n2\n\n\n0\n2\n4\n2\n\n\n-6\n-4\n16\n4\n\n\n1\n3\n9\n3\n\n\n-3\n-1\n1\n1\n\n\n0\n2\n4\n2\n\n\n\nWe can check the results in R. Let’s start with the variance:\n\nEx2&lt;-c(-4,0,-6,1,-3,0)\nvar(Ex2)\n\n[1] 7.6\n\n\nThe standard deviation can be found with the sd() function:\n\nsd(Ex2)\n\n[1] 2.75681\n\n\nThe MAD is given by:\n\n(MAD&lt;-mean(abs(Ex2-mean(Ex2))))\n\n[1] 2.333333\n\n\nLastly, the range:\n\ndiff(range(Ex2))\n\n[1] 7\n\n\n\n\nExercise 2\n\nFor the sample taken, GBTC has the less variation. The standard deviation of GBTC is \\(9.43\\), which is less than \\(16.57\\) for VTI or \\(50.38\\) for TSLA.\n\nStart by loading the data set from the website. Since the file is in csv format, we will use the read.csv() function.\n\nStockPrices&lt;-read.csv(\"https://jagelves.github.io/Data/Stocks.csv\")\n\nLet’s start with the standard deviation of the Tesla stock. The standard deviation is given by:\n\nsd(StockPrices$TSLA)\n\n[1] 50.38092\n\n\nNext, let’s find the standard deviation for the S&P 500 or VTI. The standard deviation is given by:\n\nsd(StockPrices$VTI)\n\n[1] 16.5731\n\n\nFinally, let’s calculate the standard deviation for GBTC or Bitcoin.\n\nsd(StockPrices$GBTC)\n\n[1] 9.434213\n\n\n\nThe answer is the same, since the MAD for GBTC is \\(8.46\\) which is lower than \\(14.27\\) for VTI or \\(41.67\\) for TSLA.\n\nTo calculate the MAD for TSLA we can use the following command:\n\n(MADTSLA&lt;-mean(abs(StockPrices$TSLA-mean(StockPrices$TSLA))))\n\n[1] 41.67163\n\n\nThe MAD for VTI is:\n\n(MADVTI&lt;-mean(abs(StockPrices$VTI-mean(StockPrices$VTI))))\n\n[1] 14.27169\n\n\nThe MAD for GBTC is:\n\n(MADGBTC&lt;-mean(abs(StockPrices$GBTC-mean(StockPrices$GBTC))))\n\n[1] 8.458029\n\n\n\nBy considering the magnitudes of the stock prices, it seems like VTI is the less volatile stock. VTI has a CV of \\(0.08\\) which is lower than \\(0.44\\) for GBTC or \\(0.18\\) for TSLA. In fact, by CV Bitcoin seems to be the most risky asset.\n\nThe coefficients of variations are as follows. For TSLA the CV is:\n\n(CVTSLA&lt;-sd(StockPrices$TSLA)/mean(StockPrices$TSLA))\n\n[1] 0.1793755\n\n\nFor VTI the CV is:\n\n(CVVTI&lt;-sd(StockPrices$VTI)/mean(StockPrices$VTI))\n\n[1] 0.07970004\n\n\nFor GBTC we get:\n\n(CVGBTC&lt;-sd(StockPrices$GBTC)/mean(StockPrices$GBTC))\n\n[1] 0.4442497\n\n\n\n\nExercise 3\n\nThe best performing stock on average is stock X. It has an average return of \\(-0.078\\)% vs. \\(0.097\\)% for stock Y. The safest stock is stock X as well, since the standard deviation is \\(1.062\\) percentage points vs. \\(1.14\\) percentage points for stock Y.\n\nStart by loading the ISLR2 package:\n\nlibrary(ISLR2)\n\nNext, calculate the mean for stock X:\n\nmean(Portfolio$X)\n\n[1] -0.07713211\n\n\nand stock Y.\n\nmean(Portfolio$Y)\n\n[1] -0.09694472\n\n\nThen, calculate the standard deviation for stock X\n\nsd(Portfolio$X)\n\n[1] 1.062376\n\n\nand stock Y.\n\nsd(Portfolio$Y)\n\n[1] 1.143782\n\n\n\nThe Sharpe Ratio measures the excess return per unit of risk taken. Stock X has the better Sharpe Ratio. \\(-0.106\\) vs. \\(-0.115\\). Stock X is recommended since it provides a higher excess return per unit of risk taken.\n\nTo calculate Sharpe Ratios use both the average return, and the standard deviation. For stock X, the Sharpe Ratio is:\n\n(mean(Portfolio$X)-0.035)/sd(Portfolio$X)\n\n[1] -0.1055484\n\n\nThe Sharpe Ratio for stock Y:\n\n(mean(Portfolio$Y)-0.035)/sd(Portfolio$Y)\n\n[1] -0.1153583\n\n\n\nThe portfolio has an average return of \\(-0.091\\) which is worse than stock X but better than stock Y. The standard deviation is \\(1.00\\). This is better than stock X and Y separately. The Sharpe ratio of \\(-0.091\\) is also better for the portfolio than for each stock individually.\n\nThe mean of the portfolio is given by:\n\n(mean_return=0.3*mean(Portfolio$X)+0.7*mean(Portfolio$Y))\n\n[1] -0.09100094\n\n\nThe covariance matrix is given by:\n\n(risk&lt;-cov(Portfolio))\n\n          X         Y\nX 1.1286424 0.6263583\nY 0.6263583 1.3082375\n\n\nUsing the matrix we can now calculate the standard deviation:\n\n(standard&lt;-sqrt(t(c(0.3,0.7)) %*% (risk %*% c(0.3,0.7))))\n\n         [,1]\n[1,] 1.002838\n\n\nFinally, the Sharpe ration for the portfolio is:\n\nmean_return/standard[1]\n\n[1] -0.09074338",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Stats IV</span>"
    ]
  },
  {
    "objectID": "DescriptiveV.html",
    "href": "DescriptiveV.html",
    "title": "5  Descriptive Stats V",
    "section": "",
    "text": "5.1 Quantiles and Percentiles\nThere are statistical measures that describe the shape and distribution of the data beyond simple measures of central location or dispersion. They provide a view of how data is spread out, where it concentrates, and how it deviates from what might be expected. The tools shown below will help you describe the data’s shape, symmetry, and anomalies.\nA quantile is a location within a set of ranked numbers (or distribution), below which a certain proportion, \\(q\\), of that set lie. If we instead express quantiles as a percentage, they are referred to as percentiles.\nEx: Imagine all your data points lined up from smallest to largest. If you say you’re looking at the 25th percentile, it means you’re finding the value below which 25% of your data falls. If you had 100 test scores, the 25th percentile would be the score where 25 students scored lower than that, and 75 scored higher or equal. It’s a way to see where a value stands in relation to the rest of the data in terms of percentage.\nTo calculate a percentile we follow the steps below:\nExample: Let’s use the IQ scores for a group of students to find the 25th percentile. \\(IQ=\\{80,100,110,75,130,90\\}\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Stats V</span>"
    ]
  },
  {
    "objectID": "DescriptiveV.html#concepts",
    "href": "DescriptiveV.html#concepts",
    "title": "5  Descriptive Stats V",
    "section": "",
    "text": "Quantiles and Percentiles\nA quantile is a location within a set of ranked numbers (or distribution), below which a certain proportion, \\(q\\), of that set lie. Ex: 0.25 of the data lies below the 0.25 quantile.\nPercentiles express quantiles in percentage form. Ex: 25% of the data lies below the 25th percentile. To calculate a percentile:\n\nSort the data in ascending order.\nCompute the location of the percentile desired using \\(L_{p}=\\frac{(n+1)P}{100}\\) where \\(L_{p}\\) is the location of the \\(P_{th}\\) percentile, and \\(P\\) is the percentile desired.\nThe value at \\(L_p\\), is the the \\(P_{th}\\) percentile.\n\n\n\nChevyshev’s Theorem and Empirical Rule\nChevyshev’s Theorem states that at least \\(1-1/z^2\\)% of the data lies between \\(z\\) standard deviations from the mean. This result does not depend on the shape of the distribution.\nThe Empirical Rule or (\\(68\\),\\(95\\),\\(99.7\\) rule) states that \\(68\\)%, \\(95\\)%, and \\(99.7\\)% of the data lies between \\(1\\), \\(2\\), and \\(3\\) standard deviations from the mean respectively. The rule depends on the data being normally distributed.\n\n\nFive Point Summary and Outliers\nA popular way to summarize data is by calculating the minimum, first quartile, median, third quartile and maximum (five point summary).\nThe interquartile range (IQR) is the difference between the third quartile and the first quartile.\nOutliers are extreme deviations from the mean. They are values that are not “normal”. To calculate outliers:\n\nUse a z-score to measure the distance from the mean in units of standard deviation. \\(z_{i}=\\frac{x_i-\\bar{x}}{s_x}\\). \\(z\\)-scores above \\(3\\) are suspected outliers.\nCalculate \\(Q_1-1.5(IQR)\\) and \\(Q_3+1.5(IQR)\\), where \\(Q_1\\) is the first quartile, \\(Q_3\\) is the third quartile, and \\(IQR\\) is the interquartile range. If \\(x_i\\) is less than \\(Q_1-1.5(IQR)\\) or greater than \\(Q_3+1.5(IQR)\\), then it is considered an outlier.\n\nA box plot is a graph that shows the five point summary, outliers (if any), and the distribution of data.\nTo determine if the data is skewed, calculate the Pearson’s Coefficient of Skew. \\(Sk=\\frac{3(\\bar {x}- Median)}{s_x}\\). The distribution is skewed to the left if \\(Sk&lt;0\\), skewed to the right is \\(Sk&gt;0\\), and symmetric if \\(Sk=0\\).\n\n\nUseful R Functions\nThe quantile() function returns the five point summary when no arguments are specified. For a specific quantile, specify the probs argument.\nThe boxplot() command returns a box plot for a vector of values.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Stats V</span>"
    ]
  },
  {
    "objectID": "DescriptiveV.html#exercises",
    "href": "DescriptiveV.html#exercises",
    "title": "5  Descriptive Stats V",
    "section": "5.11 Exercises",
    "text": "5.11 Exercises\nThe following exercises will help you practice other statistical measures. In particular, the exercises work on:\n\nConstructing a five point summary and a boxplot.\nApplying Chebyshev’s Theorem.\nIdentifying skewness.\nIdentifying outliers.\n\nAnswers are provided below. Try not to peak until you have a formulated your own answer and double checked your work for any mistakes.\n\nExercise 1\nFor the following exercises, make your calculations by hand and verify results using R functions when possible.\n\nUse the following observations to calculate the minimum, the first, second and third quartiles, and the maximum. Are there any outliers? Find the IQR to answer the question.\n\n\n\n3\n10\n4\n1\n0\n30\n6\n\n\n\n\n\n\nAnswer\n\nThe minimum is \\(0\\), the first quartile is \\(2\\), second quartile is \\(4\\), third quartile is \\(8\\), and maximum is \\(30\\). \\(30\\) is an outlier since it is beyond \\(Q_{3}+1.5 \\times IQR\\).\nQuartiles are calculated using the percentile formula \\((n+1)P/100\\). The data set has seven numbers. The first quartile’s location is \\(8/4=2\\), the second quartile’s location is \\(8/2=4\\) and the third quartile’s location is \\(24/4=6\\). The values at these location, when data is organized in ascending order, are \\(1\\), \\(4\\), and \\(10\\).\nIn R we can get the five number summary by using the quantile() function. Since there are various rules that can be used to calculate percentiles, we specify type \\(6\\) to match our rules.\n\nEx1&lt;-c(3,10,4,1,0,30,6)\nquantile(Ex1,type = 6)\n\n  0%  25%  50%  75% 100% \n   0    1    4   10   30 \n\n\nThe interquartile range is needed to determine if there are any outliers. The \\(IQR\\) for this data set is \\(Q_{3}-Q_{1}=9\\). This reveals that \\(30\\) is and outlier, since \\(10+1.5 \\times 9=23.5\\). Everything beyond \\(23.5\\) is an outlier.\n\n\nConfirm your finding of an outlier by calculating the \\(z\\)-score. Is \\(30\\) an outlier when using a \\(z\\)-Score?\n\n\n\nAnswer\n\nIf we use the \\(z\\)-score instead we find that \\(30\\) is not an outlier since the \\(z\\)-score is \\(Z_{30}=2.15\\). This observation is only \\(2.15\\) standard deviations away from the mean.\nIn R we can make a quick calculation of the \\(z\\)-Score to confirm our results. The \\(z\\)-score is given by \\(Z_{i}=\\frac{x_{30}-\\mu}{\\sigma}\\).\n\n(Z30&lt;-(30-mean(Ex1))/sd(Ex1))\n\n[1] 2.148711\n\n\n\n\nUse Chebyshev’s theorem to determine what percent of the data falls between the \\(z\\)-score found in \\(2\\).\n\n\n\nAnswer\n\nChebyshev’s theorem states that \\(1-\\frac{1}{z_{2}}\\) of the data lies between \\(z\\) standard deviation from the mean.\nSubstituting the \\(z\\)-score found in 2. we get \\(78.34\\)% of the data lies between the standard deviation calculated. In R:\n\n1-1/(Z30)^2\n\n[1] 0.7834073\n\n\n\n\n\nExercise 2\nYou will need the Stocks data set to answer this question. You can find this data at https://jagelves.github.io/Data/Stocks.csv The data is a sample of daily stock prices for ticker symbols TSLA (Tesla), VTI (S&P 500) and GBTC (Bitcoin).\n\nConstruct a boxplot for Stock A. Is the data skewed or symmetric?\n\n\n\nAnswer\n\nThe data is skewed to the right.\nStart by loading the data set:\n\nStockPrices&lt;-read.csv(\"https://jagelves.github.io/Data/Stocks.csv\")\n\nTo construct the boxplot in R, use the boxplot() command.\n\nStockPrices %&gt;% \n  ggplot() +\n  geom_boxplot(aes(y=VTI),\n               fill=\"lightgrey\", alpha=0.5,\n               col=\"black\", width=0.3) +\n  theme_clean() +\n  scale_x_continuous(breaks = NULL, limits=c(-1,1))\n\n\n\n\n\n\n\n\nThe boxplot shows that there are no outliers. The data also looks like it has a slight skew to the right.\n\n\nCreate a histogram of the data. Include a vertical line for the mean and median. Explain how the mean and median indicates a skew in the data. Calculate the skewness statistic to confirm your result.\n\n\n\nAnswer\n\nThe mean is more sensitive to outliers than the median. Hence, when the data is skewed to the right we expect that the mean is larger than the median.\nLet’s construct a histogram in R to search for skewness.\n\nStockPrices %&gt;% ggplot() +\n  geom_histogram(aes(VTI), bins = 8,\n                 binwidth = 8,\n                 col=\"black\", bg=\"grey\",\n                 boundary=179) +\n  theme_clean() +\n  geom_vline(xintercept = mean(StockPrices$VTI), col=\"black\")+\n  geom_vline(xintercept = median(StockPrices$VTI), col=\"orange\")\n\n\n\n\n\n\n\n\nThe lines are close to each other but the mean is slighlty larger than the median. Let’s confirm with the skewness statistic \\(3(mean-median)/sd\\).\n\n(skew&lt;-3*(mean(StockPrices$VTI-median(StockPrices$VTI))/sd(StockPrices$VTI)))\n\n[1] 0.2856304\n\n\nThis indicates that there is a slight skew to the right of the data.\n\n\nUse a line chart to plot your data. Can you explain why the data has a skew?\n\n\n\nAnswer\n\nThe line chart indicates that the data has a downward trend in the early periods. This creates a few points that are large. In later periods the stock price stabilizes to lower levels.\n\nStockPrices %&gt;% ggplot() +\n  geom_line(aes(y=VTI, x=seq(1,length(VTI)))) + theme_clean() +\n  labs(x=\"Period\")\n\n\n\n\n\n\n\n\n\n\n\nExercise 3\nYou will need the mtcars data set to answer this question. This data set is part of R. You don’t need to download any files to access it.\n\nConstruct a boxplot for the hp variable. Write a command in R that retrieves the outlier. Which car is the outlier?\n\n\n\nAnswer\n\nThe outlier is the Masserati Bora. The horse power is \\(335\\).\nIn R we can construct a boxplot with the following command:\n\nmtcars %&gt;% \n  ggplot() +\n  geom_boxplot(aes(y=hp),\n               fill=\"lightgrey\", alpha=0.5,\n               col=\"black\", width=0.3,\n               outlier.colour = \"red\") +\n  theme_clean() +\n  scale_x_continuous(breaks = NULL, limits=c(-1,1))\n\n\n\n\n\n\n\n\nFrom the graph it seems like the outlier is beyond a horsepower of 275. Let’s write an R command to retrieve the car.\n\nmtcars %&gt;% filter(hp&gt;300)\n\n              mpg cyl disp  hp drat   wt qsec vs am gear carb\nMaserati Bora  15   8  301 335 3.54 3.57 14.6  0  1    5    8\n\n\nIt’s the Masserati Bora!\n\n\nCreate a histogram of the data. Is the data skewed? Include a vertical line for the mean and median. Calculate the skewness statistic to confirm your result.\n\n\n\nAnswer\n\nThe histogram looks skewed to the right. This is confirmed by the estimation of a Pearson coefficient fo skewness of \\(1.04\\).\nIn R we can construct a histogram with vertical lines for the mean and median with the following code:\n\nmtcars %&gt;% ggplot() +\n  geom_histogram(aes(hp), bins = 5,\n                 binwidth = 60,\n                 col=\"black\", bg=\"grey\",\n                 boundary=50) +\n  theme_clean() +\n  geom_vline(xintercept = mean(StockPrices$VTI), col=\"black\")+\n  geom_vline(xintercept = median(StockPrices$VTI), col=\"orange\")\n\n\n\n\n\n\n\n\nThe histogram looks skewed to the right. Pearson’s Coefficient of Skewness is:\n\n(SkewHP&lt;-3*(mean(mtcars$hp)-median(mtcars$hp))/sd(mtcars$hp))\n\n[1] 1.036458\n\n\n\n\nTransform the data by taking a natural logarithm. Specifically, create a new variable called Loghp. Repeat the procedure in 2. Is the skew still there?\n\n\n\nAnswer\n\nThe skew is still there, but the distribution now look more symmetrical and the Skew coefficient has decreased to \\(0.44\\).\nIn R we can create an new variable that captures the log transformation. The log() function takes the natural logarithm of a number or vector.\n\nLogHP&lt;-log(mtcars$hp)\n\nLet’s use this new variable to create our histogram:\n\nggplot() +\n  geom_histogram(aes(LogHP), bins = 5,\n                 col=\"black\", bg=\"grey\") +\n  theme_clean() +\n  geom_vline(xintercept = mean(LogHP), col=\"black\")+\n  geom_vline(xintercept = median(LogHP), col=\"orange\")\n\n\n\n\n\n\n\n\nThe mean and the variance now look closer together. The tail of the distribution (skew) now also looks diminished. The Skewness coefficient has decreased significantly:\n\n(SkewLogHP&lt;-3*(mean(LogHP)-median(LogHP))/sd(LogHP))\n\n[1] 0.4402212",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Stats V</span>"
    ]
  },
  {
    "objectID": "DescriptiveV.html#answers",
    "href": "DescriptiveV.html#answers",
    "title": "5  Descriptive Stats V",
    "section": "5.8 Answers",
    "text": "5.8 Answers\n\nExercise 1\n\nThe minimum is \\(0\\), the first quartile is \\(2\\), second quartile is \\(4\\), third quartile is \\(8\\), and maximum is \\(30\\). \\(30\\) is an outlier since it is beyond \\(Q_{3}+1.5*IQR\\).\n\nQuartiles are calculated using the percentile formula \\((n+1)P/100\\). The data set has seven numbers. The first quartile’s location is \\(8/4=2\\), the second quartile’s location is \\(8/2=4\\) and the third quartile’s location is \\(24/4=6\\). The values at these location, when data is organized in ascending order, are \\(1\\), \\(4\\), and \\(10\\).\nIn R we can get the five number summary by using the quantile() function. Since there are various rules that can be used to calculate percentiles, we specify type \\(6\\) to match our rules.\n\nEx1&lt;-c(3,10,4,1,0,30,6)\nquantile(Ex1,type = 6)\n\n  0%  25%  50%  75% 100% \n   0    1    4   10   30 \n\n\nThe interquartile range is needed to determine if there are any outliers. The \\(IQR\\) for this data set is \\(Q_{3}-Q_{1}=9\\). This reveals that \\(30\\) is and outlier, since \\(10+1.5*9=23.5\\). Everything beyond \\(23.5\\) is an outlier.\n\nIf we use the \\(z\\)-score instead we find that \\(30\\) is not an outlier since the \\(z\\)-score is \\(Z_{30}=2.15\\). This observation is only \\(2.15\\) standard deviations away from the mean.\n\nIn R we can make a quick calculation of the \\(z\\)-Score to confirm our results. The \\(z\\)-score is given by \\(Z_{i}=\\frac{x_{30}-\\mu}{\\sigma}\\).\n\n(Z30&lt;-(30-mean(Ex1))/sd(Ex1))\n\n[1] 2.148711\n\n\n\nChebyshev’s theorem states that \\(1-\\frac{1}{z_{2}}\\) of the data lies between \\(z\\) standard deviation from the mean.\n\nSubstituting the \\(z\\)-score found in 2. we get \\(78.34\\)%. In R:\n\n1-1/(Z30)^2\n\n[1] 0.7834073\n\n\n\n\nExercise 2\n\nThe data is skewed to the right.\n\nStart by loading the data set:\n\nStockPrices&lt;-read.csv(\"https://jagelves.github.io/Data/Stocks.csv\")\n\nTo construct the boxplot in R, use the boxplot() command.\n\nboxplot(as.numeric(StockPrices$VTI),axes=F, ylim=c(120,260),\n        cex=1.5, col=\"#F5F5F5\",pch=21,bg=\"red\")\naxis(side=1, labels=c(\"VTI\"), at=seq(1))\naxis(side=2, labels=TRUE, at=seq(140,260,20),font=1,las=1)\n\n\n\n\n\n\n\n\nThe boxplot shows that there are no outliers. The data also looks like it has a slight skew to the right.\n\nThe mean is more sensitive to outliers than the median. Hence, when the data is skewed to the right we expect that the mean is larger than the median.\n\nLet’s construct a histogram in R to search for skewness.\n\nhist(StockPrices$VTI,main=\"\", ylim=c(0,40), \n     xlab=\"Prices\", col=\"#F5F5F5\")\nabline(v=mean(StockPrices$VTI),col=\"black\",lwd=2)\nabline(v=median(StockPrices$VTI),col=\"darkgrey\",lwd=2)\nlegend(x = \"topright\",          \n       legend = c(\"Mean\", \"Median\"),  \n       lty = c(1, 1),           \n       col = c(\"black\", \"darkgrey\"),         \n       lwd = 2,\n       bty=\"n\")    \n\n\n\n\n\n\n\n\nThe lines are close to each other but the mean is slighlty larger than the median. Let’s confirm with the skewness statistic \\(3(mean-median)/sd\\).\n\n(skew&lt;-3*(mean(StockPrices$VTI-median(StockPrices$VTI))/sd(StockPrices$VTI)))\n\n[1] 0.2856304\n\n\nThis indicates that there is a slight skew to the right of the data.\n\nThe line chart indicates that the data has a downward trend in the early periods. This creates a few points that are large. In later periods the stock price stabilizes to lower levels.\n\n\nplot(y=StockPrices$VTI,x=seq(1,length(StockPrices$VTI)),\n     type=\"l\", ylab=\"Prices\", xlab=\"Period\", axes=F)\naxis(side=1, labels=TRUE, at=seq(0,250,50),font=1,las=1)\naxis(side=2, labels=TRUE, at=seq(0,300,20),font=1,las=1)\n\n\n\n\n\n\n\n\n\n\nExercise 3\n\nThe outlier is the Masserati Bora. The horse power is \\(335\\).\n\nIn R we can construct a boxplot with the following command:\n\nboxplot(mtcars$hp,axes=F, ylim=c(0,400),\n        cex=1.5, col=\"#F5F5F5\",pch=21,bg=\"red\")\naxis(side=1, labels=c(\"HP\"), at=seq(1))\naxis(side=2, labels=TRUE, at=seq(0,400,50),font=1,las=1)\n\n\n\n\n\n\n\n\nFrom the graph it seems like the outlier is beyond a horsepower of 275. Let’s write an R command to retrieve the car.\n\nmtcars[mtcars$hp&gt;275,]\n\n              mpg cyl disp  hp drat   wt qsec vs am gear carb\nMaserati Bora  15   8  301 335 3.54 3.57 14.6  0  1    5    8\n\n\nIt’s the Masserati Bora!\n\nThe histogram looks skewed to the right. This is confirmed by the estimation of a Pearson coefficient fo skewness of \\(1.04\\).\n\nIn R we can construct a histogram with vertical lines for the mean and median wit the following code:\n\nhist(mtcars$hp,main=\"\", ylim=c(0,12), xlab=\"Horse Power\",\n     col=\"#F5F5F5\")\nabline(v=mean(mtcars$hp),col=\"black\",lwd=2)\nabline(v=median(mtcars$hp),col=\"darkgrey\",lwd=2)\nlegend(x = \"topright\",          \n       legend = c(\"Mean\", \"Median\"),  \n       lty = c(1, 1),           \n       col = c(\"black\", \"darkgrey\"),         \n       lwd = 2,\n       bty=\"n\")    \n\n\n\n\n\n\n\n\nThe histogram looks skewed to the right. Pearson’s Coefficient of Skewness is:\n\n(SkewHP&lt;-3*(mean(mtcars$hp)-median(mtcars$hp))/sd(mtcars$hp))\n\n[1] 1.036458\n\n\n\nThe skew is still there, but the distribution now look more symmetrical and the Skew coefficient has decreased to \\(0.44\\).\n\nIn R we can create an new variable that captures the log transformation. The log() function takes the natural logarithm of a number or vector.\n\nLogHP&lt;-log(mtcars$hp)\n\nLet’s use this new variable to create our histogram:\n\nhist(LogHP,main=\"\", ylim=c(0,12), xlab=\"Horse Power\", \n     col=\"#F5F5F5\")\nabline(v=mean(LogHP),col=\"black\",lwd=2)\nabline(v=median(LogHP),col=\"darkgrey\",lwd=2)\nlegend(x = \"topright\",          \n       legend = c(\"Mean\", \"Median\"),  \n       lty = c(1, 1),           \n       col = c(\"black\", \"darkgrey\"),         \n       lwd = 2,\n       bty=\"n\")    \n\n\n\n\n\n\n\n\nThe mean and the variance now look closer together. The tail of the distribution (skew) now also looks diminished. The Skewness coefficient has decreased significantly:\n\n(SkewLogHP&lt;-3*(mean(LogHP)-median(LogHP))/sd(LogHP))\n\n[1] 0.4402212",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Stats V</span>"
    ]
  },
  {
    "objectID": "RegressionI.html",
    "href": "RegressionI.html",
    "title": "6  Regression I",
    "section": "",
    "text": "6.1 Concepts",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regression I</span>"
    ]
  },
  {
    "objectID": "RegressionI.html#concepts",
    "href": "RegressionI.html#concepts",
    "title": "6  Regression I",
    "section": "",
    "text": "Measures of Association\nMeasures of association determine whether there is a linear relationship between two variables. They also determine the strength of the relationship.\n\nThe covariance is a measure that determines the direction of the relationship between two variables. It is calculated by \\(s_{xy}=\\frac {\\sum(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum (x_i-\\bar{x})^2}\\). If \\(s_{xy}&gt;0\\) there is a direct relationship, if \\(s_{xy}&lt;0\\) there is an inverse relationship, and if \\(s_{xy}=0\\) there is no relationship.\nThe correlation measures the strength of the linear relationship. It is calculated by \\(r= \\frac {s_{xy}}{s_x s_y}\\). The correlation coefficient is between \\([-1,1]\\). When the correlation coefficient is \\(1\\) (\\(-1\\)), there is a perfect direct (inverse) relationship between the two variables.\nThe coefficient of determination or \\(R^2\\), measures the percent of variation in \\(y\\) explained by variations in \\(x\\). It is calculated by \\(R^2=r^2\\). The next chapter expands on this measure.\nA scatter plot displays pairs of [\\(x\\),\\(y\\)] as points on the Cartesian plane. The plot provides a visual aid to determine the relationship between two variables.\n\n\n\nUseful R Functions\nTo calculate the covariance use the cov() function.\nThe correlation coefficient can be calculated using the cor() function.\nThe plot() function will create scatter plots.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regression I</span>"
    ]
  },
  {
    "objectID": "RegressionI.html#exercises",
    "href": "RegressionI.html#exercises",
    "title": "6  Regression I",
    "section": "6.2 Exercises",
    "text": "6.2 Exercises\nThe following exercises will help you understand statistical measures that establish the relationship between two variables. In particular, the exercises work on:\n\nCalculating covariance and correlation.\nUsing R to plot scatter diagrams.\nCalculating the coefficient of determination.\n\nAnswers are provided below. Try not to peak until you have a formulated your own answer and double checked your work for any mistakes.\n\nExercise 1\nFor the following exercises, make your calculations by hand and verify results using R functions when possible.\n\nConsider the data below. Calculate the covariance and correlation coefficient by finding deviations from the mean. Use R to verify your result. Is there a direct or inverse relationship between the two variables? How strong is the relationship?\n\n\n\n\nx\n20\n21\n15\n18\n25\n\n\n\n\ny\n17\n19\n12\n13\n22\n\n\n\n\n\nAnswer\n\nThe covariance is \\(14.9\\) and the correlation is \\(0.96\\). The results indicate that there is a strong direct relationship between the two variables.\nLet’s start by finding the deviations from the mean for the x variable in R.\n\nx&lt;-c(20,21,15,18,25)\n(devx&lt;-x-mean(x))\n\n[1]  0.2  1.2 -4.8 -1.8  5.2\n\n\nWe will do the same with y:\n\ny&lt;-c(17,19,12,13,22)\n(devy&lt;-y-mean(y))\n\n[1]  0.4  2.4 -4.6 -3.6  5.4\n\n\nNote that when the deviations in x are negative (positive), they are also negative (positive) in y. This is indicative of a direct relationship between the two variables. The covariance is given by:\n\n(Ex1Cov&lt;-sum(devx*devy)/(length(devx)-1))\n\n[1] 14.9\n\n\nWe can verify this by using cov() function in R.\n\ncov(x,y)\n\n[1] 14.9\n\n\nThe correlation coefficient is found by dividing the covariance over the product of standard deviations. In R:\n\n(Ex1Cor&lt;-Ex1Cov/(sd(x)*sd(y)))\n\n[1] 0.9678386\n\n\nWe can once more verify the result in R with the built in function cor().\n\ncor(x,y)\n\n[1] 0.9678386\n\n\n\n\nConsider the data below. Calculate the covariance and correlation coefficient by finding deviations from the mean. Use R to verify your result. Is there a direct or inverse relationship between the two variables? How strong is the relationship?\n\n\n\n\nw\n19\n16\n14\n11\n18\n\n\n\n\nz\n17\n20\n20\n16\n18\n\n\n\n\n\nAnswer\n\nThe covariance is \\(0.85\\) and the correlation is \\(0.148\\). The results indicate that there is a very weak direct relationship between the two variables. They might be unrelated.\nLet’s start with w and finding the deviations from the mean in R.\n\nw&lt;-c(19,16,14,11,18)\n(devw&lt;-w-mean(w))\n\n[1]  3.4  0.4 -1.6 -4.6  2.4\n\n\nWe will do the same with z:\n\nz&lt;-c(17,20,20,16,18)\n(devz&lt;-z-mean(z))\n\n[1] -1.2  1.8  1.8 -2.2 -0.2\n\n\nThe covariance is given by:\n\n(Ex2Cov&lt;-sum(devw*devz)/(length(devz)-1))\n\n[1] 0.85\n\n\nWe can verify this with the cov() function in R.\n\ncov(w,z)\n\n[1] 0.85\n\n\nThe correlation coefficient is found by dividing the covariance over the product of standard deviations. In R:\n\n(Ex2Cor&lt;-Ex2Cov/(sd(z)*sd(w)))\n\n[1] 0.1480558\n\n\nWe can once more verify the result in R with the built in function cor().\n\ncor(w,z)\n\n[1] 0.1480558\n\n\n\n\n\nExercise 2\nYou will need the mtcars data set to answer this question. This data set is part of R. You don’t need to download any files to access it.\n\nCalculate the correlation coefficient between hp and mpg. Explain the results. Specifically, the direction of the relationship and the strength given the context of the problem.\n\n\n\nAnswer\n\nThe correlation coefficient is \\(-0.78\\). This is indicative of a moderately strong inverse relationship between mpg and mp.\nIn R we can easily calculate the correlation coefficient with the cor() function.\n\ncor(mtcars$mpg,mtcars$hp)\n\n[1] -0.7761684\n\n\n\n\nCreate a scatter diagram of the two variables. Is the scatter diagram what you expected after you calculated the correlation coefficient?\n\n\n\nAnswer\n\nThe scatter diagram is downward sloping. Most points are close to the trend line. It is what was expected from a correlation coefficient of \\(-0.78\\).\n\nlibrary(tidyverse)\nlibrary(ggthemes)\nmtcars %&gt;% ggplot() + \n  geom_point(aes(y=mpg,x=hp), col=\"black\",\n             bg=\"grey\", pch=21) +\n  geom_smooth(aes(y=mpg,x=hp), formula=y~x,\n              method=\"lm\", se=F) +\n  theme_clean()\n\n\n\n\n\n\n\n\n\n\nCalculate the coefficient of determination. How close is it to one? What else could be explaining the variation in the mpg? Let your dependent variable be mpg.\n\n\n\nAnswer\n\nThe coefficient of determination is \\(0.6\\). This value is not very close to one. This is expected since miles per gallon can also vary because of the cars weight, and fuel efficiency. It makes sense that the hp only explains \\(60\\)% of the total variation.\nIn R we can calculate the coefficient of determination by squaring the correlation coefficient.\n\ncor(mtcars$mpg,mtcars$hp)^2\n\n[1] 0.6024373\n\n\n\n\n\nExercise 3\nYou will need the College data set to answer this question. You can find this data set here: https://jagelves.github.io/Data/College.csv\n\nCreate a scatter diagram between GRAD_DEBT_MDN (Median Debt) and MD_EARN_WNE_P10 (Median Earnings). What type of relationship do you observe between the variables?\n\n\n\nAnswer\n\nIt seems like there is a direct relationship between both variables. The more debt you take, the higher the salary.\nStart by loading the data. We’ll use the read_csv() function:\n\nlibrary(tidyverse)\nCollege&lt;-read_csv(\"https://jagelves.github.io/Data/College.csv\")\n\nThe two variables of interest are GRAD_DEBT_MDN and MD_EARN_WNE_P10. The following code creates the scatter plot:\n\nCollege %&gt;% ggplot() + \n  geom_point(aes(y=GRAD_DEBT_MDN,x=MD_EARN_WNE_P10), col=\"black\",\n             bg=\"grey\", pch=21) +\n  geom_smooth(aes(y=GRAD_DEBT_MDN,x=MD_EARN_WNE_P10), formula=y~x,\n              method=\"lm\", se=F) +\n  theme_clean()\n\n\n\n\n\n\n\n\n\n\nCalculate the correlation coefficient and the coefficient of determination. According to the data, are higher debts correlated with higher earnings?\n\n\n\nAnswer\n\nThe correlation coefficient shows a moderate direct relationship between earnings and debt \\(0.46\\). The coefficient of determination indicates that only \\(21\\)% of the variation in earnings can be explained by debt.\nIn R we can start with the correlation coefficient:\n\n(Correlation&lt;-cor(College$GRAD_DEBT_MDN,\n                  College$MD_EARN_WNE_P10,\"complete.obs\"))\n\n[1] 0.4615361\n\n\nWe can simply square the correlation to obtain the coefficient of determination:\n\nCorrelation^2\n\n[1] 0.2130155",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regression I</span>"
    ]
  },
  {
    "objectID": "RegressionI.html#answers",
    "href": "RegressionI.html#answers",
    "title": "6  Regression I",
    "section": "6.3 Answers",
    "text": "6.3 Answers\n\nExercise 1\n\nThe covariance is \\(14.9\\) and the correlation is \\(0.96\\). The results indicate that there is a strong direct relationship between the two variables.\n\nLet’s start by finding the deviations from the mean for the x variable in R.\n\nx&lt;-c(20,21,15,18,25)\n(devx&lt;-x-mean(x))\n\n[1]  0.2  1.2 -4.8 -1.8  5.2\n\n\nWe will do the same with y:\n\ny&lt;-c(17,19,12,13,22)\n(devy&lt;-y-mean(y))\n\n[1]  0.4  2.4 -4.6 -3.6  5.4\n\n\nNote that when the deviations in x are negative (positive), they are also negative (positive) in y. This is indicative of a direct relationship between the two variables. The covariance is given by:\n\n(Ex1Cov&lt;-sum(devx*devy)/(length(devx)-1))\n\n[1] 14.9\n\n\nWe can verify this by using cov() function in R.\n\ncov(x,y)\n\n[1] 14.9\n\n\nThe correlation coefficient is found by dividing the covariance over the product of standard deviations. In R:\n\n(Ex1Cor&lt;-Ex1Cov/(sd(x)*sd(y)))\n\n[1] 0.9678386\n\n\nWe can once more verify the result in R with the built in function cor().\n\ncor(x,y)\n\n[1] 0.9678386\n\n\n\nThe covariance is \\(0.85\\) and the correlation is \\(0.148\\). The results indicate that there is a very weak direct relationship between the two variables. They might be unrelated.\n\nLet’s start with w and finding the deviations from the mean in R.\n\nw&lt;-c(19,16,14,11,18)\n(devw&lt;-w-mean(w))\n\n[1]  3.4  0.4 -1.6 -4.6  2.4\n\n\nWe will do the same with z:\n\nz&lt;-c(17,20,20,16,18)\n(devz&lt;-z-mean(z))\n\n[1] -1.2  1.8  1.8 -2.2 -0.2\n\n\nThe covariance is given by:\n\n(Ex2Cov&lt;-sum(devw*devz)/(length(devz)-1))\n\n[1] 0.85\n\n\nWe can verify this with the cov() function in R.\n\ncov(w,z)\n\n[1] 0.85\n\n\nThe correlation coefficient is found by dividing the covariance over the product of standard deviations. In R:\n\n(Ex2Cor&lt;-Ex2Cov/(sd(z)*sd(w)))\n\n[1] 0.1480558\n\n\nWe can once more verify the result in R with the built in function cor().\n\ncor(w,z)\n\n[1] 0.1480558\n\n\n\n\nExercise 2\n\nThe correlation coefficient is \\(-0.78\\). This is indicative of a moderately strong inverse relationship between mpg and mp.\n\nIn R we can easily calculate the correlation coefficient with the cor() function.\n\ncor(mtcars$mpg,mtcars$hp)\n\n[1] -0.7761684\n\n\n\nThe scatter diagram is downward sloping. Most points are close to the trend line. It is what was expected from a correlation coefficient of \\(-0.78\\).\n\n\nplot(y=mtcars$mpg,x=mtcars$hp, main=\"\", \n     axes=F,pch=21, bg=\"blue\",\n     xlab=\"Horse Power\",\n     ylab=\"Miles Per Gallon\", ylim=c(10,40),xlim=c(50,400))\naxis(side=1, labels=TRUE, font=1,las=1)\naxis(side=2, labels=TRUE, font=1,las=1)\nabline(lm(mtcars$mpg~mtcars$hp),\n       col=\"darkgray\",lwd=2)\n\n\n\n\n\n\n\n\n\nThe coefficient of determination is \\(0.6\\). This value is not very close to one. This is expected since miles per gallon can also vary because of the cars weight, and fuel efficiency. It makes sense that the hp only explains \\(60\\)% of the total variation.\n\nIn R we can calculate the coefficient of determination by squaring the correlation coefficient.\n\ncor(mtcars$mpg,mtcars$hp)^2\n\n[1] 0.6024373\n\n\n\n\nExercise 3\n\nIt seems like there is a direct relationship between both variables. The more debt you take, the higher the salary.\n\nStart by loading the data. We’ll use the read.csv() function:\n\nCollege&lt;-read.csv(\"https://jagelves.github.io/Data/College.csv\")\n\nThe two variables of interest are GRAD_DEBT_MDN and MD_EARN_WNE_P10. The following code creates the scatter plot:\n\nThe correlation coefficient shows a moderate direct relationship between earnings and debt \\(0.43\\). The coefficient of determination indicates that only \\(19\\)% of the variation in earnings can be explained by debt.\n\nIn R we can start with the correlation coefficient:\nThe coefficient of determination is:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regression I</span>"
    ]
  },
  {
    "objectID": "RegressionII.html",
    "href": "RegressionII.html",
    "title": "7  Regression II",
    "section": "",
    "text": "7.1 Concepts",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regression II</span>"
    ]
  },
  {
    "objectID": "RegressionII.html#concepts",
    "href": "RegressionII.html#concepts",
    "title": "7  Regression II",
    "section": "",
    "text": "The Regression Line\nThe regression line is fitted so that the average distance between the line and the sample points is as small as possible. The line is defined by a slope (\\(\\beta\\)) and an intercept (\\(\\alpha\\)). Mathematically, the regression line is expressed as \\(\\hat{y_i}=\\hat{\\alpha}+\\hat{\\beta}x_i\\), where \\(\\hat{y_i}\\) are the predicted values of \\(y\\) given the \\(x\\)’s.\n\nThe slope determines the steepness of the line. The estimate quantifies how much a unit increase in \\(x\\) changes \\(y\\). The estimate is given by \\(\\hat{\\beta}= \\frac {s_{xy}}{s_{x}^2}\\).\nThe intercept determines where the line crosses the \\(y\\) axis. It returns the value of \\(y\\) when \\(x\\) is zero. The estimate is given by \\(\\hat{\\alpha}=\\bar{y}-\\hat{\\beta}\\bar{x}\\).\n\n\n\nGoodness of Fit\nThere are a couple of popular measures that determine the goodness of fit of the regression line.\n\nThe coefficient of determination or \\(R^2\\) is the percent of the variation in \\(y\\) that is explained by changes in \\(x\\). The higher the \\(R^2\\) the better the explanatory power of the model. The \\(R^2\\) is always between [0,1]. To calculate use \\(R^2=SSR/SST\\).\n\n\\(SSR\\) (Sum of Squares due to Regression) is the part of the variation in \\(y\\) explained by the model. Mathematically, \\(SSR=\\sum{(\\hat{y_i}-\\bar{y})^2}\\).\n\\(SSE\\) (Sum of Squares due to Error) is the part of the variation in \\(y\\) that is unexplained by the model. Mathematically, \\(SSE=\\sum{(y_i-\\hat{y_i})^2}\\).\nSST (Sum of Squares Total) is the total variation of \\(y\\) with respect to the mean. Mathematically, \\(SST=\\sum{(y_i-\\bar{y})^2}\\).\nNote that \\(SST=SSR+SSE\\).\n\nThe adjusted \\(R^2\\) recognizes that the \\(R^2\\) is a non-decreasing function of the number of explanatory variables in the model. This metric penalizes a model with more explanatory variables relative to a simpler model. It is calculated by \\(1-(1-R^2) \\frac {n-1}{n-k-1}\\), where \\(k\\) is the number of explanatory variables used in the model and \\(n\\) is the sample size.\nThe Residual Standard Error estimates the average dispersion of the data points around the regression line. It is calculated by \\(s_e =\\sqrt{\\frac{SSE}{n-k-1}}\\).\n\n\n\nUseful R Functions\nThe lm() function to estimates the linear regression model.\nThe predict() function uses the linear model object to predict values. New data is entered as a data frame.\nThe coef() function returns the model’s coefficients.\nThe summary() function returns the model’s coefficients, and goodness of fit measures.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regression II</span>"
    ]
  },
  {
    "objectID": "RegressionII.html#exercises",
    "href": "RegressionII.html#exercises",
    "title": "7  Regression II",
    "section": "7.2 Exercises",
    "text": "7.2 Exercises\nThe following exercises will help you get practice on Regression Line estimation and interpretation. In particular, the exercises work on:\n\nEstimating the slope and intercept.\nCalculating measures of goodness of fit.\nPrediction using the regression line.\n\nAnswers are provided below. Try not to peak until you have a formulated your own answer and double checked your work for any mistakes.\n\nExercise 1\nFor the following exercises, make your calculations by hand and verify results using R functions when possible.\n\nConsider the data below. Calculate the deviations from the mean for each variable and use the results to estimate the regression line. Use R to verify your result. On average by how much does y increase per unit increase of x?\n\n\n\n\nx\n20\n21\n15\n18\n25\n\n\n\n\ny\n17\n19\n12\n13\n22\n\n\n\n\n\nAnswer\n\nThe regression lines is \\(\\hat{y}=-4.93+1.09x\\). For each unit increase in x, y increases on average \\(1.09\\).\nStart by generating the deviations from the mean for each variable. For x the deviations are:\n\nx&lt;-c(20,21,15,18,25)\n(devx&lt;-x-mean(x))\n\n[1]  0.2  1.2 -4.8 -1.8  5.2\n\n\nNext, find the deviations for y:\n\ny&lt;-c(17,19,12,13,22)\n(devy&lt;-y-mean(y))\n\n[1]  0.4  2.4 -4.6 -3.6  5.4\n\n\nFor the slope we need to find the deviation squared of the x’s. This can easily be done in R:\n\n(devx2&lt;-devx^2)\n\n[1]  0.04  1.44 23.04  3.24 27.04\n\n\nThe slope is calculated by \\(\\frac{\\sum_{i=i}^{n} (x_{i}-\\bar{x})(y_{i}-\\bar{y})}{\\sum_{i=i}^{n} (x_{i}-\\bar{x})^2}\\). In R we can just find the ratio between the summations of (devx)(devy) and devx2.\n\n(slope&lt;-sum(devx*devy)/sum(devx2))\n\n[1] 1.087591\n\n\nThe intercept is given by \\(\\bar{y}-\\beta(\\bar{x})\\). In R we find that the intercept is equal to:\n\n(intercept&lt;-mean(y)-slope*mean(x))\n\n[1] -4.934307\n\n\nOur results can be easily verified by using the lm() and coef() functions in R.\n\nfitEx1&lt;-lm(y~x)\ncoef(fitEx1)\n\n(Intercept)           x \n  -4.934307    1.087591 \n\n\n\n\nCalculate SST, SSR, and SSE. Confirm your results in R. What is the \\(R^2\\)? What is the Standard Error estimate? Is the regression line a good fit for the data?\n\n\n\nAnswer\n\nSST is \\(69.2\\), SSR is \\(64.82\\) and SSE is \\(4.38\\) (note that \\(SSR+SSE=SST\\)). The \\(R^2\\) is just \\(\\frac{SSR}{SST}=0.94\\) and the Standard Error estimate is \\(1.21\\). They both indicate a great fit of the regression line to the data.\nLet’s start by calculating the SST. This is just \\(\\sum{(y_{i}-\\bar{y})^2}\\).\n\n(SST&lt;-sum((y-mean(y))^2))\n\n[1] 69.2\n\n\nNext, we can calculate SSR. This is calculated by the following formula \\(\\sum{(\\hat{y_{i}}-\\bar{y})^2}\\). To obtain the predicted values in R, we can use the output of the lm() function. Recall our fitEx1 object created in Exercise 1. It has fitted.values included:\n\n(SSR&lt;-sum((fitEx1$fitted.values-mean(y))^2))\n\n[1] 64.82044\n\n\nThe ratio of SSR to SST is the \\(R^2\\):\n\n(R2&lt;-SSR/SST)\n\n[1] 0.9367115\n\n\nFinally, let’s calculate SSE \\(\\sum{(y_{i}-\\hat{y_{i}})^2}\\):\n\n(SSE&lt;-sum((y-fitEx1$fitted.values)^2))\n\n[1] 4.379562\n\n\nWith the SSE we can calculate the Standard Error estimate:\n\nsqrt(SSE/3)\n\n[1] 1.208244\n\n\nWe can confirm these results using the summary() function.\n\nsummary(fitEx1)\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n      1       2       3       4       5 \n 0.1825  1.0949  0.6204 -1.6423 -0.2555 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  -4.9343     3.2766  -1.506  0.22916   \nx             1.0876     0.1632   6.663  0.00689 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.208 on 3 degrees of freedom\nMultiple R-squared:  0.9367,    Adjusted R-squared:  0.9156 \nF-statistic:  44.4 on 1 and 3 DF,  p-value: 0.00689\n\n\n\n\nAssume that x is observed to be 32, what is your prediction of y? How confident are you in this prediction?\n\n\n\nAnswer\n\nIf \\(x=32\\) then \\(\\hat{y}=29.87\\). The regression is a good fit, so we can feel good about our prediction. However, we would be concerned about the sample size of the data.\nIn R we can obtain a prediction by using the predict() function. This function requires a data frame as an input for new data.\n\npredict(fitEx1, newdata = data.frame(x=c(32)))\n\n       1 \n29.86861 \n\n\n\n\n\nExercise 2\nYou will need the Education data set to answer this question. You can find the data set at https://jagelves.github.io/Data/Education.csv . The data shows the years of education (Education), and annual salary in thousands (Salary) for a sample of \\(100\\) people.\n\nEstimate the regression line using R. By how much does an extra year of education increase the annual salary on average? What is the salary of someone without any education?\n\n\n\nAnswer\n\nAn extra year of education increases the annual salary about \\(5,300\\) dollars (slope). A person that has no education would be expected to earn \\(17,2582\\) dollars (intercept).\nStart by loading the data in R:\n\nlibrary(tidyverse)\nEducation&lt;-read_csv(\"https://jagelves.github.io/Data/Education.csv\")\n\nNext, let’s use the lm() function to estimate the regression line and obtain the coefficients:\n\nfitEducation&lt;-lm(Salary~Education, data = Education)\ncoefficients(fitEducation)\n\n(Intercept)   Education \n  17.258190    5.301149 \n\n\n\n\nConfirm that the regression line is a good fit for the data. What is the estimated salary of a person with \\(16\\) years of education?\n\n\n\nAnswer\n\nThe \\(R^2\\) is \\(0.668\\) and the standard error is \\(21\\). The line is a moderately good fit. If someone has \\(16\\) years of experience, the regression line would predict a salary of \\(102,000\\) dollars.\nLet’s get the \\(R^2\\) and the Standard Error estimate by using the summary() function and fitEx1 object.\n\nsummary(fitEducation)\n\n\nCall:\nlm(formula = Salary ~ Education, data = Education)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-62.177  -9.548   1.988  15.330  45.444 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  17.2582     4.0768   4.233  5.2e-05 ***\nEducation     5.3011     0.3751  14.134  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.98 on 98 degrees of freedom\nMultiple R-squared:  0.6709,    Adjusted R-squared:  0.6675 \nF-statistic: 199.8 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nLastly, let’s use the regression line to predict the salary for someone who has \\(16\\) years of education.\n\npredict(fitEducation, newdata = data.frame(Education=c(16)))\n\n       1 \n102.0766 \n\n\n\n\n\nExercise 3\nYou will need the FoodSpend data set to answer this question. You can find this data set at https://jagelves.github.io/Data/FoodSpend.csv .\n\nOmit any NA’s that the data has. Create a dummy variable that is equal to \\(1\\) if an individual owns a home and \\(0\\) if the individual doesn’t. Find the mean of your dummy variable. What proportion of the sample owns a home?\n\n\n\nAnswer\n\nApproximately, \\(36\\)% of the sample owns a home.\nStart by loading the data into R and removing all NA’s:\n\nSpend&lt;-read_csv(\"https://jagelves.github.io/Data/FoodSpend.csv\")\n\nRows: 80 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): OwnHome\ndbl (1): Food\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nSpend&lt;-na.omit(Spend)\n\nTo create a dummy variable for OwnHome we can use the ifelse() function:\n\nSpend$dummyOH&lt;-ifelse(Spend$OwnHome==\"Yes\",1,0)\n\nThe average of the dummy variable is given by:\n\nmean(Spend$dummyOH)\n\n[1] 0.3625\n\n\n\n\nRun a regression with Food being the dependent variable and your dummy variable as the independent variable. What is the interpretation of the intercept and slope?\n\n\n\nAnswer\n\nThe intercept is the average food expenditure of individuals without homes (\\(6417\\)). The slope, is the difference in food expenditures between individuals that do have homes minus those who don’t. We then conclude that individuals that do have a home spend about \\(-2516\\) less on food than those who don’t have homes.\nTo run the regression use the lm() function:\n\nlm(Food~dummyOH,data=Spend)\n\n\nCall:\nlm(formula = Food ~ dummyOH, data = Spend)\n\nCoefficients:\n(Intercept)      dummyOH  \n       6473        -3418  \n\n\n\n\nNow run a regression with Food being the independent variable and your dummy variable as the dependent variable. What is the interpretation of the intercept and slope? Hint: you might want to plot the scatter diagram and the regression line.\n\n\n\nAnswer\n\nThe scatter plot shows that most of the points for home owners are below \\(6000\\). For non-home owners they are mainly above \\(6000\\). The line can be used to predict the likelihood of owning a home given someones food expenditure. The intercept is above one, but still it gives us the indication that it is likely that low food expenditures are highly predictive of owning a home. The slope tells us how that likelihood changes as the food expenditures increase by 1. In general, the likelihood of owning a home decreases as the food expenditure increases.\nRun the lm() function once again:\n\nfitFood&lt;-lm(dummyOH~Food,data=Spend)\ncoefficients(fitFood)\n\n  (Intercept)          Food \n 1.4320766616 -0.0002043632 \n\n\nFor the scatter plot use the following code:\n\nlibrary(ggthemes)\nSpend %&gt;% ggplot() + \n  geom_point(aes(y=dummyOH,x=Food), \n             col=\"black\", pch=21, bg=\"grey\") +\n  geom_smooth(aes(y=dummyOH,x=Food), method=\"lm\",\n              formula=y~x, se=F) + \n  theme_clean()\n\n\n\n\n\n\n\n\n\n\n\nExercise 4\nYou will need the Population data set to answer this question. You can find this data set at https://jagelves.github.io/Data/Population.csv .\n\nRun a regression of Population on Year. How well does the regression line fit the data?\n\n\n\nAnswer\n\nIf we follow the \\(R^2=0.81\\) the model fits the data very well.\nLet’s load the data from the web:\n\nPopulation&lt;-read_csv(\"https://jagelves.github.io/Data/Population.csv\")\n\nNew names:\nRows: 16492 Columns: 4\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): Country.Name dbl (3): ...1, Year, Population\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\nNow let’s filter the data so that we can focus on the population for Japan.\n\nJapan&lt;-filter(Population,Country.Name==\"Japan\")\n\nNext, we can run the regression of Population against the Year. Let’s also run the summary() function to obtain the fit and the coefficients.\n\nfit&lt;-lm(Population~Year,data=Japan)\nsummary(fit)\n\n\nCall:\nlm(formula = Population ~ Year, data = Japan)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-9583497 -4625571  1214644  4376784  5706004 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -988297581   68811582  -14.36   &lt;2e-16 ***\nYear            555944      34569   16.08   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4871000 on 60 degrees of freedom\nMultiple R-squared:  0.8117,    Adjusted R-squared:  0.8086 \nF-statistic: 258.6 on 1 and 60 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nCreate a prediction for Japan’s population in 2030. What is your prediction?\n\n\n\nAnswer\n\nThe prediction for \\(2030\\) is about \\(140\\) million people.\nLet’s use the predict() function:\n\npredict(fit,newdata=data.frame(Year=c(2030)))\n\n        1 \n140268585 \n\n\n\n\nCreate a scatter diagram and include the regression line. How confident are you of your prediction after looking at the diagram?\n\n\n\nAnswer\n\nAfter looking at the scatter plot, it seems unlikely that the population in Japan will hit \\(140\\) million. Population has been decreasing in Japan!\nUse the plot() and abline() functions to create the figure.\n\nJapan %&gt;% ggplot() +\n  geom_point(aes(y=Population,x=Year), \n             col=\"black\", pch=21, bg=\"grey\") +\n  geom_smooth(aes(y=Population,x=Year), \n              formula=y~x, method=\"lm\", se=F) +\n  theme_clean()",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regression II</span>"
    ]
  },
  {
    "objectID": "RegressionII.html#answers",
    "href": "RegressionII.html#answers",
    "title": "7  Regression II",
    "section": "7.3 Answers",
    "text": "7.3 Answers\n\nExercise 1\n\nThe regression lines is \\(\\hat{y}=-4.93+1.09x\\). For each unit increase in x, y increases on average \\(1.09\\).\n\nStart by generating the deviations from the mean for each variable. For x the deviations are:\n\nx&lt;-c(20,21,15,18,25)\n(devx&lt;-x-mean(x))\n\n[1]  0.2  1.2 -4.8 -1.8  5.2\n\n\nNext, find the deviations for y:\n\ny&lt;-c(17,19,12,13,22)\n(devy&lt;-y-mean(y))\n\n[1]  0.4  2.4 -4.6 -3.6  5.4\n\n\nFor the slope we need to find the deviation squared of the x’s. This can easily be done in R:\n\n(devx2&lt;-devx^2)\n\n[1]  0.04  1.44 23.04  3.24 27.04\n\n\nThe slope is calculated by \\(\\frac{\\sum_{i=i}^{n} (x_{i}-\\bar{x})(y_{i}-\\bar{y})}{\\sum_{i=i}^{n} (x_{i}-\\bar{x})^2}\\). In R we can just find the ratio between the summations of (devx)(devy) and devx2.\n\n(slope&lt;-sum(devx*devy)/sum(devx2))\n\n[1] 1.087591\n\n\nThe intercept is given by \\(\\bar{y}-\\beta(\\bar{x})\\). In R we find that the intercept is equal to:\n\n(intercept&lt;-mean(y)-slope*mean(x))\n\n[1] -4.934307\n\n\nOur results can be easily verified by using the lm() and coef() functions in R.\n\nfitEx1&lt;-lm(y~x)\ncoef(fitEx1)\n\n(Intercept)           x \n  -4.934307    1.087591 \n\n\n\nSST is \\(69.2\\), SSR is \\(64.82\\) and SSE is \\(4.38\\) (note that \\(SSR+SSE=SST\\)). The \\(R^2\\) is just \\(\\frac{SSR}{SST}=0.94\\) and the Standard Error estimate is \\(1.21\\). They both indicate a great fit of the regression line to the data.\n\nLet’s start by calculating the SST. This is just \\(\\sum{(y_{i}-\\bar{y})^2}\\).\n\n(SST&lt;-sum((y-mean(y))^2))\n\n[1] 69.2\n\n\nNext, we can calculate SSR. This is calculated by the following formula \\(\\sum{(\\hat{y_{i}}-\\bar{y})^2}\\). To obtain the predicted values in R, we can use the output of the lm() function. Recall our fitEx1 object created in Exercise 1. It has fitted.values included:\n\n(SSR&lt;-sum((fitEx1$fitted.values-mean(y))^2))\n\n[1] 64.82044\n\n\nThe ratio of SSR to SST is the \\(R^2\\):\n\n(R2&lt;-SSR/SST)\n\n[1] 0.9367115\n\n\nFinally, let’s calculate SSE \\(\\sum{(y_{i}-\\hat{y_{i}})^2}\\):\n\n(SSE&lt;-sum((y-fitEx1$fitted.values)^2))\n\n[1] 4.379562\n\n\nWith the SSE we can calculate the Standard Error estimate:\n\nsqrt(SSE/3)\n\n[1] 1.208244\n\n\nWe can confirm these results using the summary() function.\n\nsummary(fitEx1)\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n      1       2       3       4       5 \n 0.1825  1.0949  0.6204 -1.6423 -0.2555 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  -4.9343     3.2766  -1.506  0.22916   \nx             1.0876     0.1632   6.663  0.00689 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.208 on 3 degrees of freedom\nMultiple R-squared:  0.9367,    Adjusted R-squared:  0.9156 \nF-statistic:  44.4 on 1 and 3 DF,  p-value: 0.00689\n\n\n\nIf \\(x=32\\) then \\(\\hat{y}=29.87\\). The regression is a good fit, so we can feel good about our prediction. However, we would be concerned about the sample size of the data.\n\nIn R we can obtain a prediction by using the predict() function. This function requires a data frame as an input for new data.\n\npredict(fitEx1, newdata = data.frame(x=c(32)))\n\n       1 \n29.86861 \n\n\n\n\nExercise 2\n\nAn extra year of education increases the annual salary about \\(5,300\\) dollars (slope). A person that has no education would be expected to earn \\(17,2582\\) dollars (intercept).\n\nStart by loading the data in R:\n\nEducation&lt;-read.csv(\"https://jagelves.github.io/Data/Education.csv\")\n\nNext, let’s use the lm() function to estimate the regression line and obtain the coefficients:\n\nfitEducation&lt;-lm(Salary~Education, data = Education)\ncoefficients(fitEducation)\n\n(Intercept)   Education \n  17.258190    5.301149 \n\n\n\nThe \\(R^2\\) is \\(0.668\\) and the standard error is \\(21\\). The line is a moderately good fit. If someone has \\(16\\) years of experience, the regression line would predict a salary of \\(102,000\\) dollars.\n\nLet’s get the \\(R^2\\) and the Standard Error estimate by using the summary() function and fitEx1 object.\n\nsummary(fitEducation)\n\n\nCall:\nlm(formula = Salary ~ Education, data = Education)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-62.177  -9.548   1.988  15.330  45.444 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  17.2582     4.0768   4.233  5.2e-05 ***\nEducation     5.3011     0.3751  14.134  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.98 on 98 degrees of freedom\nMultiple R-squared:  0.6709,    Adjusted R-squared:  0.6675 \nF-statistic: 199.8 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nLastly, let’s use the regression line to predict the salary for someone who has \\(16\\) years of education.\n\npredict(fitEducation, newdata = data.frame(Education=c(16)))\n\n       1 \n102.0766 \n\n\n\n\nExercise 3\n\nApproximately, \\(36\\)% of the sample owns a home.\n\nStart by loading the data into R and removing all NA’s:\n\nSpend&lt;-read.csv(\"https://jagelves.github.io/Data/FoodSpend.csv\")\nSpend&lt;-na.omit(Spend)\n\nTo create a dummy variable for OwnHome we can use the ifelse() function:\n\nSpend$dummyOH&lt;-ifelse(Spend$OwnHome==\"Yes\",1,0)\n\nThe average of the dummy variable is given by:\n\nmean(Spend$dummyOH)\n\n[1] 0.3625\n\n\n\nThe intercept is the average food expenditure of individuals without homes (\\(6417\\)). The slope, is the difference in food expenditures between individuals that do have homes minus those who don’t. We then conclude that individuals that do have a home spend about \\(-2516\\) less on food than those who don’t have homes.\n\nTo run the regression use the lm() function:\n\nlm(Food~dummyOH,data=Spend)\n\n\nCall:\nlm(formula = Food ~ dummyOH, data = Spend)\n\nCoefficients:\n(Intercept)      dummyOH  \n       6473        -3418  \n\n\n\nThe scatter plot shows that most of the points for home owners are below \\(6000\\). For non-home owners they are mainly above \\(6000\\). The line can be used to predict the likelihood of owning a home given someones food expenditure. The intercept is above one, but still it gives us the indication that it is likely that low food expenditures are highly predictive of owning a home. The slope tells us how that likelihood changes as the food expenditures increase by 1. In general, the likelihood of owning a home decreases as the food expenditure increases.\n\nRun the lm() function once again:\n\nfitFood&lt;-lm(dummyOH~Food,data=Spend)\ncoefficients(fitFood)\n\n  (Intercept)          Food \n 1.4320766616 -0.0002043632 \n\n\nFor the scatter plot use the following code:\n\nplot(y=Spend$dummyOH,x=Spend$Food, \n     main=\"\", axes=F, pch=21, bg=\"blue\",\n     xlab=\"Food\",ylab=\"Dummy\")\naxis(side=1, labels=TRUE, font=1,las=1)\naxis(side=2, labels=TRUE, font=1,las=1)\nabline(fitFood,\n       col=\"gray\",lwd=2)\n\n\n\n\n\n\n\n\n\n\nExercise 4\n\nIf we follow the \\(R^2=0.81\\) the model fits the data very well.\n\nLet’s load the data from the web:\n\nPopulation&lt;-read.csv(\"https://jagelves.github.io/Data/Population.csv\")\n\nNow let’s filter the data so that we can focus on the population for Japan.\n\nlibrary(dplyr)\nJapan&lt;-filter(Population,Country.Name==\"Japan\")\n\nNext, we can run the regression of Population against the Year. Let’s also run the summary() function to obtain the fit and the coefficients.\n\nfit&lt;-lm(Population~Year,data=Japan)\nsummary(fit)\n\n\nCall:\nlm(formula = Population ~ Year, data = Japan)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-9583497 -4625571  1214644  4376784  5706004 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -988297581   68811582  -14.36   &lt;2e-16 ***\nYear            555944      34569   16.08   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4871000 on 60 degrees of freedom\nMultiple R-squared:  0.8117,    Adjusted R-squared:  0.8086 \nF-statistic: 258.6 on 1 and 60 DF,  p-value: &lt; 2.2e-16\n\n\n\nThe prediction for \\(2030\\) is about \\(140\\) million people.\n\nLet’s use the predict() function:\n\npredict(fit,newdata=data.frame(Year=c(2030)))\n\n        1 \n140268585 \n\n\n\nAfter looking at the scatter plot, it seems unlikely that the population in Japan will hit \\(140\\) million. Population has been decreasing in Japan!\n\nUse the plot() and abline() functions to create the figure.\n\nplot(y=Japan$Population,x=Japan$Year, main=\"\", \n     axes=F,pch=21, bg=\"#A7C7E7\",\n     xlab=\"Year\",\n     ylab=\"Population\")\naxis(side=1, labels=TRUE, font=1,las=1)\naxis(side=2, labels=TRUE, font=1,las=1)\nabline(fit,\n       col=\"darkgray\",lwd=2)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regression II</span>"
    ]
  },
  {
    "objectID": "ProbabilityI.html",
    "href": "ProbabilityI.html",
    "title": "8  Probability I",
    "section": "",
    "text": "8.1 Concepts",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability I</span>"
    ]
  },
  {
    "objectID": "ProbabilityI.html#concepts",
    "href": "ProbabilityI.html#concepts",
    "title": "8  Probability I",
    "section": "",
    "text": "Frequentist Vs. Bayesian\nThe frequentist interpretation assumes that probabilities represent proportions of specific events occurring over infinitely identical trials.\nThe Bayesian interpretation assumes that probabilities are subjective beliefs about the relative likelihood of events.\n\n\nExperiments and Sets\nAn experiment is a process that leads to one of several outcomes. Ex: Tossing a Die, Tossing a Coin, Drawing a Card, etc.\nAn outcome is the result of an experiment. Ex: A coin landing on heads, drawing the ace of spades.\nThe sample space \\((S)\\) of an experiment contains all possible outcomes of the experiment. Ex: \\(S\\)={\\(1\\),\\(2\\),\\(3\\),\\(4\\),\\(5\\),\\(6\\)} is the sample space for tossing a die.\nAn event is a subset of the sample space. \\(A\\)={\\(2\\),\\(4\\),\\(6\\)} is the event of tossing an even number when rolling a die.\n\n\nBasic Probability Concepts\nA probability is a numerical value that measures the likelihood that an event occurs.\nTo calculate probabilities, find the ratio between favorable outcomes and total outcomes. \\(p=favorable/total\\).\n\nThe probability of any event \\(A\\) is a value between \\(0\\) and \\(1\\) inclusive. Formally, \\(0\\leq P(A) \\leq1\\).\nWhen the probability of the event is \\(0\\) then the event is impossible. When the probability is \\(1\\) then the event is certain.\nThe sum of the probabilities of a list of mutually exclusive and exhaustive events equals \\(1\\). Formally, \\(\\sum P(x_i)=1\\).\n\nMutually exclusive events do not share any common outcomes. The occurrence of one event precludes the occurrence of others.\nExhaustive events include all outcomes in the sample space.\n\n\nTo assign probabilities you can use the Empirical, Classical, or Subjective Methods.\n\nEmpirical: calculated as a relative frequency of occurrence.\nClassical: based on logical analysis.\nSubjective: calculated by drawing on personal and subjective judgement.\n\n\n\nProbability Rules\nThe Complement Rule: \\(P(A^c)=1-P(A)\\), where \\(A^c\\) is the complement of \\(A\\).\nThe Addition Rule: \\(P(A \\cup B)=P(A)+P(B)-P(A \\cap B)\\), where \\(\\cap\\) is intersection and \\(\\cup\\) is union.\nThe Multiplication Rule:\n\nif events are dependent \\(P(A \\cap B)= P(A|B)P(B)\\), where \\(P(A|B)\\) is the conditional probability.\nif events are independent \\(P(A \\cap B)= P(A)P(B)\\).\n\nThe Law of Total Probability: \\(P(A)=P(A|B)P(B)+P(A|B^c)P(B^c)\\).\nBayes’ Theorem: \\(P(A|B)=P(B|A)P(A)/P(B)\\).\n\n\nCounting Rules\nThe Combination function counts the number of ways to choose \\(x\\) objects from a total of \\(n\\) objects. The order in which the \\(x\\) objects are listed does not matter.\n\nIf repetition is not allowed use \\(C_n^x= \\frac{n!}{(n-x)!x!}\\).\nIf repetition is allowed use \\(\\frac{(x+n-1)!}{(n-1)!x!}\\).\n\nThe Permutation function also counts the number of ways to choose \\(x\\) objects from a total of \\(n\\) objects. However, the order in which the \\(x\\) objects are listed does matter.\n\nIf repetition is not allowed use \\(P_n^x= \\frac{n!}{(n-x)!}\\).\nIf repetition is allowed use \\(n^x\\).\n\n\n\nUseful R Functions\nThe table() function can be used to construct frequency distributions.\nThe factorial() function returns the factorial of a number.\nThe gtools package contains the combinations() and permutations() functions used to calculate combinations and permutations. Use the repeats.allowed argument to specify counting with repetition or no repetition. The v argument allows you to specify a vector of elements.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability I</span>"
    ]
  },
  {
    "objectID": "ProbabilityI.html#exercises",
    "href": "ProbabilityI.html#exercises",
    "title": "8  Probability I",
    "section": "8.2 Exercises",
    "text": "8.2 Exercises\nThe following exercises will help you practice some probability concepts and formulas. In particular, the exercises work on:\n\nCalculating simple probabilities.\nApplying probability rules.\nUsing counting rules.\n\nAnswers are provided below. Try not to peak until you have a formulated your own answer and double checked your work for any mistakes.\n\nExercise 1\nFor the following exercises, make your calculations by hand and verify results with a calculator or R.\n\nA sample space \\(S\\) yields five equally likely events, \\(A\\), \\(B\\), \\(C\\), \\(D\\), and \\(E\\). Find \\(P(D)\\), \\(P(B^c)\\), and \\(P(A \\cup C \\cup E)\\).\nConsider the roll of a die. Define \\(A\\) as {1,2,3}, \\(B\\) as {1,2,3,5,6}, \\(C\\) as {4,6}, and \\(D\\) as {4,5,6}. Are the events \\(A\\) and \\(B\\) mutually exclusive, exhaustive, both or none? What about events \\(A\\) and \\(D\\)?\nA recent study suggests that \\(33.1\\)% of the adult U.S. population is overweight and \\(35.7\\)% obese. What is the probability that a randomly selected adult in the U.S. is either obese or overweight? What is the probability that their weight is normal? Are the events mutually exclusive and exhaustive?\n\n\n\nExercise 2\nFor the following exercises, make your calculations by hand and verify results with a calculator or R.\n\nLet \\(P(A)=0.65\\), \\(P(B)=0.3\\), and \\(P(A|B)=0.45\\). Calculate \\(P(A \\cap B)\\), \\(P(A \\cup B)\\), and \\(P(B|A)\\).\nLet \\(P(A)=0.4\\), \\(P(B)=0.5\\), and \\(P(A^c \\cap B^c)=0.24\\). Calculate \\(P(A^c|B^c)\\), \\(P(A^c \\cup B^c)\\), and \\(P(A \\cup B)\\).\nStock \\(A\\) will rise in price with a probability of \\(0.4\\), stock \\(B\\) will rise with a probability of \\(0.6\\). If stock \\(B\\) rises in price, then \\(A\\) will also rise with a probability of \\(0.5\\). What is the probability that at least one of the stocks will rise in price? Prove that events \\(A\\) and \\(B\\) are (are not) mutually exclusive (independent).\n\n\n\nExercise 3\n\nCreate a joint probability table from the contingency table below. Find \\(P(A)\\), \\(P(A \\cap B)\\), \\(P(A|B)\\), and \\(P(B|A^c)\\). Determine whether the events are independent or mutually exclusive.\n\n\n\n\n\n\\(B\\)\n\\(B^c\\)\n\n\n\\(A\\)\n26\n34\n\n\n\\(A^c\\)\n14\n26\n\n\n\n\n\nExercise 4\nYou will need the Crash data set and R to answer this question. The data shows information on several car crashes. Specifically, if the crash was Head-On or Not Head-On and whether there was Daylight or No Daylight. You can find the data here: https://jagelves.github.io/Data/Crash.csv\n\nCreate a contingency table.\nFind the probability that a) a car crash is Head-On, b) a car crash is in daylight c) a car crash is Head-On given that there is daylight.\nShow that Crashes and Light are dependent.\n\n\n\nExercise 5\n\nUse Bayes’ Theorem in the following question. Let \\(P(A)=0.7\\), \\(P(B|A)=0.55\\), and \\(P(B|A^c)=0.10\\). Find \\(P(A^c)\\), \\(P(A \\cap B)\\), \\(P(A^c \\cap B)\\), \\(P(B)\\), and \\(P(A|B)\\).\nSome find tutors helpful when taking a course. Julia has a 40% chance to fail a course if she does not have a tutor. With a tutor, the probability of failing is only 10%. There is a 50% chance that Julia finds an available tutor. What is the probability that Julia will fail the course? If she ends up failing the course, what is the probability that she had a tutor?\n\n\n\nExercise 6\n\nCalculate the following values and verify your results using R. a) 3!, b) 4!, c) \\(C_6^8\\), d) \\(P_6^8\\).\nThere are 10 players in a local basketball team. If we chose 5 players to randomly start a game, in how many ways can we select the five players if order doesn’t matter? What if order matters?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability I</span>"
    ]
  },
  {
    "objectID": "ProbabilityI.html#answers",
    "href": "ProbabilityI.html#answers",
    "title": "8  Probability I",
    "section": "8.3 Answers",
    "text": "8.3 Answers\n\nExercise 1\n\n\\(P(D)=1/5=0.2\\) since all events are equally likely. \\(P(B^c)=4/5=0.8\\), and \\(P(A \\cup C \\cup E)=P(A + C + E)=3/5=0.6\\).\nEvents \\(A\\) and \\(B\\) are not mutually exclusive since they share some of the same elements. They are not exhaustive since the union of both doesn’t create the sample space.\nThe probability is \\(68.8\\)%. The events are mutually exclusive. If someone is classified as obese, the person is not classified again as overweight. The events are not exhaustive since there are people in the U.S. that have a normal weight. The probability that the person drawn has normal weight is \\(31.2\\)%.\n\n\n\nExercise 2\n\nFrom the multiplication rule, \\(P(A|B)*P(B)=P(A \\cap B)\\).\nSubstituting values yields, \\(P(A \\cap B)=0.45*0.3=0.135\\).\nFrom the addition rule, \\(P(A \\cup B)=P(A)+P(B)-P(A \\cap B)\\).\nSubstituting yields, \\(P(A \\cup B)=0.65+0.3-0.135=0.815\\).\nFrom the multiplication rule once again, \\(P(B|A)=\\frac{P(A \\cap B)}{P(A)}\\). Substituting yields, \\(P(B|A)=0.135/0.65=0.2076923\\).\nFrom the complement rule we have that \\(P(A^c)=0.6\\) and \\(P(B^c)=0.5\\).\nUsing the multiplication rule, \\(P(A^c|B^c)=\\frac{P(A^c \\cap B^c)}{P(B^c)}\\). Substituting yields \\(P(A^c|B^c)=0.24/0.5=0.48\\).\nFrom the addition rule \\(P(A^c \\cup B^c)=P(A^c)+P(B^c)-P(A^c \\cap B^c)\\).\nSubstituting yields \\(P(A^c \\cup B^c)=0.6+0.5-0.24=0.86\\).\nThe event that has no elements of \\(A\\) or \\(B\\) is given by \\(P(A^c \\cap B^c)\\). Therefore \\(P(A \\cup B)=1-0.24=0.76\\) has all the elements of A and B.\nIn short the problem states \\(P(A)=0.4\\), \\(P(B)=0.6\\), and \\(P(A|B)=0.5\\). Where \\(A\\) and \\(B\\) are events of stocks rising in price. The question asks for \\(P(A \\cup B)=P(A)+P(B)-P(A \\cap B)\\).\nUsing the multiplication rule \\(P(A \\cap B)=0.5*0.6=0.3\\).\nHence, \\(P(A \\cup B)=0.4+0.6-0.3=0.7\\).\nThe events are not mutually exclusive since \\(P(A \\cap B)=0.3 \\neq 0\\).\nThe events are also not independent since \\(P(A|B)=0.5 \\neq 0.4=P(A)\\).\n\n\n\nExercise 3\n\nBelow is the joint probability table. The \\(P(A)=0.26+0.34=0.6\\), \\(P(A \\cap B)=0.26\\), \\(P(A|B)=0.26/0.4=0.65\\), and \\(P(B|A^c)=0.14/0.4=0.35\\). Events \\(A\\) and \\(B\\) are not independent since \\(P(A) \\neq P(A|B)\\). The events are not mutually exclusive since \\(P(A \\cap B)=0.26 \\neq 0\\).\n\n\n\n\n\n\\(B\\)\n\\(B^c\\)\nTotal\n\n\n\\(A\\)\n0.26\n0.34\n0.6\n\n\n\\(A^c\\)\n0.14\n0.26\n0.4\n\n\nTotal\n0.4\n0.6\n1\n\n\n\n\n\nExercise 4\n\nThe probability of a Head-On crash is \\((166+108)/4858=0.056\\). The probability of a daylight crash is \\((166+3258)/4858=0.70\\). The probability that the car crash is Head-On given daylight is \\(166/(166+3258)=0.048\\).\n\nStart by loading the data into R.\n\nCrash&lt;-read.csv(\"https://jagelves.github.io/Data/Crash.csv\")\n\nTo create a contingency table use the table() command in R.\n\n(freq&lt;-table(Crash$Crash.Type,Crash$Light.Condition))\n\n             \n              Daylight Not Daylight\n  Head-on          166          108\n  Not Head-On     3258         1326\n\n\nThis table is used to calculate probabilities. We can pass it through the prop.table() function to get the contingency table.\n\nround(prop.table(freq),2)\n\n             \n              Daylight Not Daylight\n  Head-on         0.03         0.02\n  Not Head-On     0.67         0.27\n\n\n\nThe two variables are dependent since \\(P(Head-On|Daylight) \\neq P(Head-On)\\), that is \\(0.048 \\neq 0.56\\).\n\n\n\nExercise 5\n\n\\(P(A^c)=1-P(A)=1-0.7=0.3\\), \\(P(A \\cap B)=𝑃(𝐵|𝐴)𝑃(𝐴) = 0.55(0.70) = 0.385\\), \\(P(A^c \\cap B)=𝑃(B|A^c)𝑃(A^c) = 0.10(0.30) = 0.03\\), \\(P(B)= 𝑃(A \\cap B) + 𝑃(𝐴^c \\cap 𝐵) = 0.385 + 0.03 = 0.415\\), and \\(P(A|B)= \\frac{𝑃(A \\cap B)}{P(B)}=0.385/0.415=0.9277\\).\nLet the event of failing be \\(F\\), the event of not failing be \\(NF\\), the event of having a tutor be \\(T\\), and the event of not having a tutor be \\(NT\\). The probability of failing the course is \\(0.25\\). \\((𝐹) = 𝑃(𝐹 \\cap 𝑇) + 𝑃(𝐹 \\cap 𝑇^c) = 𝑃(𝐹|𝑇)𝑃(𝑇) + 𝑃(𝐹|𝑇^c)𝑃(𝑇^c) = 0.10(0.50) + 0.40(0.50) = 0.05 + 0.20 = 0.25\\) The probability of not having a tutor, given that she failed the course is \\(0.2\\). \\(P(𝑇|𝐹) = \\frac{𝑃(𝐹\\cap𝑇)}{𝑃(𝐹\\cap𝑇)+𝑃(𝐹 \\cap𝑇^c)}= 0.05/0.25 = 0.20\\)\n\n\n\nExercise 6\n\n\\(3!=3 \\times 2 \\times 1=6\\), \\(4!=6 \\times 4=24\\), \\(C_6^8=28\\), and \\(P_6^8=20,160\\)\n\nIn R we can just use the factorial command. So \\(3!\\) is:\n\nfactorial(3)\n\n[1] 6\n\n\nand \\(4!\\) is:\n\nfactorial(4)\n\n[1] 24\n\n\nFor combinations and permutations we can use the gtools package:\n\nlibrary(gtools)\nC&lt;-combinations(8,6)\nnrow(C)\n\n[1] 28\n\n\n\nIf order doesn’t matter, there are \\(252\\) ways. If order matters, then there are \\(30,240\\) ways.\n\nIn R we can once more use the combination and permutation functions:\n\nB1&lt;-combinations(10,5)\nnrow(B1)\n\n[1] 252\n\n\n\nB2&lt;-permutations(10,5)\nnrow(B2)\n\n[1] 30240",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability I</span>"
    ]
  },
  {
    "objectID": "ProbabilityII.html",
    "href": "ProbabilityII.html",
    "title": "9  Probability II",
    "section": "",
    "text": "9.1 Concepts",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability II</span>"
    ]
  },
  {
    "objectID": "ProbabilityII.html#concepts",
    "href": "ProbabilityII.html#concepts",
    "title": "9  Probability II",
    "section": "",
    "text": "Random Variables\nA random variable associates a numerical value with each possible experimental outcome. Specifically, the random variable takes on a value with some probability.\nA random variable is fully characterized by its probability density function (PDF) if continuous or the probability mass function (PMF) if discrete.\n\n\nExpected Value and Variance\nWhen summarizing a random variable, we are mostly interested in the variable’s central tendency (Expected Value) and dispersion (Variance).\nThe expected value (mean) is a measure of central location. For a discrete random variable it is given by \\(E(x)=\\mu=\\sum xf(x)\\), where \\(f(x)\\) is the probability mass function. For a continuous random variable it is given by \\(E(x)= \\int_{-\\infty}^{\\infty} x f(x) dx\\), where \\(f(x)\\) is the probability density function.\nThe variance summarizes the deviation of the values of the random variable from the mean. It is calculated by \\(var(x)=E[(x-E(x))^2]=E[x^2]-E[x]^2\\). Note that this formula can be used for both discrete and continuous random variables.\n\n\nDiscrete Uniform Distribution\nThe discrete uniform distribution is a probability distribution that assigns equal probability to each outcome in a finite set of possible outcomes. In other words, each outcome in the set is equally likely to occur.\nThe probability mass function is given by \\(f(x)=1/n\\), where \\(n\\) is the number of elements in the sample space (all possible outcomes).\nThe expected value is given by \\(E(x)=\\frac {\\sum x_i}{n}\\), where \\(x_i\\) are the possible values, and \\(n\\) is the number of possible values.\nThe variance is given by \\(var(x)=\\frac {\\sum (x_i-E(x))^2}{n-1}\\).\n\n\nBinomial Distribution\nThe binomial distribution is a probability distribution that describes the outcome of a sequence of \\(n\\) independent Bernoulli trials. In a Bernoulli trial, there are only two possible outcomes: “success” and “failure”. The probability of success is denoted by \\(p\\), and the probability of failure is denoted by \\(q = 1 - p\\). In a sequence of \\(n\\) independent Bernoulli trials, the number of successes (\\(x\\)) is a random variable that follows a binomial distribution.\nThe probability mass function is given by \\(f(x)=C_x^n (p^x)(1-p)^{n-x}\\), where \\(n\\) is the number of trials, \\(x\\) is the number of successes, \\(p\\) is the probability of success, and \\(C_x^n\\) is the number of ways there can be \\(x\\) successes in \\(n\\) trials.\nThe expected value of the binomial distribution is \\(E(x)=np\\).\nThe variance of the binomial distribution is \\(var(x)=np(1-p)\\).\n\n\nThe Hypergeometric Distribution\nThe hypergeometric distribution is a probability distribution that describes the outcome of drawing a sample from a population without replacement. It is used to calculate the probability of drawing a certain number of successes (\\(x\\)) in a sample of a given size (\\(n\\)), where the success or failure of each individual draw is not dependent on the success or failure of other draws.\nThe hypergeometric experiment differs from the binomial since:\n\ntrials are not independent.\nthe probability of success changes from trial to trial.\n\nThe probability mass function is given by \\(f(x)=\\frac {C_x^r C_{n-x}^{N-r}}{C_n^N}\\), where \\(n\\) is the number of trials, \\(x\\) is the number of successes, \\(r\\) is the number of elements in the population labeled as success, and \\(N\\) is the number of elements in the population.\nThe expected value of the hypergeometric distribution is \\(E(x)=n \\frac {r}{N}\\).\nThe variance of the hypergeometric distribution is \\(var(x)= n \\frac {r}{N} (1- \\frac {r}{N}) (\\frac {N-n}{N-1})\\).\n\n\nPoisson Distribution\nThe Poisson distribution estimates the number of successes (\\(x\\)) over a specified interval of time or space.\nThe probability mass function is given by \\(f(x)= \\frac {\\mu e^{-x}}{x!}\\), where \\(\\mu\\) is the expected number of successes in any given interval and also the variance, and \\(e\\) is Euler’s number (2.71828…).\nAn experiment satisfies a Poisson process if:\n\nThe number of successes with a specified time or space interval equals any integer between zero and infinity.\nThe number of successes counted in non-overlapping intervals are independent.\nThe probability of success in any interval is the same for all intervals of equal size and is proportional to the size of the interval.\n\n\n\nUseful R Functions\nTo calculate probabilities based on discrete random variables use the pbinom(), phyper(), and ppois() functions. For the uniform distribution use the extraDistr package and the pdunif() function.\nTo calculate cumulative probabilities use the dbinom(), dhyper(), dpois(), and ddunif() functions.\nTo calculate quantiles use the qbinom(), qhyper(), qpois(), and qdunif() functions.\nTo generate random numbers use the rbinom(), rhyper(), rpois(), and rdunif() functions.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability II</span>"
    ]
  },
  {
    "objectID": "ProbabilityII.html#exercises",
    "href": "ProbabilityII.html#exercises",
    "title": "9  Probability II",
    "section": "9.2 Exercises",
    "text": "9.2 Exercises\nThe following exercises will help you practice some probability concepts and formulas. In particular, the exercises work on:\n\nCalculating probabilities for discrete random variables.\nCalculating the expected value and standard deviation.\nApplying the binomial, Poisson and hypergeometric probability distributions.\n\nAnswers are provided below. Try not to peak until you have a formulated your own answer and double checked your work for any mistakes.\n\nExercise 1\nFor the following exercises, make your calculations by hand and verify results with a calculator or R.\n\nConsider the table below. Calculate the mean and standard deviation. What is the probability that \\(x&lt;15\\)?\n\n\n\n\n\\(x\\)\n5\n10\n15\n20\n\n\n\\(P(X=x)\\)\n0.35\n0.3\n0.2\n0.15\n\n\n\n\nConsider the table below. Calculate the mean and standard deviation. What is the probability that \\(x\\geq-9\\)?\n\n\n\n\n\\(y\\)\n-23\n-17\n-9\n-3\n\n\n\\(P(Y=y)\\)\n0.5\n0.25\n0.15\n0.1\n\n\n\n\nThe returns on a couple of funds depends on the state of the economy. The economy is expected to be Good with a probability of 20%, Fair with probability of 50% and Poor with probability of 30%. Which fund would you choose if you want to maximize your return? What would you choose if you really dislike risk?\n\n\n\n\nState of Economy\nFund 1\nFund 2\n\n\n\n\nGood\n20\n40\n\n\nFair\n10\n20\n\n\nPoor\n-10\n-40\n\n\n\n\n\nExercise 2\n\nUse the table below. A portfolio has 200,000 dollars invested in Asset \\(X\\) and 300,000 dollars in asset \\(Y\\). If the correlation coefficient between the two investments is \\(0.4\\), what is the expected return and standard deviation of the portfolio?\n\n\n\n\nMeasure\nX\nY\n\n\n\n\nExpected Return (%)\n8\n12\n\n\nStandard Deviation (%)\n12\n20\n\n\n\n\n\nExercise 3\n\nLet \\(Z\\) be a binomial random variable with \\(n=5\\) and \\(p=0.35\\) use the binomial formula to find \\(P(Z=1)\\), \\(P(Z \\geq 2)\\). What is the expected value and standard deviation of \\(Z\\)?\nLet \\(W\\) be a binomial random variable with \\(n=200\\) and \\(p=0.77\\) use the binomial formula to find \\(P(W&gt;160)\\), \\(P(155 \\leq W \\leq 165)\\). What is the expected value and standard deviation of \\(W\\)?\nSixty percent of a firm’s employees are men. Suppose four of the firm’s employees are randomly selected. What is more likely, finding three men and one woman, or two men and one woman? Does your answer change if the proportion falls to \\(50\\)%?\n\n\n\nExercise 4\n\nAssume that \\(S\\) is a Poisson process with mean of \\(\\mu=1.5\\). Calculate \\(P(S=2)\\) and \\(P(S \\geq 2)\\). What is the mean and standard deviation of \\(S\\)?\nAssume that \\(T\\) is a Poisson process with mean of \\(\\mu=20\\). Calculate \\(P(T=14)\\) and \\(P(18 \\leq T \\leq 23)\\).\nA local pharmacy administers on average \\(84\\) Covid-19 vaccines per week. The vaccines shots are evenly administered across all days. Find the probability that the number of vaccine shots administered on a Wednesday is more than eight but less than \\(12\\).\n\n\n\nExercise 5\n\nAssume that \\(X\\) is a hypergeometric random variable with \\(N=25\\), \\(S=3\\), and \\(n=4\\). Calculate \\(P(X=0)\\), \\(P(X=1)\\), and \\(P(X \\leq 1)\\).\nCompute the probability of at least eight successes in a random sample of \\(20\\) items obtained from a population of \\(100\\) items that contains \\(25\\) successes. What are the expected value and standard deviation of the number of successes?\nFor \\(1\\) dollar a player gets to select six numbers for the base game of Powerball. In the game, five balls are randomly drawn from 59 consecutively numbered white balls. One ball, called the Powerball, is randomly drawn from \\(39\\) consecutively numbered red balls. What is the probability that a player is able to match two out of five randomly drawn white balls? What is the probability of winning the jackpot?",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability II</span>"
    ]
  },
  {
    "objectID": "ProbabilityII.html#answers",
    "href": "ProbabilityII.html#answers",
    "title": "9  Probability II",
    "section": "9.3 Answers",
    "text": "9.3 Answers\n\nExercise 1\n\nThe expected value is \\(10.75\\) and the standard deviation is \\(5.31\\). The probability of \\(x&lt;15\\) is \\(0.65\\).\n\nIn R we can create vectors for both \\(x\\) and the probabilities \\(P(X=x)\\).\n\nx&lt;-c(5,10,15,20)\npx&lt;-c(0.35,0.3,0.2,0.15)\n\nThe expected value is the sum product of probabilities and values. Formally, \\(\\sum_{i=1}^{n}x_{i}p_{i}\\) and in R:\n\n(ex&lt;-sum(x*px))\n\n[1] 10.75\n\n\nThe standard deviation is given by \\(\\sqrt{\\sum_{i=1}^{n}(x_{i}-\\mu)^2p_{i}}\\). We can calculate it in R with the following code:\n\n(sd&lt;-sqrt(sum((x-ex)^2*px)))\n\n[1] 5.30919\n\n\n\nThe expected value is \\(-17.4\\) and the standard deviation is \\(6.86\\). The probability of is \\(0.25\\).\n\nLet’s create the vectors once more in R.\n\ny&lt;-c(-23,-17,-9,-3)\npy&lt;-c(0.5,0.25,0.15,0.1)\n\nThe expected value is given by:\n\n(ey&lt;-sum(y*py))\n\n[1] -17.4\n\n\nThe standard deviation is given by:\n\n(sdy&lt;-sqrt(sum((y-ey)^2*py)))\n\n[1] 6.858571\n\n\n\nBoth funds have the same expected return of \\(6\\). The safest return comes from fund 1 since the standard deviation is only \\(11.14\\) vs. \\(31.05\\) for fund 2.\n\nIn R we can create a data frame with probabilities and the performance of the funds.\n\nfunds&lt;-data.frame(probs=c(0.2,0.5,0.3),fund1=c(20,10,-10), fund2=c(40,20,-40))\n\nLet’s create a function for the expected value and standard deviation. For the expected value:\n\nExpected_Value&lt;-function(x,p){\n  sum(x*p)\n}\n\nNow we can use the formula to calculate the expected value of fund1:\n\nExpected_Value(funds$fund1,funds$probs)\n\n[1] 6\n\n\nand fund 2:\n\nExpected_Value(funds$fund2,funds$probs)\n\n[1] 6\n\n\nFor the standard deviation we can create another function:\n\nStandard_Deviation&lt;-function(x,p){\n  sqrt(sum((x-Expected_Value(x,p))^2*p))\n}\n\nUsing the function to get the standard deviation of fund 1 we get:\n\nStandard_Deviation(funds$fund1,funds$probs)\n\n[1] 11.13553\n\n\nand for fund 2:\n\nStandard_Deviation(funds$fund2,funds$probs)\n\n[1] 31.04835\n\n\n\n\nExercise 2\n\nThe expected return of the portfolio is \\(10.4\\) and the standard deviation is \\(14.60\\).\n\nIn R we can start by calculating the expected return. This is given by the formula \\(\\alpha R_{1} +\\beta R_{2}\\):\n\n(ER&lt;-(2/5)*8+(3/5)*12)\n\n[1] 10.4\n\n\nNext we can find the standard deviation with the formula \\(\\sqrt{\\alpha^2 \\sigma_{1}^2+\\beta^2 \\sigma_{2}+\\alpha \\beta \\rho \\sigma_{1} \\sigma_{2}}\\):\n\n(Risk&lt;-sqrt(0.4^2*12^2 + 0.6^2*20^2+2*0.4*0.6*0.4*12*20))\n\n[1] 14.59863\n\n\n\n\nExercise 3\n\n\\(P(Z=1)=0.31\\), and \\(P(Z \\geq 2)=0.57\\). The expected value is \\(np=1.75\\) and the standard deviation is \\(\\sqrt{np(1-p)}=1.067\\).\n\nLet’s use R and the dbinom() function to find \\(P(Z=1)\\).\n\ndbinom(1,5,0.35)\n\n[1] 0.3123859\n\n\nWe can now use pbinom() to find the cumulative distribution. Since we want the right tale of the distribution, we will specify this with an argument.\n\npbinom(1,5,0.35, lower.tail=F)\n\n[1] 0.571585\n\n\n\n\\(P(W&gt;160)=0.14\\), and \\(P(155 \\leq W \\leq 165 )=0.45\\). The expected value is \\(np=154\\) and the standard deviation is \\(\\sqrt{np(1-p)}=5.95\\).\n\nUsing the pbinom() function we find that \\(P(W&gt;160)\\).\n\npbinom(160,200,0.77, lower.tail = F)\n\n[1] 0.136611\n\n\nWe make two calculations to find the probability. First, \\(P(W \\leq 165)\\) and then \\(P(W \\geq 154)\\). The difference between these two, gives us the desired outcome.\n\npbinom(165,200,0.77, lower.tail=T)-pbinom(154,200,0.77, lower.tail=T)\n\n[1] 0.4487104\n\n\n\nThe probabilities are the same. Each event has a probability of \\(0.3456\\). If the probability changes to \\(0.5\\) now the event of two women and two men is more likely.\n\nLet’s calculate the probabilities in R. First, the probability of three men and one woman.\n\ndbinom(3,4,0.6)\n\n[1] 0.3456\n\n\nNow the probability of two men and two women.\n\ndbinom(2,4,0.6)\n\n[1] 0.3456\n\n\nChanging the probabilities reveals that:\n\ndbinom(3,4,0.5)\n\n[1] 0.25\n\n\n\ndbinom(2,4,0.5)\n\n[1] 0.375\n\n\nHaving two of each is the most likely outcome.\n\n\nExercise 4\n\nThe \\(P(S=2)=0.25\\) and \\(P(S \\geq 2)=0.44\\). The expected value and the variance is \\(1.5\\).\n\nIn R we will make use of the dpois() function:\n\ndpois(2,1.5)\n\n[1] 0.2510214\n\n\nFor the second probability we will use ppois():\n\nppois(1,1.5, lower.tail=F)\n\n[1] 0.4421746\n\n\n\nThe \\(P(T=14)=0.039\\) and \\(P(18 \\leq T \\leq 23)=0.49\\).\n\nUsing the dpois() function once more:\n\ndpois(14,20)\n\n[1] 0.03873664\n\n\nFor the second probability we will find the difference between two probabilities:\n\nppois(23,20, lower.tail=T)-ppois(17,20, lower.tail=T)\n\n[1] 0.4904644\n\n\n\nThe probability of administering more than \\(8\\) but less than \\(12\\) shots is \\(0.3\\).\n\nLet’s first note that if \\(84\\) shots are administered on average weekly, then \\(12\\) are administered daily. Now we can use this average and the ppois() function to find the probability:\n\nppois(11,12)-ppois(8,12)\n\n[1] 0.3065696\n\n\n\n\nExercise 5\n\n\\(P(X=0)=0.58\\), \\(P(X=1)=0.37\\), and \\(P(X \\leq 1)=0.94\\).\n\nIn R we can use the dhyper() function\n\ndhyper(0,3,22,4)\n\n[1] 0.5782609\n\n\nonce more for the second probability:\n\ndhyper(1, 3, 22, 4)\n\n[1] 0.3652174\n\n\nFor the last probability we can add the previous probabilities or use the phyper() function:\n\nphyper(1, 3, 22, 4)\n\n[1] 0.9434783\n\n\n\nThe probability is \\(0.545\\).\n\nIn R we use the dhyper() function once more:\n\ndhyper(0, 2, 10, 3)\n\n[1] 0.5454545\n\n\n\nThe probability of matching two white balls is \\(5%\\). Winning the jackpot is extremely unlikely! A probability of \\(0.00000000512\\). It is more likely to be struck by lightning according to the CDC.\n\nIn R use the dhyper() function:\n\ndhyper(2, 5, 54, 5)\n\n[1] 0.04954472\n\n\nFor the jackpot we first calculate the probability of getting all of the white balls.\n\noptions(digits = 5,scipen=999)\ndhyper(5, 5, 54, 5)\n\n[1] 0.00000019974\n\n\nNow the probability of getting the Powerball.\n\ndhyper(1, 1, 38, 1)\n\n[1] 0.025641\n\n\nSince the two events are independent, we can multiply them to find the probability of a jackpot.\n\ndhyper(5, 5, 54, 5)*dhyper(1, 1, 38, 1)\n\n[1] 0.0000000051217",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability II</span>"
    ]
  },
  {
    "objectID": "ProbabilityIII.html",
    "href": "ProbabilityIII.html",
    "title": "10  Probability III",
    "section": "",
    "text": "10.1 Concepts",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Probability III</span>"
    ]
  },
  {
    "objectID": "ProbabilityIII.html#concepts",
    "href": "ProbabilityIII.html#concepts",
    "title": "10  Probability III",
    "section": "",
    "text": "Continuous Random Variables\nContinuous random variables are characterized by their probability density function \\(f(x)\\). The probability density function does not directly provide probabilities!\nThe probability of a continuous random variable assuming a single value is zero. Instead, probabilities are defined for intervals. These are calculated by areas under the PDF curve (integral).\n\n\nUniform Distribution\nThe uniform probability density function is given by \\(f(x)= \\frac {1}{b-a}\\) when \\(a \\leq x \\leq b\\) and \\(0\\) otherwise.\nThe expected value of the uniform distribution is \\(E(x)= \\frac {a+b}{2}\\).\nThe variance of the uniform distribution is \\(var(x)= \\frac {(b-a)^2} {12}\\)\n\n\nNormal Distribution\nThe normal PDF is given by \\(f(x)= \\frac {1}{\\sigma \\sqrt{2\\pi}} e^{\\frac {-1}{2} (\\frac {x-\\mu}{\\sigma})}\\), where \\(\\mu\\) is the mean, \\(\\sigma\\) is the standard deviation, \\(\\pi\\) is 3.1415… , and \\(e\\) is 2.7282… . The normal distribution has the following properties:\n\nThe normal curve is symmetrical about the mean \\(\\mu\\).\nThe mean is at the middle and divides the area of the distribution into halves.\nThe total area under the curve is equal to 1.\nThe distribution is completely determined by its mean and standard deviation.\n\nThe standard normal distribution has a mean of \\(0\\) and a standard deviation of \\(1\\).\n\n\nExponential Distribution\nThe exponential distribution is useful in computing probabilities for the time it takes to complete a task. It describes the time between events in a Poisson process.\nThe probability density function is given by \\(f(x)=\\frac {1}{\\mu}e^{ \\frac {-x}{\\mu}}\\).\n\n\nTriangular Distribution\nThe triangular distribution is characterized by a single mode (the peak of the distribution) and two boundaries. It is often used in situations where the lower and upper bounds of a potential outcome are known, but the exact likelihood of the outcome is uncertain.\nThe probability density function is given by \\(f(x)=\\frac {2(x-a)}{(b-a)(c-a)}\\) for \\(a \\leq x &lt; c\\); \\(f(x)=\\frac {2}{(b-a)}\\) for \\(x=c\\); \\(f(x)=\\frac {2(b-x)}{(b-a)(b-c)}\\) for \\(c &lt; x \\leq b\\), and \\(f(x)=0\\) otherwise.\nThe expected value of the distribution is \\(E(x)= \\frac {a+b+c}{3}\\).\nThe variance of the triangular distribution is \\(var(x) = \\frac {a^2+b^2+c^2-ab-ac-bc}{18}\\).\n\n\nUseful R Functions\nTo calculate the density of continuous random variables use the dunif(), dnorm(), and dexp() functions. For the triangular distribution use the extraDistr package and the dtriang() function.\nTo calculate probabilities of continuous random variables use the punif(), pnorm(), pexp(), and ptriang() functions.\nTo calculate quartiles of continuous random variables use the qunif(), qnorm(),qexp(), and qtriang() functions.\nTo calculate generate random variables based on continuous random variables use the runif(), rnorm(), rexp(), and rtriang() functions.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Probability III</span>"
    ]
  },
  {
    "objectID": "ProbabilityIII.html#exercises",
    "href": "ProbabilityIII.html#exercises",
    "title": "10  Probability III",
    "section": "10.2 Exercises",
    "text": "10.2 Exercises\nThe following exercises will help you practice some probability concepts and formulas. In particular, the exercises work on:\n\nCalculating probabilities for continuous random variables.\nCalculating the expected value and standard deviation.\nApplying the uniform, normal, and exponential distributions.\n\nAnswers are provided below. Try not to peak until you have a formulated your own answer and double checked your work for any mistakes.\n\nExercise 1\nFor the following exercises, make your calculations by hand and verify results with a calculator or R.\n\nA random variable \\(X\\) follows a continuous uniform distribution with minimum of \\(-2\\) and maximum of \\(4\\). Determine the height of the density function \\(f(x)\\), the mean, the standard deviation, and calculate \\(P(X \\leq -1)\\).\nYour internet provider will arrive sometime between 10:00 am and 12:00 pm. Suppose you have to run a quick errand at 10:00 am. If it takes \\(15\\) minutes to run the errand, what is the probability that you will be back before the internet provider arrives? What if you take \\(30\\) minutes?\n\n\n\nExercise 2\n\nA random variable \\(Z\\) follows a standard normal distribution. Find \\(P(-0.67 \\leq Z \\leq -0.23)\\), \\(P(0 \\leq Z \\leq 1.96)\\), \\(P(-1.28 \\leq Z \\leq 0)\\) and \\(P(Z &gt; 4.2)\\).\nLet \\(Y\\) be normally distributed with \\(\\mu=2.5\\) and \\(\\sigma=2\\). Find \\(P(Y&gt;7.6)\\), \\(P(7.4 \\leq Y \\leq 10.6)\\), a \\(y\\) such that \\(P(Y&gt;y)=0.025\\), and a \\(y\\) such that \\(P(y \\leq Y \\leq 2.5)=0.4943\\).\nAssume that football game times are normally distributed with a mean of \\(3\\) hours and a standard deviation of \\(0.4\\) hour. What is the probability that the game lasts at most \\(2.5\\) hours? Find the maximum value for a game to be in the bottom \\(1\\)% of the distribution.\n\n\n\nExercise 3\n\nRandom variable \\(S\\) is exponentially distributed with mean of \\(0.1\\). What is the standard deviation of \\(S\\)? What is \\(P(0.10 \\leq S \\leq 0.2)\\)?\nA tollbooth operator has observed that cars arrive randomly at a rate of \\(360\\) cars per hour. What is the mean time between car arrivals? What is the probability that the next car will arrive within ten seconds?",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Probability III</span>"
    ]
  },
  {
    "objectID": "ProbabilityIII.html#answers",
    "href": "ProbabilityIII.html#answers",
    "title": "10  Probability III",
    "section": "10.3 Answers",
    "text": "10.3 Answers\n\nExercise 1\n\nThe height of the density function \\(f(x)=0.1667\\), the mean is \\(1\\), standard deviation is \\(1.73\\), and \\(P(X \\leq -1)=0.1667\\).\n\n\\(f(x)\\) can be easily estimated by using the formula of the continuous uniform random variable. \\(f(x)=\\frac{1}{b-a}\\). Using R as a calculator we find:\n\n1/(4-(-2))\n\n[1] 0.1666667\n\n\nThe mean is given by \\(\\mu = \\frac{a+b}{2}\\). In R we determine that the mean is:\n\n(-2+4)/2\n\n[1] 1\n\n\nThe standard deviation is \\(\\sigma = \\sqrt {\\frac{(b-a)^2}{12}}\\). Using R we find:\n\nsqrt((4-(-2))^2/12)\n\n[1] 1.732051\n\n\nFinally, we can find the probability of \\(Z\\) being less than \\(-1\\) by using the punif() function:\n\npunif(-1,-2,4)\n\n[1] 0.1666667\n\n\n\nThe probability that you will arrive on time is \\(0.875\\). If the time of the errand is 30 minutes, then the probability goes down to \\(0.75\\).\n\nThere is a \\(120\\) minute interval in which the IP can arrive. The density function is given by \\(f(x)=1/120\\). Using R we can find \\(P(X&gt;15)\\):\n\npunif(15,0,120,lower.tail=F)\n\n[1] 0.875\n\n\nOnce more we can find \\(P(X&gt;30)\\):\n\npunif(30,0,120,lower.tail=F)\n\n[1] 0.75\n\n\n\n\nExercise 2\n\n\\(P(-0.67 \\leq Z \\leq -0.23)=0.158\\), \\(P(0 \\leq Z \\leq 1.96)=0.475\\), \\(P(-1.28 \\leq Z \\leq 0)=0.4\\) and \\(P(Z &gt; 4.2) \\approx 0\\).\n\nUse the pnorm() function to find the probabilities. \\(P(-0.67 \\leq Z \\leq -0.23)\\):\n\npnorm(-0.23)-pnorm(-0.67)\n\n[1] 0.157617\n\n\n\\(P(0 \\leq Z \\leq 1.96)\\)\n\npnorm(1.96)-pnorm(0)\n\n[1] 0.4750021\n\n\n\\(P(-1.28 \\leq Z \\leq 0)\\)\n\npnorm(0)-pnorm(-1.28)\n\n[1] 0.3997274\n\n\n\\(P(Z &gt; 4.2)\\)\n\noptions(scipen=999)\npnorm(4.2,lower.tail = F)\n\n[1] 0.00001334575\n\n\n\n\\(P(Y&gt;7.6)=0.005386\\), \\(P(7.4 \\leq Y \\leq 10.6)=0.0071\\), a \\(y\\) such that \\(P(Y&gt;y)=0.025\\) is \\(6.42\\), and a \\(y\\) such that \\(P(y \\leq Y \\leq 2.5)\\) is \\(-2.56\\).\n\nLet’s use once more the pnorm() function in R.\n\\(P(Y&gt;7.6)\\)\n\npnorm(7.6,2.5,2,lower.tail = F)\n\n[1] 0.005386146\n\n\n\\(P(7.4 \\leq Y \\leq 10.6)\\)\n\npnorm(10.6,2.5,2)-pnorm(7.4,2.5,2)\n\n[1] 0.007117202\n\n\n\\(y\\) such that \\(P(Y&gt;y)=0.025\\)\n\nqnorm(0.025,2.5,2,lower.tail = F)\n\n[1] 6.419928\n\n\n\\(y\\) such that \\(P(y \\leq Y \\leq 2.5)=0.4943\\). Note that \\(2.5\\) is the mean. Hence we are looking for a \\(y\\) that has \\(0.5-0.4943=0.0057\\) on the left:\n\nqnorm(0.0057,2.5,2)\n\n[1] -2.560385\n\n\n\nThe probability is \\(10.56\\)%. A game lasting no more than \\(2.069\\) hours would be in the bottom \\(1\\)%.\n\nLet’s use pnorm() once more in R.\n\npnorm(2.5,3,0.4)\n\n[1] 0.1056498\n\n\nFor the threshold we can use qnorm()\n\nqnorm(0.01,3,0.4)\n\n[1] 2.069461\n\n\n\n\nExercise 3\n\nThe standard deviation is equal to the mean \\(0.1\\). \\(P(0.10 \\leq S \\leq 0.2)=0.2325\\)\n\nLet’s use pexp() in R:\n\npexp(0.2,rate = 10)-pexp(0.1,rate = 10)\n\n[1] 0.2325442\n\n\n\nThe mean time between car arrivals is \\(1/360=0.002778\\). The probability that the next car will arrive within the next 10 seconds is \\(0.6321\\).\n\nOnce more we use pexp() in R\n\npexp(1/360,360)\n\n[1] 0.6321206",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Probability III</span>"
    ]
  },
  {
    "objectID": "InferenceI.html",
    "href": "InferenceI.html",
    "title": "11  Inference I",
    "section": "",
    "text": "11.1 Concepts",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Inference I</span>"
    ]
  },
  {
    "objectID": "InferenceI.html#concepts",
    "href": "InferenceI.html#concepts",
    "title": "11  Inference I",
    "section": "",
    "text": "Statistical Inference\nThe goal of statistical inference is gain insight on a population parameter by using a sample statistic. It is required that the sample statistic be calculated from a random sample from the population where each element is selected independently.\nA sample mean is used to infer the population mean. Some properties of the sample mean are:\n\nThe expected value of the sample means is equal to the population mean (i.e., the sample mean is unbiased). Formally, \\(E(\\bar x_i) = \\mu\\).\nThe standard deviation of the sample means is lower than the population standard deviation. \\(\\sigma_{\\bar x}= \\sigma/\\sqrt{n}\\). We call this measure the standard error.\nIf the population is normally distributed, then the sample means (\\(\\bar x\\)’s) are normally distributed.\nIf the population is not normally distributed, the the sample means are also normally distributed if the sample size is large (i.e., \\(n&gt;30\\)). This is the central limit theorem.\n\n\n\nProportions\nRecall that the binomial distribution describes the number of successes \\(x\\) in \\(n\\) trials of a Bernoulli process where \\(p\\) is the probability of success. Here, \\(x/n\\) is the proportion of successes.\n\nTo estimate the population proportion use the sample proportion \\(\\bar p = x/n\\). This estimate is unbiased (i.e., \\(E(\\bar p)=P\\)), where \\(P\\) is the population proportion.\nThe standard error of the estimate is \\(se(\\bar P)= \\sqrt { \\frac {p(1-p)}{n}}\\), where \\(p\\) is the sample proportion, and \\(n\\) is the sample size.\nBy the central limit theorem, the sampling distribution of \\(\\bar p\\) is approximately normal when \\(np \\geq 5\\) and \\(n(1-p)\\geq 5\\).\n\n\n\nUseful R Functions\nHere are some functions that are handy when simulating data in R.\nThe pnorm() and punif() functions calculate probabilities for the normal and uniform distributions, respectively.\nThe rnorm() and runif() functions generate random numbers from a normal and uniform distribution, respectively.\nThe for() function creates a loop that repeats a procedure a specified amount of times.\nThe set.seed() function is used to create reproducible results in R when random numbers are used.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Inference I</span>"
    ]
  },
  {
    "objectID": "InferenceI.html#exercises",
    "href": "InferenceI.html#exercises",
    "title": "11  Inference I",
    "section": "11.2 Exercises",
    "text": "11.2 Exercises\nThe following exercises will help you test your knowledge on the Inference. In particular, the exercises work on:\n\nThe Central Limit Theorem.\nSampling Distribution for means.\nSampling Distribution for proportions.\n\nAnswers are provided below. Try not to peak until you have a formulated your own answer and double checked your work for any mistakes.\n\nExercise 1\nIn this exercise we will be simulating the central limit theorem. You will need R to complete this problem.\n\nCreate a random sample of 1000 data points and store it in an object called Population. Use the uniform distribution with min of 100 and max of 200 to generate the sample. Calculate the mean and standard deviation of the random sample and call PopMean and PopSD, respectively.\nCreate a for loop (with 1000 iterations) that takes a sample of 10 points from population, calculate the mean, and then store the result in a vector called SampleMeans. Calculate the mean of the SampleMeans object. How does this mean compare to PopMean? How does the standard deviation compare to PopSD?\nCreate a histogram for the sample means. Is the distribution uniform? Is it normal? What is the probability that the sample mean is between 140 and 160?\n\n\n\nExercise 2\n\nA random sample of \\(n=100\\) is taken from a population with mean \\(\\mu=80\\) and standard deviation \\(\\sigma=14\\). Calculate the expected value and standard error for the sampling distribution of the sampling means. What is the probability that the sample mean falls between \\(77\\) and \\(85\\)?\nAssume that miles-per-gallons of combustion cars are normally distributed with mean of \\(33.8\\) and standard deviation of \\(3.5\\). What is the probability that the mean mpg of four randomly selected cars is more than \\(35\\)? What is the probability that all four selected cars have mpg greater than \\(35\\)?\n\n\n\nExercise 3\n\nA random sample of \\(n=200\\) is taken from a population with a proportion of \\(p=0.75\\). Calculate the expected value and standard error of the proportion sampling distribution. What is the probability that the sample proportion is between \\(0.7\\) and \\(0.8\\)?\n\n2.Twenty-three percent of employees at a fintech firm work from home. If we take a sample of 50 employees, what is the probability that more than 20% of them are working from home? What if the sample increases to 200? Why does the probability change?\n\n\nExercise 4\n\nA production process for energy drinks is being evaluated. The machine that fills the cans is calibrated so that each can has \\(350\\)ml of drink with a standard deviation of \\(10\\)ml. Every hour, ten cans are sampled and the average amount of drink is recorded (see table below). Is the machine working properly?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\n\\(\\bar{x}=310\\)\n\\(\\bar{x}=315\\)\n\\(\\bar{x}=325\\)\n\\(\\bar{x}=330\\)\n\\(\\bar{x}=328\\)\n\\(\\bar{x}=347\\)\n\\(\\bar{x}=339\\)\n\\(\\bar{x}=350\\)\n\n\n\n\nThe production of Good Guy dolls has a \\(1\\)% defective rate. A quality inspector takes five samples of size \\(1000\\). The proportions are shown in the table below. Is the production process under control?\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n\n\n\n\n\\(\\bar{p}=0.009\\)\n\\(\\bar{p}=0.012\\)\n\\(\\bar{p}=0.008\\)\n\\(\\bar{p}=0.011\\)\n\\(\\bar{p}=0.0102\\)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Inference I</span>"
    ]
  },
  {
    "objectID": "InferenceI.html#answers",
    "href": "InferenceI.html#answers",
    "title": "11  Inference I",
    "section": "11.3 Answers",
    "text": "11.3 Answers\n\nExercise 1\nLet’s start by creating the random sample. We can use the runif() function in R to do this. We will set a seed so that results are reproducible.\n\nset.seed(10)\nPopulation&lt;-runif(1000,100,200)\n\nNext, we can save the mean and the standard deviation of the population in two different object:\n\nPopMean&lt;-mean(Population)\nPopSD&lt;-sd(Population)\n\nThe mean and standard deviation are \\(150.53\\) and \\(29.2\\). Let’s quickly create a histogram of population, so that we can convince ourselves that the data is uniformly distributed.\n\nhist(Population, main=\"\", ylim=c(0,160), col=\"#F5F5F5\")\n\n\n\n\n\n\n\n\n\nNow let’s create a for loop that allows us to sample the population several times. In fact, we will sample the population 1000 times and record the mean of the samples.\n\n\nnrep&lt;-1000\nSampleMeans&lt;-c()\nfor (i in 1:nrep){\n  x&lt;-sample(Population,10,replace=T)\n  SampleMeans&lt;-c(SampleMeans,mean(x))\n}\n\nNow we can calculate the mean of the sample means in R:\n\nmean(SampleMeans)\n\n[1] 150.4177\n\n\nNote that the mean is very close to PopMean. In the limit (that is if we take many more samples), these two values are equal to each other. Now let’s calculate the standard deviation of the sample means.\n\nsd(SampleMeans)\n\n[1] 9.134147\n\n\nAs you can see, the standard deviation is much lower. In fact, if we take PopSD and divide by 10 (the size of the sample), we should get close to the standard deviation of the sample means.\n\nPopSD/sqrt(10)\n\n[1] 9.233644\n\n\n\nTo create the histogram we use the hist() function once more:\n\n\nhist(SampleMeans, main=\"\", ylim=c(0,300), col=\"#F5F5F5\")\n\n\n\n\n\n\n\n\nThe distribution looks normal. To be clear, if the population follows a uniform distribution, we have shown that the distribution of the sample means is normal with a mean equal to the population mean and a smaller standard deviation.\nWe can use the distribution of the sample means to calculate the probability. Noting the the distribution is normal:\n\npnorm(160,mean(SampleMeans),sd(SampleMeans))-pnorm(140,mean(SampleMeans),sd(SampleMeans))\n\n[1] 0.7258913\n\n\nThere is a \\(72.59\\)% probability that the sample mean is between \\(140\\) and \\(160\\).\n\n\nExercise 2\n\nThe expected value is \\(80\\) since it is equal to the mean of the population. The standard error is \\(1.4\\). The probability is \\(98.38\\)%.\n\nWe can use R as a calculator to find the standard error.\n\n14/sqrt(100)\n\n[1] 1.4\n\n\nWe can use pnorm() to find the probability:\n\npnorm(85,80,1.4)-pnorm(77,80,1.4)\n\n[1] 0.9837602\n\n\n\nThe probabilities are \\(24.66\\)% and \\(1.8\\)%.\n\nFor the first probability we can use a sample size of \\(4\\) and use the standard error in the pnorm() function.\n\npnorm(35,33.8,3.5/sqrt(4),lower.tail = F)\n\n[1] 0.2464466\n\n\nFor the second probability we can first calculate the probability that a randomly selected car has mpg greater than \\(35\\). In R:\n\n(p35&lt;-pnorm(35,33.8,3.5,lower.tail = F))\n\n[1] 0.365853\n\n\nSince draws are independent we get:\n\np35^4\n\n[1] 0.01791539\n\n\n\n\nExercise 3\n\nThe expected value is \\(0.75\\), the same as the population. The standard error is \\(\\sqrt{p(1-p)/n}=0.03\\). The probability for a sample of \\(200\\) is \\(0.8975\\).\n\nThe standard error is given by:\n\nsqrt(0.75*0.25/200)\n\n[1] 0.03061862\n\n\nIn R we can use the pnorm() function one more time to find the probability.\n\npnorm(0.8,0.75,sqrt(0.75*0.25/200))-pnorm(0.7,0.75,sqrt(0.75*0.25/200))\n\n[1] 0.8975296\n\n\n\nThe probability with a sample of \\(50\\) is \\(69.29\\)%. When the sample is \\(200\\) the probability is \\(84.33\\)%. As the sample size increases the standard error goes down. This means that the distribution of the sample proportions gets tighter and there is more area to the right of \\(\\bar{p}=0.2\\).\n\nIn R we can use the pnorm() function one more time with a mean of \\(0.2\\) and \\(n=50\\).\n\npnorm(0.2,0.23,sqrt(0.23*0.77/50),lower.tail = F)\n\n[1] 0.6928964\n\n\nUpdating the code so that \\(n=200\\) yields:\n\npnorm(0.2,0.23,sqrt(0.23*0.77/200),lower.tail = F)\n\n[1] 0.8433098\n\n\n\n\nExercise 4\n\nThe process seems to be out of control. In the early samples, the machine is not filling the cans with enough drink. Although, in the later periods the machine reverts back to the expected performance, it seems unlikely that it will remain functioning correctly.\n\nLet’s start by calculating the upper and lower limits in R.\n\ndataEx1&lt;-c(310,315,325,330,328,347,339,350)\nulEx1&lt;-350+3*(10/sqrt(10))\nllEx1&lt;-350-3*(10/sqrt(10))\n\nWe can graph the samples and the limits to determine the stability of the production process.\n\nplot(dataEx1, type=\"b\", ylab=\"Mean Gallons\",\n     xlab=\"Period\", pch=21, bg=\"blue\",ylim=c(280,380))\nabline(h=ulEx1,col=\"red\")\nabline(h=llEx1,col=\"red\")\n\n\n\n\n\n\n\n\n\nGood Dolls production looks good. All proportions fall between three standard errors of the mean.\n\nOnce more we can calculate upper and lower limits for the proportions.\n\ndataEx2&lt;-c(0.009,0.012,0.008,0.011,0.0102)\nulEx2&lt;-0.01+3*sqrt(0.01*0.99/1000)\nllEx2&lt;-0.01-3*sqrt(0.01*0.99/1000)\n\nGraphing the results in R we can observe the production process and the sample proportions.\n\nplot(dataEx2, type=\"b\", ylab=\"Proportion Defective\",\n     xlab=\"Period\", pch=21, bg=\"blue\",ylim=c(0,0.02))\nabline(h=ulEx2,col=\"red\")\nabline(h=llEx2,col=\"red\")",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Inference I</span>"
    ]
  },
  {
    "objectID": "InferenceII.html",
    "href": "InferenceII.html",
    "title": "12  Inference II",
    "section": "",
    "text": "12.1 Concepts",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Inference II</span>"
    ]
  },
  {
    "objectID": "InferenceII.html#concepts",
    "href": "InferenceII.html#concepts",
    "title": "12  Inference II",
    "section": "",
    "text": "Confidence Intervals\nA confidence interval provides a range of values that, with a certain level of confidence, contains the population parameter of interest. For proper confidence intervals ensure that the sampling distributions are normal.\nA \\(95\\)% confidence level, indicates that if the interval were constructed many times (from independent samples of the population), it would include the true population parameter \\(95\\)% of the time.\nA significance level (\\(\\alpha\\)) of \\(5\\)%, means that the confidence interval would would not include the true population parameter \\(5\\)% of the time.\nThe interval for the population mean when the population standard deviation is unknown is given by \\(\\bar x \\pm t_{\\alpha/2, df} \\frac {s}{\\sqrt{n}}\\), where \\(\\bar x\\) is the point estimate, \\(t_{a/2, df} \\frac {s}{\\sqrt{n}}\\) is the margin of error, \\(\\alpha\\) is the allowed probability that the interval does not include \\(\\mu\\), and \\(df\\) are the degrees of freedom \\(n-1\\).\nThe interval for the population proportion mean is given by \\(\\bar p \\pm z_{\\alpha/2} \\sqrt{\\frac {\\bar p (1-\\bar p)}{n}}\\).\n\n\nUseful R Functions\nThe qnorm() and qt() functions calculate quartiles for the normal and \\(t\\) distributions, respectively.\nThe if() function creates a conditional statement in R.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Inference II</span>"
    ]
  },
  {
    "objectID": "InferenceII.html#exercises",
    "href": "InferenceII.html#exercises",
    "title": "12  Inference II",
    "section": "12.2 Exercises",
    "text": "12.2 Exercises\nThe following exercises will help you test your knowledge on Statistical Inference. In particular, the exercises work on:\n\nSimulating confidence intervals.\nEstimating confidence intervals in R.\nEstimating confidence intervals for proportions.\n\nAnswers are provided below. Try not to peak until you have a formulated your own answer and double checked your work for any mistakes.\n\nExercise 1\nIn this exercise you will be simulating confidence intervals.\n\nSet the seed to \\(9\\). Create a random sample of 1000 data points and store it in an object called Population. Use the exponential distribution with rate of \\(0.02\\) to generate the data. Calculate the mean and standard deviation of Population and call them PopMean and PopSD respectively. What are the mean and standard deviation of Population?\nCreate a for loop (with 10,000 iterations) that takes a sample of 50 points from Population, calculates the mean, and then stores the result in a vector called SampleMeans. What is the mean of the SampleMeans?\nCreate a \\(90\\)% confidence interval using the first data point in the SampleMeans vector. Does the confidence interval include PopMean?\nNow take the minimum of the SampleMeans vector. Create a new \\(90\\)% confidence interval. Does the interval include PopMean? Out of the \\(10,000\\) intervals that you could construct with the vector SampleMeans, how many would you expect to include PopMean?\n\n\n\nExercise 2\n\nA random sample of \\(24\\) observations is used to estimate the population mean. The sample mean is \\(104.6\\) and the standard deviation is \\(28.8\\). The population is normally distributed. Construct a \\(90\\)% and \\(95\\)% confidence interval for the population mean. How does the confidence level affect the size of the interval?\nA random sample from a normally distributed population yields a mean of \\(48.68\\) and a standard deviation of \\(33.64\\). Compute a \\(95\\)% confidence interval assuming a) that the sample size is \\(16\\) and b) the sample size is \\(25\\). What happens to the confidence interval as the sample size increases?\n\n\n\nExercise 3\nYou will need the sleep data set for this problem. The data is built into R, and displays the effect of two sleep inducing drugs on students. Calculate a \\(95\\)% confidence interval for group 1 and for group 2. Which drug would you expect to be more effective at increasing sleeping times?\n\n\nExercise 4\n\nA random sample of \\(100\\) observations results in \\(40\\) successes. Construct a \\(90\\)% and \\(95\\)% confidence interval for the population proportion. Can we conclude at either confidence level that the population proportion differs from \\(0.5\\)?\nYou will need the HairEyeColor data set for this problem. The data is built into R, and displays the distribution of hair and eye color for \\(592\\) statistics students. Construct a \\(95\\) confidence interval for the proportion of Hazel eye color students.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Inference II</span>"
    ]
  },
  {
    "objectID": "InferenceII.html#answers",
    "href": "InferenceII.html#answers",
    "title": "12  Inference II",
    "section": "12.3 Answers",
    "text": "12.3 Answers\n\nExercise 1\n\nThe mean of Population is \\(48.61\\). The standard deviation is \\(47.94\\).\n\nStart by generating values from the exponential distribution. You can use the rexp() function in R to do this. Setting the seed to \\(9\\) yields:\n\nset.seed(9)\nPopulation&lt;-rexp(1000,0.02)\n\nThe population mean is:\n\n(PopMean&lt;-mean(Population))\n\n[1] 48.61053\n\n\nThe standard deviation is:\n\n(PopSD&lt;-sd(Population))\n\n[1] 47.94411\n\n\n\nThe mean is very close to the population mean \\(48.83\\). The standard deviation is \\(6.83\\).\n\nIn R you can use a for loop to create the vector of sample means.\n\nnrep&lt;-10000\nSampleMeans&lt;-c()\nfor (i in 1:nrep){\n  x&lt;-sample(Population,50,replace=T)\n  SampleMeans&lt;-c(SampleMeans,mean(x))\n}\n\nThe mean of SampleMeans is:\n\n(xbar&lt;-mean(SampleMeans))\n\n[1] 48.7005\n\n\nThe standard deviation is:\n\n(Standard&lt;-sd(SampleMeans))\n\n[1] 6.827595\n\n\n\nThe confidence interval is [\\(47.71\\),\\(70.17\\)]. Since the population mean is equal to \\(48.61\\), the confidence interval does include the population mean.\n\nLet’s construct the upper an lower limits of the interval in R.\n\n(ll&lt;-SampleMeans[1]+qnorm(0.05)*Standard)\n\n[1] 47.71385\n\n(ul&lt;-SampleMeans[1]-qnorm(0.05)*Standard)\n\n[1] 70.17464\n\n\n\nThe confidence interval is [\\(14.86\\),\\(37.32\\)]. This interval does not include the population mean of \\(48.61\\). Out of the \\(10,000\\) confidence intervals, one would expect about \\(9,000\\) to include the population mean.\n\nLet’s find the confidence interval limits using R.\n\n(Minll&lt;-min(SampleMeans)+qnorm(0.05)*Standard)\n\n[1] 14.85631\n\n(Minul&lt;-min(SampleMeans)-qnorm(0.05)*Standard)\n\n[1] 37.31709\n\n\nWe can confirm in R that about \\(9,000\\) of the intervals include PopMean. Once more, let’s use a for loop to construct confidence intervals for each element in SampleMeans and check whether the PopMean is included. The count variable keeps track of how many intervals include the population mean.\n\ncount=0\n\nfor (i in SampleMeans){\n  (ll&lt;-i+qnorm(0.05)*Standard)\n  (ul&lt;-i-qnorm(0.05)*Standard)\n  if (PopMean&lt;=ul & PopMean&gt;=ll){\n    count=count+1\n  }\n}\n\ncount\n\n[1] 8978\n\n\n\n\nExercise 2\n\nThe \\(90\\)% confidence interval is [\\(94.52\\),\\(114.67\\)] and the \\(95\\)% confidence interval is [\\(114.68\\),\\(116.76\\)]. The larger the confidence level, the larger the interval.\n\nLet’s construct the intervals using R. Since the population standard deviation is unknown we will use the t-distribution. The interval is constructed as \\(\\bar{x}\\pm t_{\\alpha/2} \\frac{s}{\\sqrt{n}}\\).\n\n(ul90&lt;-104.6-qt(0.05,23)*28.8/sqrt(24))\n\n[1] 114.6755\n\n(ll90&lt;-104.6+qt(0.05,23)*28.8/sqrt(24))\n\n[1] 94.52453\n\n\nFor the \\(95\\)% confidence interval we adjust the significance level accordingly.\n\n(ul95&lt;-104.6-qt(0.025,23)*28.8/sqrt(24))\n\n[1] 116.7612\n\n(ll95&lt;-104.6+qt(0.025,23)*28.8/sqrt(24))\n\n[1] 92.43883\n\n\n\nThe confidence interval for a sample size of \\(16\\) is [\\(30.75\\),\\(66.61\\)]. The confidence interval when the sample size is \\(25\\) is [\\(34.79\\),\\(62.57\\)]. As the sample size gets larger, the confidence interval gets narrower and more precise.\n\nLet’s use R again to calculate the confidence interval. For a sample size of \\(16\\) the interval is:\n\n(ul16&lt;-48.68-qt(0.025,15)*33.64/sqrt(16))\n\n[1] 66.60549\n\n(ll16&lt;-48.68+qt(0.025,15)*33.64/sqrt(16))\n\n[1] 30.75451\n\n\nIncreasing the ample size to \\(25\\) yields:\n\n(ul25&lt;-48.68-qt(0.025,24)*33.64/sqrt(25))\n\n[1] 62.56591\n\n(ll25&lt;-48.68+qt(0.025,24)*33.64/sqrt(25))\n\n[1] 34.79409\n\n\n\n\nExercise 3\n\nThe 95% confidence interval for group 1 is [\\(-0.53\\),\\(2.03\\)].\n\nLet’s first calculate the standard error for group 1.\n\n(se1&lt;-sd(sleep$extra[sleep$group==1])/sqrt(length(sleep$extra[sleep$group==1])))\n\n[1] 0.5657345\n\n\nWe can now use the standard error to estimate the lower and upper limits of the confidence interval.\n\n(ll1&lt;-mean(sleep$extra[sleep$group==1])+qt(0.025,9)*se1)\n\n[1] -0.5297804\n\n(ul1&lt;-mean(sleep$extra[sleep$group==1])-qt(0.025,9)*se1)\n\n[1] 2.02978\n\n\n\nThe 95% confidence interval for group 2 is [\\(0.90\\),\\(3.76\\)].\n\nLet’s repeat the procedure for group 2. Start by finding the standard error.\n\n(se2&lt;-sd(sleep$extra[sleep$group==2])/sqrt(length(sleep$extra[sleep$group==2])))\n\n[1] 0.6331666\n\n\nUsing the standard error we can complete the confidence interval.\n\n(ll2&lt;-mean(sleep$extra[sleep$group==2])+qt(0.025,9)*se2)\n\n[1] 0.8976775\n\n(ul2&lt;-mean(sleep$extra[sleep$group==2])-qt(0.025,9)*se2)\n\n[1] 3.762322\n\n\n\nDrug 2. Drug 2 does not include zero in the interval, and the interval is to the right of zero. It is unlikely, that drug 2 has no effect on students sleeping time. Additionally, Drug 2’s mean increase in sleeping hours is \\(2.33\\) vs. \\(0.75\\) for drug 1.\n\n\n\nExercise 4\n\nThe 90% and 95% confidence intervals are [\\(0.319\\),\\(0.481\\)], and [\\(0.304\\),\\(0.496\\)] respectively. Since they do not include 0.5, we can conclude that the population proportion is significantly different from 0.5.\n\nWe can create an object that stores the sample proportion and sample in R:\n\n(p&lt;-0.4)\n\n[1] 0.4\n\n(n&lt;-100)\n\n[1] 100\n\n\nThe \\(90\\)% confidence interval is given by:\n\n(Ex1ll90&lt;-p+qnorm(0.05)*sqrt(p*(1-p)/100))\n\n[1] 0.319419\n\n(Ex1ul90&lt;-p-qnorm(0.05)*sqrt(p*(1-p)/100))\n\n[1] 0.480581\n\n\nThe \\(95\\)% confidence interval is:\n\n(Ex1ll90&lt;-p+qnorm(0.025)*sqrt(p*(1-p)/100))\n\n[1] 0.3039818\n\n(Ex1ul90&lt;-p-qnorm(0.025)*sqrt(p*(1-p)/100))\n\n[1] 0.4960182\n\n\n\nThe \\(90\\)% confidence interval is [\\(0.132\\),\\(0.182\\)].The \\(95\\)% confidence interval is [\\(0.128\\),\\(0.186\\)].\n\nThe data can easily be viewed by calling HairEyeColor in R.\n\nHairEyeColor\n\n, , Sex = Male\n\n       Eye\nHair    Brown Blue Hazel Green\n  Black    32   11    10     3\n  Brown    53   50    25    15\n  Red      10   10     7     7\n  Blond     3   30     5     8\n\n, , Sex = Female\n\n       Eye\nHair    Brown Blue Hazel Green\n  Black    36    9     5     2\n  Brown    66   34    29    14\n  Red      16    7     7     7\n  Blond     4   64     5     8\n\n\nNote that there are three dimensions to this table (Hair, Eye, Sex). We can calculate the proportion of Hazel eye colored students with the following command that makes use of indexing:\n\n(p&lt;-(sum(HairEyeColor[,3,1])+sum(HairEyeColor[,3,2]))/sum(HairEyeColor))\n\n[1] 0.1570946\n\n\nNow we can use this proportion to construct the intervals. Recall that for proportions the interval is calculated by \\(\\bar{p} \\pm z_{\\alpha/2} \\sqrt{\\frac{\\bar{p}(1-\\bar{p})}{n}}\\). The \\(90\\)% confidence interval is given by:\n\n(Ex2ll90&lt;-p+qnorm(0.05)*sqrt(p*(1-p)/592))\n\n[1] 0.1324945\n\n(Ex2ul90&lt;-p-qnorm(0.05)*sqrt(p*(1-p)/592))\n\n[1] 0.1816947\n\n\nThe 95% confidence interval is:\n\n(Ex2ll95&lt;-p+qnorm(0.025)*sqrt(p*(1-p)/592))\n\n[1] 0.1277818\n\n(Ex2ul95&lt;-p-qnorm(0.025)*sqrt(p*(1-p)/592))\n\n[1] 0.1864074",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Inference II</span>"
    ]
  },
  {
    "objectID": "InferenceIII.html",
    "href": "InferenceIII.html",
    "title": "13  Inference II",
    "section": "",
    "text": "13.1 Concepts",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Inference II</span>"
    ]
  },
  {
    "objectID": "InferenceIII.html#concepts",
    "href": "InferenceIII.html#concepts",
    "title": "13  Inference II",
    "section": "",
    "text": "Confidence Intervals\nA confidence interval provides a range of values that, with a certain level of confidence, contains the population parameter of interest. For proper confidence intervals ensure that the sampling distributions are normal.\nA \\(95\\)% confidence level, indicates that if the interval were constructed many times (from independent samples of the population), it would include the true population parameter \\(95\\)% of the time.\nA significance level (\\(\\alpha\\)) of \\(5\\)%, means that the confidence interval would would not include the true population parameter \\(5\\)% of the time.\nThe interval for the population mean when the population standard deviation is unknown is given by \\(\\bar x \\pm t_{\\alpha/2, df} \\frac {s}{\\sqrt{n}}\\), where \\(\\bar x\\) is the point estimate, \\(t_{a/2, df} \\frac {s}{\\sqrt{n}}\\) is the margin of error, \\(\\alpha\\) is the allowed probability that the interval does not include \\(\\mu\\), and \\(df\\) are the degrees of freedom \\(n-1\\).\nThe interval for the population proportion mean is given by \\(\\bar p \\pm z_{\\alpha/2} \\sqrt{\\frac {\\bar p (1-\\bar p)}{n}}\\).\n\n\nUseful R Functions\nThe qnorm() and qt() functions calculate quartiles for the normal and \\(t\\) distributions, respectively.\nThe if() function creates a conditional statement in R.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Inference II</span>"
    ]
  },
  {
    "objectID": "InferenceIII.html#exercises",
    "href": "InferenceIII.html#exercises",
    "title": "13  Inference II",
    "section": "13.2 Exercises",
    "text": "13.2 Exercises\nThe following exercises will help you test your knowledge on Statistical Inference. In particular, the exercises work on:\n\nSimulating confidence intervals.\nEstimating confidence intervals in R.\nEstimating confidence intervals for proportions.\n\nAnswers are provided below. Try not to peak until you have a formulated your own answer and double checked your work for any mistakes.\n\nExercise 1\nIn this exercise you will be simulating confidence intervals.\n\nSet the seed to \\(9\\). Create a random sample of 1000 data points and store it in an object called Population. Use the exponential distribution with rate of \\(0.02\\) to generate the data. Calculate the mean and standard deviation of Population and call them PopMean and PopSD respectively. What are the mean and standard deviation of Population?\nCreate a for loop (with 10,000 iterations) that takes a sample of 50 points from Population, calculates the mean, and then stores the result in a vector called SampleMeans. What is the mean of the SampleMeans?\nCreate a \\(90\\)% confidence interval using the first data point in the SampleMeans vector. Does the confidence interval include PopMean?\nNow take the minimum of the SampleMeans vector. Create a new \\(90\\)% confidence interval. Does the interval include PopMean? Out of the \\(10,000\\) intervals that you could construct with the vector SampleMeans, how many would you expect to include PopMean?\n\n\n\nExercise 2\n\nA random sample of \\(24\\) observations is used to estimate the population mean. The sample mean is \\(104.6\\) and the standard deviation is \\(28.8\\). The population is normally distributed. Construct a \\(90\\)% and \\(95\\)% confidence interval for the population mean. How does the confidence level affect the size of the interval?\nA random sample from a normally distributed population yields a mean of \\(48.68\\) and a standard deviation of \\(33.64\\). Compute a \\(95\\)% confidence interval assuming a) that the sample size is \\(16\\) and b) the sample size is \\(25\\). What happens to the confidence interval as the sample size increases?\n\n\n\nExercise 3\nYou will need the sleep data set for this problem. The data is built into R, and displays the effect of two sleep inducing drugs on students. Calculate a \\(95\\)% confidence interval for group 1 and for group 2. Which drug would you expect to be more effective at increasing sleeping times?\n\n\nExercise 4\n\nA random sample of \\(100\\) observations results in \\(40\\) successes. Construct a \\(90\\)% and \\(95\\)% confidence interval for the population proportion. Can we conclude at either confidence level that the population proportion differs from \\(0.5\\)?\nYou will need the HairEyeColor data set for this problem. The data is built into R, and displays the distribution of hair and eye color for \\(592\\) statistics students. Construct a \\(95\\) confidence interval for the proportion of Hazel eye color students.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Inference II</span>"
    ]
  },
  {
    "objectID": "InferenceIII.html#answers",
    "href": "InferenceIII.html#answers",
    "title": "13  Inference II",
    "section": "13.3 Answers",
    "text": "13.3 Answers\n\nExercise 1\n\nThe mean of Population is \\(48.61\\). The standard deviation is \\(47.94\\).\n\nStart by generating values from the exponential distribution. You can use the rexp() function in R to do this. Setting the seed to \\(9\\) yields:\n\nset.seed(9)\nPopulation&lt;-rexp(1000,0.02)\n\nThe population mean is:\n\n(PopMean&lt;-mean(Population))\n\n[1] 48.61053\n\n\nThe standard deviation is:\n\n(PopSD&lt;-sd(Population))\n\n[1] 47.94411\n\n\n\nThe mean is very close to the population mean \\(48.83\\). The standard deviation is \\(6.83\\).\n\nIn R you can use a for loop to create the vector of sample means.\n\nnrep&lt;-10000\nSampleMeans&lt;-c()\nfor (i in 1:nrep){\n  x&lt;-sample(Population,50,replace=T)\n  SampleMeans&lt;-c(SampleMeans,mean(x))\n}\n\nThe mean of SampleMeans is:\n\n(xbar&lt;-mean(SampleMeans))\n\n[1] 48.7005\n\n\nThe standard deviation is:\n\n(Standard&lt;-sd(SampleMeans))\n\n[1] 6.827595\n\n\n\nThe confidence interval is [\\(47.71\\),\\(70.17\\)]. Since the population mean is equal to \\(48.61\\), the confidence interval does include the population mean.\n\nLet’s construct the upper an lower limits of the interval in R.\n\n(ll&lt;-SampleMeans[1]+qnorm(0.05)*Standard)\n\n[1] 47.71385\n\n(ul&lt;-SampleMeans[1]-qnorm(0.05)*Standard)\n\n[1] 70.17464\n\n\n\nThe confidence interval is [\\(14.86\\),\\(37.32\\)]. This interval does not include the population mean of \\(48.61\\). Out of the \\(10,000\\) confidence intervals, one would expect about \\(9,000\\) to include the population mean.\n\nLet’s find the confidence interval limits using R.\n\n(Minll&lt;-min(SampleMeans)+qnorm(0.05)*Standard)\n\n[1] 14.85631\n\n(Minul&lt;-min(SampleMeans)-qnorm(0.05)*Standard)\n\n[1] 37.31709\n\n\nWe can confirm in R that about \\(9,000\\) of the intervals include PopMean. Once more, let’s use a for loop to construct confidence intervals for each element in SampleMeans and check whether the PopMean is included. The count variable keeps track of how many intervals include the population mean.\n\ncount=0\n\nfor (i in SampleMeans){\n  (ll&lt;-i+qnorm(0.05)*Standard)\n  (ul&lt;-i-qnorm(0.05)*Standard)\n  if (PopMean&lt;=ul & PopMean&gt;=ll){\n    count=count+1\n  }\n}\n\ncount\n\n[1] 8978\n\n\n\n\nExercise 2\n\nThe \\(90\\)% confidence interval is [\\(94.52\\),\\(114.67\\)] and the \\(95\\)% confidence interval is [\\(114.68\\),\\(116.76\\)]. The larger the confidence level, the larger the interval.\n\nLet’s construct the intervals using R. Since the population standard deviation is unknown we will use the t-distribution. The interval is constructed as \\(\\bar{x}\\pm t_{\\alpha/2} \\frac{s}{\\sqrt{n}}\\).\n\n(ul90&lt;-104.6-qt(0.05,23)*28.8/sqrt(24))\n\n[1] 114.6755\n\n(ll90&lt;-104.6+qt(0.05,23)*28.8/sqrt(24))\n\n[1] 94.52453\n\n\nFor the \\(95\\)% confidence interval we adjust the significance level accordingly.\n\n(ul95&lt;-104.6-qt(0.025,23)*28.8/sqrt(24))\n\n[1] 116.7612\n\n(ll95&lt;-104.6+qt(0.025,23)*28.8/sqrt(24))\n\n[1] 92.43883\n\n\n\nThe confidence interval for a sample size of \\(16\\) is [\\(30.75\\),\\(66.61\\)]. The confidence interval when the sample size is \\(25\\) is [\\(34.79\\),\\(62.57\\)]. As the sample size gets larger, the confidence interval gets narrower and more precise.\n\nLet’s use R again to calculate the confidence interval. For a sample size of \\(16\\) the interval is:\n\n(ul16&lt;-48.68-qt(0.025,15)*33.64/sqrt(16))\n\n[1] 66.60549\n\n(ll16&lt;-48.68+qt(0.025,15)*33.64/sqrt(16))\n\n[1] 30.75451\n\n\nIncreasing the ample size to \\(25\\) yields:\n\n(ul25&lt;-48.68-qt(0.025,24)*33.64/sqrt(25))\n\n[1] 62.56591\n\n(ll25&lt;-48.68+qt(0.025,24)*33.64/sqrt(25))\n\n[1] 34.79409\n\n\n\n\nExercise 3\n\nThe 95% confidence interval for group 1 is [\\(-0.53\\),\\(2.03\\)].\n\nLet’s first calculate the standard error for group 1.\n\n(se1&lt;-sd(sleep$extra[sleep$group==1])/sqrt(length(sleep$extra[sleep$group==1])))\n\n[1] 0.5657345\n\n\nWe can now use the standard error to estimate the lower and upper limits of the confidence interval.\n\n(ll1&lt;-mean(sleep$extra[sleep$group==1])+qt(0.025,9)*se1)\n\n[1] -0.5297804\n\n(ul1&lt;-mean(sleep$extra[sleep$group==1])-qt(0.025,9)*se1)\n\n[1] 2.02978\n\n\n\nThe 95% confidence interval for group 2 is [\\(0.90\\),\\(3.76\\)].\n\nLet’s repeat the procedure for group 2. Start by finding the standard error.\n\n(se2&lt;-sd(sleep$extra[sleep$group==2])/sqrt(length(sleep$extra[sleep$group==2])))\n\n[1] 0.6331666\n\n\nUsing the standard error we can complete the confidence interval.\n\n(ll2&lt;-mean(sleep$extra[sleep$group==2])+qt(0.025,9)*se2)\n\n[1] 0.8976775\n\n(ul2&lt;-mean(sleep$extra[sleep$group==2])-qt(0.025,9)*se2)\n\n[1] 3.762322\n\n\n\nDrug 2. Drug 2 does not include zero in the interval, and the interval is to the right of zero. It is unlikely, that drug 2 has no effect on students sleeping time. Additionally, Drug 2’s mean increase in sleeping hours is \\(2.33\\) vs. \\(0.75\\) for drug 1.\n\n\n\nExercise 4\n\nThe 90% and 95% confidence intervals are [\\(0.319\\),\\(0.481\\)], and [\\(0.304\\),\\(0.496\\)] respectively. Since they do not include 0.5, we can conclude that the population proportion is significantly different from 0.5.\n\nWe can create an object that stores the sample proportion and sample in R:\n\n(p&lt;-0.4)\n\n[1] 0.4\n\n(n&lt;-100)\n\n[1] 100\n\n\nThe \\(90\\)% confidence interval is given by:\n\n(Ex1ll90&lt;-p+qnorm(0.05)*sqrt(p*(1-p)/100))\n\n[1] 0.319419\n\n(Ex1ul90&lt;-p-qnorm(0.05)*sqrt(p*(1-p)/100))\n\n[1] 0.480581\n\n\nThe \\(95\\)% confidence interval is:\n\n(Ex1ll90&lt;-p+qnorm(0.025)*sqrt(p*(1-p)/100))\n\n[1] 0.3039818\n\n(Ex1ul90&lt;-p-qnorm(0.025)*sqrt(p*(1-p)/100))\n\n[1] 0.4960182\n\n\n\nThe \\(90\\)% confidence interval is [\\(0.132\\),\\(0.182\\)].The \\(95\\)% confidence interval is [\\(0.128\\),\\(0.186\\)].\n\nThe data can easily be viewed by calling HairEyeColor in R.\n\nHairEyeColor\n\n, , Sex = Male\n\n       Eye\nHair    Brown Blue Hazel Green\n  Black    32   11    10     3\n  Brown    53   50    25    15\n  Red      10   10     7     7\n  Blond     3   30     5     8\n\n, , Sex = Female\n\n       Eye\nHair    Brown Blue Hazel Green\n  Black    36    9     5     2\n  Brown    66   34    29    14\n  Red      16    7     7     7\n  Blond     4   64     5     8\n\n\nNote that there are three dimensions to this table (Hair, Eye, Sex). We can calculate the proportion of Hazel eye colored students with the following command that makes use of indexing:\n\n(p&lt;-(sum(HairEyeColor[,3,1])+sum(HairEyeColor[,3,2]))/sum(HairEyeColor))\n\n[1] 0.1570946\n\n\nNow we can use this proportion to construct the intervals. Recall that for proportions the interval is calculated by \\(\\bar{p} \\pm z_{\\alpha/2} \\sqrt{\\frac{\\bar{p}(1-\\bar{p})}{n}}\\). The \\(90\\)% confidence interval is given by:\n\n(Ex2ll90&lt;-p+qnorm(0.05)*sqrt(p*(1-p)/592))\n\n[1] 0.1324945\n\n(Ex2ul90&lt;-p-qnorm(0.05)*sqrt(p*(1-p)/592))\n\n[1] 0.1816947\n\n\nThe 95% confidence interval is:\n\n(Ex2ll95&lt;-p+qnorm(0.025)*sqrt(p*(1-p)/592))\n\n[1] 0.1277818\n\n(Ex2ul95&lt;-p-qnorm(0.025)*sqrt(p*(1-p)/592))\n\n[1] 0.1864074",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Inference II</span>"
    ]
  },
  {
    "objectID": "InfReg.html",
    "href": "InfReg.html",
    "title": "14  Regression and Inference",
    "section": "",
    "text": "14.1 Concepts",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Regression and Inference</span>"
    ]
  },
  {
    "objectID": "InfReg.html#concepts",
    "href": "InfReg.html#concepts",
    "title": "14  Regression and Inference",
    "section": "",
    "text": "Correlation Significance\nTo determine the statistical significance of the correlation coefficient we test:\n\n\\(H_o: \\rho \\geq 0\\); \\(H_a: \\rho &lt;0\\) left tail\n\\(H_o: \\rho \\leq 0\\); \\(H_a: \\rho &gt;0\\) right tail\n\\(H_o: \\rho = 0\\); \\(H_a: \\rho \\neq 0\\) two tails\n\nThe test statistic for the correlation is given by \\(t_{df}= \\frac{r_{xy}\\sqrt{n-2}}{\\sqrt{1-r_{xy}^2}}\\), where \\(df=n-2\\) and \\(r_{xy}\\) is the sample correlation coefficient.\nRun the cor.test() function to perform the test on two vectors. Here is a list of arguments to use:\n\nalternative: is a choice between “two.sided”, “less” and “greater”.\nconf.level: sets the confidence level. Enter as a decimal and not percentage.\n\n\n\nDifference of Means Tests\nTests for inference about the difference of two population means.\n\nThe test for unpaired mean differences (not equal variances) is given by \\(t_{df}= \\frac {(\\bar x_1 - \\bar x_2)- \\bar d_o}{\\sqrt {\\frac {s_1^2}{n_1} \\frac{s_2^2}{n_2}}}\\).\nThe test for unpaired mean difference (equal variances) is given by \\(t_{df}= \\frac {(\\bar x_1 - \\bar x_2)- \\bar d_o}{\\sqrt {s_p^2 (\\frac {1}{n_1} + \\frac {1}{n_2})}}\\).\nThe test for paired mean difference is given by \\(t_{df}= \\frac {\\bar d- d_o}{\\frac {s}{\\sqrt{n}}}\\).\n\nRun these test in R by using the t.test() function. Here is a list of arguments to use:\n\npaired: use True for paired, False for independent. The default is False.\nvar.equal: use True for equal variances, False for unequal. The default is False.\nmu: a value that indicate the hypothesized value of the mean or mean difference.\nalternative: is a choice between “two.sided”, “less” and “greater”.\nconf.level: sets the confidence level. Enter as a decimal and not percentage.\n\n\n\nRegression Inference\nWhen running regression a couple of test can be performed on the coefficients to determine significance:\n\nThe first test competing hypothesis are \\(H_o: \\beta_j = 0\\); \\(H_a: \\beta_j \\ne 0\\). The test statistic for the intercept (slope) coefficient is given by \\(t_{df}= \\frac {b_j}{se(b_j)}\\).\nThe second test competing hypothesis are \\(H_o: \\beta_1=\\beta_2=...\\beta_k=0\\); \\(H_a:\\) at least one \\(\\beta_i \\neq 0\\). The joint test of significance is given by \\(F_{df_1,df_2} = \\frac {SSR/k}{SSE/(n-k-1)} = \\frac {MSR}{MSE}\\). The Anova table below shows more detail on this test.\n\n\n\n\n\n\n\n\n\n\n\n\nAnova\ndf\nSS\nMS\nF\nSignificance\n\n\n\n\nRegression\n\\(k\\)\n\\(SSR\\)\n\\(MSR=\\frac{SSR}{k}\\)\n\\(F_{df_1,df_2} = \\frac {MSR}{MSE}\\)\n\\(P(F) \\geq \\frac{MSR}{MSE}\\)\n\n\nResidual\n\\(n-k-1\\)\n\\(SSE\\)\n\\(MSE=\\frac {SSE}{n-k-1}\\)\n\n\n\n\nTotal\n\\(n-1\\)\n\\(SST\\)\n\n\n\n\n\n\nTo conduct these tests, save the lm() model into an object. The summary() function can then be used to retrieve the results of the tests on the model’s parameters. Use the anova() function to obtain the Anova table.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Regression and Inference</span>"
    ]
  },
  {
    "objectID": "InfReg.html#exercises",
    "href": "InfReg.html#exercises",
    "title": "14  Regression and Inference",
    "section": "14.2 Exercises",
    "text": "14.2 Exercises\nThe following exercises will help you test your knowledge on Regression and Inference. In particular, the exercises work on:\n\nDetermining the significance of correlations.\nConduct paired and unpaired test of means and proportions.\nDetermining the significance of the slope and intercept estimates both individually and jointly.\nDeveloping prediction intervals.\n\nAnswers are provided below. Try not to peak until you have a formulated your own answer and double checked your work for any mistakes.\n\nExercise 1\n\nConsider the following competing hypothesis: \\(H_{o}: \\rho=0\\), \\(H_{a}: \\rho \\neq 0\\). A sample of \\(25\\) observations reveals that the correlation coefficient between two variables is \\(0.15\\). At a \\(5\\)% confidence level, can we reject the null hypothesis?\nInstall the ISLR2 package in R. Use the Hitters data set to look at the relationship between Hits and Salary. Specifically, calculate the correlation coefficient and test the competing hypothesis \\(H_{o}: \\rho=0\\), \\(H_{a}: \\rho \\neq 0\\) at the \\(1\\)% significance level.\n\n\n\nExercise 2\n\nInstall the ISLR2 package in R. Use the Hitters data set to investigate if the average hits were significantly different between the two divisions (American and National). Use the NewLeague and Hits variables to test the hypothesis at the \\(5\\)% significance level. Is there reason to believe that the population variances are different?\nUse the ISLR2 package for this question. Particularly, use the BrainCancer data set to test whether males have a higher average survival time than women. Use the sex and time variables to test the hypothesis at the \\(5\\)% significance level. Is there reason to believe that the population variances are different?\n\n\n\nExercise 3\n\nUse the sleep data set included in R. At the \\(1\\)% significance level, is there an effect of the drug on the \\(10\\) patients? Assume that the group variable denotes before (\\(1\\)) the drug is administered and after (\\(2\\)) the drug is administered.\n\n\n\nExercise 4\n\nInstall the ISLR2 package in R. Use the Hitters data set to investigate the effect of HmRun,RBI, and Years on a players Salary. Which variables are statistically different from zero? Are the variables jointly significant? Does the \\(R^2\\) suggest a good fit of the data to the model?\nJosé Altuve had \\(28\\) home runs, \\(57\\) RBI’s, and has been in the league for \\(12\\) years. What is the model’s predicted salary for him? What is the \\(95\\)% prediction interval? Note: The model predicts his salary if he played in \\(1987\\).",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Regression and Inference</span>"
    ]
  },
  {
    "objectID": "InfReg.html#answers",
    "href": "InfReg.html#answers",
    "title": "14  Regression and Inference",
    "section": "14.3 Answers",
    "text": "14.3 Answers\n\nExercise 1\n\nAt the \\(5\\)% significance level, we can not reject the null since the p-value is \\(0.47&gt;0.05\\).\n\nRecall that the t-stat is calculated by \\(\\frac {r_{xy}\\sqrt{n-2}}{\\sqrt{1-r_{xy}^2}}\\). We can use R as a calculator to calculate this value:\n\nrxy&lt;-0.15\nn&lt;-25\n(tstat&lt;-(rxy*sqrt(n-2))/(sqrt(1-rxy^2)))\n\n[1] 0.7276069\n\n\nNow, we can estimate the \\(p\\)-value using the pt() function:\n\n2*pt(tstat,n-2,lower.tail = F)\n\n[1] 0.4741966\n\n\n\nThe estimated correlation of \\(0.44\\) and the t-value is \\(7.89\\). Since the \\(p\\)-value is approximately \\(0\\) we reject the null hypothesis \\(H_{o}: \\rho=0\\).\n\nOnce the ISLR2 package is downloaded, it can be loaded to R using the library() function. The cor.test() function conducts the appropriate test of significance.\n\nlibrary(ISLR2)\ncor.test(Hitters$Salary,Hitters$Hits, conf.level = 0.95)\n\n\n    Pearson's product-moment correlation\n\ndata:  Hitters$Salary and Hitters$Hits\nt = 7.8863, df = 261, p-value = 8.531e-14\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3355210 0.5314332\nsample estimates:\n      cor \n0.4386747 \n\n\n\n\nExercise 2\n\nThere is no reason to believe that the population variances are different. Players are recruited from what seems to be a common pool. At a \\(5\\)% significance level, the difference of the two means is not significantly different from zero. We can’t reject the null hypothesis.\n\nWe will use the t.test() function in R to test the hypothesis. We note that the test is not paired, two sided and of equal variances in the population.\n\nt.test(Hitters$Hits[Hitters$NewLeague==\"A\"],\n       Hitters$Hits[Hitters$NewLeague==\"N\"],paired = F, \n       alternative = \"two.sided\",mu = 0,var.equal = T,\n       conf.level = 0.95 )\n\n\n    Two Sample t-test\n\ndata:  Hitters$Hits[Hitters$NewLeague == \"A\"] and Hitters$Hits[Hitters$NewLeague == \"N\"]\nt = 1.0862, df = 320, p-value = 0.2782\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -4.581286 15.875028\nsample estimates:\nmean of x mean of y \n103.58523  97.93836 \n\n\n\nThere might be reason to believe that the population variances are different. Women and men are known to have medical differences. At a \\(5\\)% significance level, the average survival time of men seems not to be larger than that of women. We can’t reject the null hypothesis \\(H_{o}:\\bar {x_{1}}-\\bar {x_{2}} \\leq 0\\).\n\nOnce more use the t.test() function in R to test the hypothesis. Note that the test is not paired, right-tailed and of different variances in the population.\n\nt.test(BrainCancer$time[BrainCancer$sex==\"Male\"],\n       BrainCancer$time[BrainCancer$sex==\"Female\"],paired = F, \n       alternative = \"greater\",mu = 0, var.equal = F,\n       conf.level = 0.95 )\n\n\n    Welch Two Sample t-test\n\ndata:  BrainCancer$time[BrainCancer$sex == \"Male\"] and BrainCancer$time[BrainCancer$sex == \"Female\"]\nt = -0.30524, df = 84.867, p-value = 0.6195\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n -8.504999       Inf\nsample estimates:\nmean of x mean of y \n 26.78302  28.10200 \n\n\n\n\nExercise 3\n\nThere drug seems to have an effect as we can reject the null hypothesis \\(H_{o}:\\bar {d} = 0\\). The difference of means seems to be statistically different from zero.\n\nUse the t.test() function once more in R. Make sure to note that the test is paired, and two-tailed.\n\nt.test(sleep$extra[sleep$group==1],\n       sleep$extra[sleep$group==2], paired=T,\n       alternative = \"two.sided\", mu=0, conf.level = 0.99)\n\n\n    Paired t-test\n\ndata:  sleep$extra[sleep$group == 1] and sleep$extra[sleep$group == 2]\nt = -4.0621, df = 9, p-value = 0.002833\nalternative hypothesis: true mean difference is not equal to 0\n99 percent confidence interval:\n -2.8440519 -0.3159481\nsample estimates:\nmean difference \n          -1.58 \n\n\n\n\nExercise 4\n\nBoth RBI and Years are statistically significant and the salary of a player increases as they gain more experience and have more RBI’s. Home runs do not seem to have an impact on the salary of a player according to the data. The F-Statistics reveals that the coefficients are jointly significant since the p-value is approximately zero. Both the Multiple and Adjusted \\(R^2\\) suggest that the model only accounts for \\(32\\)% of the variation in Salary. We might have to include more variable in our model to better explain the salary of a player.\n\nWe can run a linear regression in R by using the lm() function. We’ll use the summary() function to get more details on the model’s performance.\n\nfit&lt;-lm(Salary~HmRun+RBI+Years,data=Hitters)\nsummary(fit)\n\n\nCall:\nlm(formula = Salary ~ HmRun + RBI + Years, data = Hitters)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-752.31 -197.27  -66.80   97.73 2151.78 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -90.086     61.142  -1.473    0.142    \nHmRun         -7.346      4.972  -1.478    0.141    \nRBI            9.156      1.685   5.432 1.28e-07 ***\nYears         32.818      4.838   6.783 7.97e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 372.2 on 259 degrees of freedom\n  (59 observations deleted due to missingness)\nMultiple R-squared:  0.3269,    Adjusted R-squared:  0.3191 \nF-statistic: 41.93 on 3 and 259 DF,  p-value: &lt; 2.2e-16\n\n\n\nThe predicted salary is \\(619.93\\) and the \\(95\\)% prediction interval is [\\(-129.89\\),\\(1369.7\\)].\n\n\nnew&lt;-data.frame(HmRun=28,RBI=57,Years=12)\npredict(fit,newdata=new,level=0.95,interval=\"prediction\")\n\n       fit       lwr      upr\n1 619.9268 -129.8905 1369.744",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Regression and Inference</span>"
    ]
  },
  {
    "objectID": "ObjectsVectors.html",
    "href": "ObjectsVectors.html",
    "title": "15  Objects and Vectors",
    "section": "",
    "text": "15.1 Concepts",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Objects and Vectors</span>"
    ]
  },
  {
    "objectID": "ObjectsVectors.html#concepts",
    "href": "ObjectsVectors.html#concepts",
    "title": "15  Objects and Vectors",
    "section": "",
    "text": "Objects\nAn object is a data structure that stores a value or a set of values, along with information about the type of data and any associated attributes. Objects are usually created by assigning a value to a variable name. You can assign values by using either = or &lt;-.\nWhen naming objects in R use PascalCase, camelCase, snake_case or dot.case.\n\nScreenTime&lt;-120\n\n\n\nVectors\nA vector is a one-dimensional array that can hold elements of any data type.\nSome common data types are numeric, character, logical, and complex.\nUse the c() function to concatenate (combine) elements and store them in a vector.\n\nScreenTimeDays&lt;-c(110,115,120,98,60)\n\n\n\nFunctions\nIn general, functions relate an input (arguments) to an output. For example, the sum() function takes as an input a vector with numeric values and returns the sum of the elements.\n\nSleepingHours&lt;-c(10,9,6,8)\nsum(SleepingHours)\n\n[1] 33\n\n\nTo learn more about a function you can use ?. For example, to learn more about the sum() function, write ?sum in the console.\n\n\nData Types\nThe main data types are numeric, character, logical, date, and complex. To identify the data type stored in a vector use the class() function.\n\nclass(SleepingHours)\n\n[1] \"numeric\"\n\n\n\n\nUseful R Functions",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Objects and Vectors</span>"
    ]
  },
  {
    "objectID": "ObjectsVectors.html#exercises",
    "href": "ObjectsVectors.html#exercises",
    "title": "15  Objects and Vectors",
    "section": "15.2 Exercises",
    "text": "15.2 Exercises",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Objects and Vectors</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Grolemund, Garret. 2014. “Hands-on Programming with r.” https://jjallaire.github.io/hopr/.\n\n\nWickham, Hadley. 2017. “R for Data Science.” https://r4ds.hadley.nz.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "DescriptiveII.html#freqdistnum",
    "href": "DescriptiveII.html#freqdistnum",
    "title": "2  Descriptive Stats II",
    "section": "2.2 Frequency Distributions (Numerical)",
    "text": "2.2 Frequency Distributions (Numerical)\nWhen working with numerical data, building a frequency distributions requires additional steps compared to categorical data. The challenge lies in the absence of predefined categories or classes. To construct a frequency distribution for numerical data, it is essential to determine the number, width, and limits of the classes. Here are the steps to create a frequency distribution when data is numerical:\n1. Determine the Number of Classes: The number of classes can be estimated using the \\(2^k\\) rule, where \\(k\\) is the smallest integer such that \\(2^k\\) exceeds the total number of observations by the least amount. This ensures the chosen number of classes provides a reasonable level of granularity for summarizing the data.\nEx: If a data set has 50 observations, we would choose six classes since \\(2^6=64\\) is greater than \\(50\\) by the least amount.\n2. Calculate the Width of Each Class: The width of a class is determined using the formula: range/(# of Classes).\nEx: If the data set has 50 observations and the minimum value 20 and the maximum is 78, then the width of each class is \\(58/6 \\approx 9.7\\). Hence, we can round up and use a class width of 10.\n3. Establish Class Limits: The class limits define the range of values in each class. These limits should be chosen such that each data point belongs to only one class.\nEx: Consider a data set of 50 observations where each class has a width of 10. Set the class limits of the first class to [20,30). Note that the square bracket indicates that the point should be included in the class, whereas ) indicates that the point should no be included in the class. The six classes would be [20,30), [30,40), [40,50), [50,60), [60,70), and [70,80).\nExample: Let’s look at a snapshot of the Dow Jones Industrial 30 stock prices. Below you can see the data:\n\n\n\n\n\n\n\n\n\nLet’s follow the steps to build the frequency distribution.\n1. Determine the Number of Classes: Here we choose five classes since \\(2^5=32\\) is greater than \\(30\\) by the least amount.\n2. Calculate the Width of Each Class: The smallest values in the data set is \\(23\\) and the maximum is \\(501\\). This gives us a range of \\(478\\). Now we can just take the range and divide by five to get \\(95.6\\). To make things simple we can round to \\(100\\) and use a class width of \\(100\\).\n3. Establish Class Limits: Since we have rounded up we can be flexible with our class limits. The following class limits are suggested [20,120), [120,220), [220,320), [320,420), and [420,520). Note that each class has a width of \\(100\\), and that each data point belongs to one single class.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Stats II</span>"
    ]
  },
  {
    "objectID": "DescriptiveII.html#frequency-distributions-in-r-categorical",
    "href": "DescriptiveII.html#frequency-distributions-in-r-categorical",
    "title": "2  Descriptive Stats II",
    "section": "2.3 Frequency Distributions in R (Categorical)",
    "text": "2.3 Frequency Distributions in R (Categorical)\nThe process of constructing frequency distributions in R is straightforward. We will be mainly using the table() function. Let’s start by saving the data in a vector:\n\nfood&lt;-c(\"Pizza\",\"Sushi\",\"Sushi\",\"Chicken\",\n        \"Chicken\",\"Pasta\",\"Pasta\",\"Pasta\",\n        \"Sushi\",\"Pasta\",\"Chicken\",\"Pizza\",\n        \"Chicken\",\"Sushi\",\"Pizza\",\"Sushi\",\n        \"Sushi\",\"Sushi\",\"Sushi\",\"Pizza\",\n        \"Pizza\",\"Chicken\",\"Sushi\",\"Pizza\",\n        \"Sushi\")\n\nHere we define a vector called food by assigning (&lt;-) the combination (c) of all the food items. To generate the frequency distribution we simply pass the food vector into the table() command.\n\ntable(food)\n\nfood\nChicken   Pasta   Pizza   Sushi \n      5       4       6      10 \n\n\nAs you can see this tallies all the instances for each item. If instead we wanted to obtain the relative frequency we can use the prop.table() function. This function requires as an input a frequency distribution created by the table() function. Hence, we can first create the frequency distribution, save it into an object, and the generate the relative frequency. The code is below:\n\nfreq&lt;-table(food)\nprop.table(freq)\n\nfood\nChicken   Pasta   Pizza   Sushi \n   0.20    0.16    0.24    0.40 \n\n\nAs a last modification. If you want percent frequencies, you can multiply the prop.table by 100, as shown below:\n\nprop.table(freq)*100\n\nfood\nChicken   Pasta   Pizza   Sushi \n     20      16      24      40",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Stats II</span>"
    ]
  },
  {
    "objectID": "DescriptiveII.html#bar-plot-in-r",
    "href": "DescriptiveII.html#bar-plot-in-r",
    "title": "2  Descriptive Stats II",
    "section": "2.4 Bar Plot in R",
    "text": "2.4 Bar Plot in R\nTo create the bar plot we will be using the geom_bar() function within the tidyverse package. We start by loading the package:\n\nlibrary(tidyverse)\n\nNow R will identify the functions ggplot() and geom_bar() from the ggplot library. To construct the plot we will first call on ggplot and then specify the type of graph we want by calling on geom_bar(). In the aes() function we will specify which variable (or vector) we want to plot.\n\nggplot() + geom_bar(aes(food))\n\n\n\n\n\n\n\n\nWe can enhance the visualization by adding a title and changing the theme. The labs() function allows us to change the titles and the ggthemes package allows us to choose from a variety of themes.\n\nggplot() + geom_bar(aes(food), col=\"black\", alpha=0.5, bg=\"blue\") +\n  labs(title=\"Favorite Food Items\",\n       subtitle=\"Class of 25 Students\",\n       x=\"\", y=\"Frequency\") + \n  theme_clean()\n\n\n\n\n\n\n\n\nA few arguments are worth explaining in the code above. The arguments in the geom_bar() function change the background color (bg), the transparency of the color (alpha), and the color of the outline for the bars (col). Title and subtitles are added within the labs() function. To omit label we can just open and close quotations. Hence, x=\"\" omits the x label.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Stats II</span>"
    ]
  },
  {
    "objectID": "DescriptiveII.html#frequency-distribution-in-r-numerical",
    "href": "DescriptiveII.html#frequency-distribution-in-r-numerical",
    "title": "2  Descriptive Stats II",
    "section": "2.5 Frequency Distribution in R (Numerical)",
    "text": "2.5 Frequency Distribution in R (Numerical)\nTo construct the frequency distribution in R we will be first generating the classes and the using the table() function as we did in the categorical case. Let’s first get the data into R:\n\ndow&lt;-c(277,174,202,383,358,188,293,156,\n       212,42,149,410,303,165,203,103,\n       22,59,287,121,312,52,53,158,500,\n       96,95,43,188,200)\n\nTo generate the bins we will use the example and procedure found in 2.2. That is we will be using five classes, of width 100. Below is the code to do this:\n\nthresh&lt;-seq(20,520,100)\nprice.cut&lt;-cut(dow,thresh,right=F)\n(dowfreq&lt;-table(price.cut))\n\nprice.cut\n [20,120) [120,220) [220,320) [320,420) [420,520) \n        9        12         5         3         1 \n\n\nThe process involves three steps. First we generate a sequence that captures the thresholds for our bins. We start at the minimum 20 and each class is generated at an increment of 100 (the class width). Next, we place each observation in the dow, into the bins by using the cut() function. This involve using, the data (dow), the thresholds for each class (thresh), and specifying if the right limit should be included in the bins. The last step is to tally the results with the table() function.\nTo obtain the cumulative distribution, we can use the cumsum() function. Below we just wrap the frequency distribution (freq) into the cumsum() function.\n\ncumsum(dowfreq)\n\n [20,120) [120,220) [220,320) [320,420) [420,520) \n        9        21        26        29        30",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Stats II</span>"
    ]
  },
  {
    "objectID": "DescriptiveII.html#histograms-in-r",
    "href": "DescriptiveII.html#histograms-in-r",
    "title": "2  Descriptive Stats II",
    "section": "2.6 Histograms in R",
    "text": "2.6 Histograms in R\nTo generate the histogram in R we will use once again the tidyverse package. This time we will use the geom_histogram() function. Below is the code to generate the histogram for the dow data:\n\nggplot() + \n  geom_histogram(aes(dow), bg=\"blue\", alpha=0.5, col=\"black\",\n                 bins=5,\n                 binwidth = 100,\n                 boundary=20) +\n  labs(title=\"Dow Stock Prices\",\n       y=\"Frequency\", x=\"\") + \n  theme_clean()\n\n\n\n\n\n\n\n\nWithin the geom_histogram() command we can set the classes (or bins) for the histogram. The bins argument specifies the number of bins, bin.width specifies the bin width, and the boundary specifies where should the histogram starts. This histogram allows us to observe quickly that most stocks in the dow are price between 20 and 220 dollars.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Stats II</span>"
    ]
  },
  {
    "objectID": "DescriptiveIII.html#the-median",
    "href": "DescriptiveIII.html#the-median",
    "title": "3  Descriptive Statistics III",
    "section": "3.2 The Median",
    "text": "3.2 The Median\nThe median is the value in the middle when data is organized in ascending order. When \\(n\\) is even, the median is the average between the two middle values. The median is resistant to outliers, making it an ideal measure for skewed data or data sets with irregular distributions. It is particularly useful for ordinal data, where precise ranking is important. However, the median does not utilize all data points, which can lead to less precise comparisons compared to other measures like the mean.\nEx 1: Consider the following numbers \\(x={1,4,2,1}\\). We first sort the data to obtain \\(x_{sorted}={1,1,2,4}\\). Since the number of observations is even (\\(n=4\\)), the median is the average of the two middle numbers (1,2). \\(median_x=\\frac{1+2}{2}\\) or \\(1.5\\).\nEx 2: Consider the following numbers \\(x={1,4,2,1,100}\\). We once again sort the number obtaining \\(x_{sorted}={1,1,2,4,100}\\). Since \\(n=5\\) in this case we can identify the median as the third value in \\(x_{sorted}\\). \\(median_x=2\\). Note that the inclusion of \\(100\\) in the data did not change much the measure of central location.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics III</span>"
    ]
  },
  {
    "objectID": "DescriptiveIII.html#the-mode",
    "href": "DescriptiveIII.html#the-mode",
    "title": "3  Descriptive Statistics III",
    "section": "3.3 The Mode",
    "text": "3.3 The Mode\nThe mode is the value with highest frequency from a set of observations. This measure is particularly useful for categorical data, as it helps determine popularity or commonality, and it can be applied to both numerical and non-numerical data sets. However, the mode has its limitations; it may not exist in cases where all values occur with equal frequency, and there may be multiple modes, which can complicate interpretation. Additionally, since the mode focuses only on the most frequent value, it does not account for other data points, limiting its overall utility as a comprehensive measure.\nEx: Consider the following numbers \\(x={1,4,2,1}\\). Since \\(1\\) is repeated twice and all other numbers just repeated once, \\(x_{mode}=1\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics III</span>"
    ]
  },
  {
    "objectID": "DescriptiveIII.html#the-weighted-mean",
    "href": "DescriptiveIII.html#the-weighted-mean",
    "title": "3  Descriptive Statistics III",
    "section": "3.4 The Weighted Mean",
    "text": "3.4 The Weighted Mean\nThe weighted mean is useful in scenarios where some data points are more significant than others, such as in financial portfolios, grade point averages, or survey results, as it accounts for variability in importance across observations. However, this measure requires additional information in the form of weights, which may not always be available or accurate. Furthermore, it is sensitive to errors in weighting, which can distort the results and lead to misleading conclusions. It is calculated by calculating the sum product of values (\\(x_i\\)) and weights (\\(w_i\\)) and then dividing by the sum of weights. Mathematically, the weighted average is \\(\\frac{\\sum w_{i}x_{i}}{\\sum w_{i}}\\).\nEx: Consider three different stocks \\(S={T, C, X}\\) with stock returns of \\(R={2,4,10}\\). Each stock has a weight in the portfolio of \\(W={0.3,0.2,0.5}\\). The average return of the portfolio is \\(\\bar{x}_{weighted}=\\frac{0.6+0.8+5}{1}\\) or \\(\\bar{x}_{weighted}=6.4\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics III</span>"
    ]
  },
  {
    "objectID": "DescriptiveIII.html#the-geometric-mean",
    "href": "DescriptiveIII.html#the-geometric-mean",
    "title": "3  Descriptive Statistics III",
    "section": "3.5 The Geometric Mean",
    "text": "3.5 The Geometric Mean\nThe geometric mean is a multiplicative average that is less sensitive to outliers. It is useful when averaging growth rates or rates of return. It is calculated by \\(\\sqrt[n]{(1+r_1)*(1+r_2)...(1+r_n)}-1\\), where \\(\\sqrt[n]{}\\) is the \\(n_{th}\\) root, and \\(r_i\\) are the returns or growth rates. When working with growth rates or rates of return, you add 1 to each rate because these metrics represent changes relative to a base value.\nFor most all other values there is no need to add 1 because proportions already reflect a standalone quantity, not a relative change. In this case the geometric mean simplifies to \\(\\sqrt[n]{(x_1) \\times (x_2)...(x_n)}\\)\nEx: Consider the variable \\(x={0.2,0.3,0.1,0.1}\\). The geometric mean is equal to \\(\\bar{x}_g=\\sqrt[4]{(1.2 \\times 1.3 \\times 1.1 \\times 1.1)}-1\\) or \\(\\bar{x_g}=0.17\\) if x represents growth rates. When x represents ratios from a whole, the geometric mean is \\(\\bar{x}_g=\\sqrt[4]{0.2 \\times 0.3 \\times 0.1 \\times 0.1)}\\) or 0.156.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics III</span>"
    ]
  },
  {
    "objectID": "DescriptiveIII.html#measures-of-central-location-in-r",
    "href": "DescriptiveIII.html#measures-of-central-location-in-r",
    "title": "3  Descriptive Statistics III",
    "section": "3.6 Measures of Central Location in R",
    "text": "3.6 Measures of Central Location in R\nBase R has a collection of functions that calculate measures of central location. Let’s consider the following data on approval ratings:\n\nlibrary(tidyverse)\n(poll&lt;-tibble(date=c(\"01/01/24\", \"02/01/24\", \n                    \"03/01/24\", \"04/01/24\"),\n             people=c(50,100,30,250),\n             approval=c(0.25,0.25,0.7,0.85)))\n\n# A tibble: 4 × 3\n  date     people approval\n  &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 01/01/24     50     0.25\n2 02/01/24    100     0.25\n3 03/01/24     30     0.7 \n4 04/01/24    250     0.85\n\n\nTo calculate the mean we can just pass a vector into the mean() function. Hence, the mean approval is:\n\nmean(poll$approval)\n\n[1] 0.5125\n\n\nTo calculate the mode we will use the table() function, as there is no mode function in base R.\n\ntable(poll$approval)\n\n\n0.25  0.7 0.85 \n   2    1    1 \n\n\nFor the median we will use the median() function.\n\nmedian(poll$approval)\n\n[1] 0.475\n\n\nThe weighted average can be calculated using the weighted.mean() function. Let the approval be the value and number of people surveyed the weight.\n\nweighted.mean(x=poll$approval,w = poll$people)\n\n[1] 0.6302326\n\n\nLastly, the geometric mean has no built in function in base R. However, we can easily calculate it with the command:\n\ngeometric_mean &lt;- prod(poll$approval)^(1/length(poll$approval))\n\nSince the approval rating is a percentage of the total people polled there is no need to add one to these numbers.\nThe summary() calculates a collection of summary statistics for a vector or data frame. Below we apply it to the entire data set:\n\nsummary(poll)\n\n     date               people         approval     \n Length:4           Min.   : 30.0   Min.   :0.2500  \n Class :character   1st Qu.: 45.0   1st Qu.:0.2500  \n Mode  :character   Median : 75.0   Median :0.4750  \n                    Mean   :107.5   Mean   :0.5125  \n                    3rd Qu.:137.5   3rd Qu.:0.7375  \n                    Max.   :250.0   Max.   :0.8500",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics III</span>"
    ]
  },
  {
    "objectID": "DescriptiveIV.html#the-variance",
    "href": "DescriptiveIV.html#the-variance",
    "title": "4  Descriptive Stats IV",
    "section": "4.2 The Variance",
    "text": "4.2 The Variance\nVariance gives us a more comprehensive look at dispersion by considering how each data point deviates from the mean. It summarizes the squared deviations from the mean by finding their average.The population parameter is given by \\(\\sigma^2= \\frac{\\sum (x_i-\\mu)^2}{N}\\), while the sample statistic is \\(s^2=\\frac{\\sum (x_i-\\bar{x})^2}{n-1}\\).\nExample: Let’s consider a sample of the price of Ether (a famous crypto currency). Below is a graph of the prices:\n\n\n\n\n\n\n\n\n\nThe y axis represents the price, while the x axis represents the period of time. The red line is the average price of Ether. The way the variance calculates dispersion, is by first finding the distance between each point and the average. The image below illustrates these distances:\n\n\n\n\n\n\n\n\n\nIdeally, our measure of dispersion would simply average the distances from each data point to the mean, giving us a clear indication of how much the data varies around this central point. However, because the mean is a measure that balances the distances from the mean, the sum of all deviations is equal to zero. This “balancing” effect is visually apparent in the graph, where positive deviations are exactly offset by negative ones.\nTo address this issue, the variance squares each deviation from the mean before finding their average. This squaring eliminates negative values, ensuring a non-zero measure of spread. The table below illustrates these calculations:\n\n\n\n\n\n\n\n\n\nThe second column shows the deviations from the mean. You can convince yourself that these add up to zero. The third column squares the deviations. The variance, averages the numbers in the third column. The result is \\(s^2=977594\\), which means that the price of Ether varies on average 977,594 squared deviations from the mean. The result by itself is not very intuitive as it is measured in squared dollars. We can, however, compare the variances from different variables to assess which variable has the most dispersion.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Stats IV</span>"
    ]
  },
  {
    "objectID": "DescriptiveIV.html#the-standard-deviation",
    "href": "DescriptiveIV.html#the-standard-deviation",
    "title": "4  Descriptive Stats IV",
    "section": "4.3 The Standard Deviation",
    "text": "4.3 The Standard Deviation\nThe standard deviation is derived by taking the square root of the variance. Remember, the variance employs squared deviations to eliminate negative values and calculate spread. By taking the square root, the standard deviation reverts the measure of dispersion back to the original units of the variable. This transformation makes the standard deviation more intuitive; it directly quantifies how much each data point deviates from the mean in the same units as the variable itself. Consequently, the standard deviation is a clear and intuitive measure of variability.\nTo find the standard deviation, take the square root of the variance. For the population parameter use \\(\\sigma=\\sqrt{\\sigma^2}\\) and \\(s=\\sqrt{s^2}\\) for the sample statistic.\n*Ex: Consider once more the price of Ether. That is. \\(x=\\{480, 1050, 1400, 2500, 3200\\}\\). In the previous section we found that the variance \\(s^2=977594\\). Using the squared root we find that \\(s=\\sqrt{977594}=988.73\\). If the price of Ether is in dollars, then, the price varies from the mean 988.73 dollars on average.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Stats IV</span>"
    ]
  },
  {
    "objectID": "DescriptiveIV.html#mean-absolute-deviation",
    "href": "DescriptiveIV.html#mean-absolute-deviation",
    "title": "4  Descriptive Stats IV",
    "section": "4.4 Mean Absolute Deviation",
    "text": "4.4 Mean Absolute Deviation\nAnother measure of data variability is the Mean Absolute Deviation (MAD), which calculates variability using absolute deviations from the mean. This approach not only keeps the measure in the same units as the data but also makes it less sensitive to large deviations. Practically, the Mean Absolute Deviation measures the average deviation from the mean, by using absolute deviations. It is calculated by \\(MAD=\\frac{\\sum |x_i-\\mu|}{N}\\) for the population and \\(mad=\\frac{\\sum |x_i-\\bar{x}|}{n}\\) for the sample.\nExample: Let’s consider once more the price of Ether. Recall, that if we calculate the deviations from the mean we obtain the second column in the table below:\n\n\n\n\n\n\n\n\n\nFor the MAD, focus on the fourth column which lists the absolute deviations from the mean. Averaging these gives us the Mean Absolute Deviation (MAD), which equals \\(MAD=899.2\\). This result states that, on average, each data point is roughly 900 dollars away from the mean. Note that the standard deviation is higher (\\(988.73\\)) since the squaring of deviations disproportionately amplifies larger ones, pushing the standard deviation above the MAD.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Stats IV</span>"
    ]
  },
  {
    "objectID": "DescriptiveIV.html#coefficient-of-variation",
    "href": "DescriptiveIV.html#coefficient-of-variation",
    "title": "4  Descriptive Stats IV",
    "section": "4.5 Coefficient of Variation",
    "text": "4.5 Coefficient of Variation\nThe Coefficient of Variation simplifies comparisons of variability across variables with different units or scales, by dividing the standard deviation by the mean. It is calculated by \\(CV=s/\\bar{x}\\).\nExample: Consider the table below, that shows information on two different stocks:\n\n\n\n\n\n\n\n\n\nThe third column shows the standard deviation of each stock. One could conclude that both stocks vary the same as they have the same standard deviation of one. Recall that the coefficient of variation considers the variable’s scale by incorporating the mean into its calculation. Since one stock is centered around 1 dollar and the other around 100 dollars (second column), it is clear that they do not vary similarly percentage-wise. Stock A varies 100% from the mean, whereas Stock B only varies 1%. Hence, the coefficient of variation would identify Stock A as the more variable stock.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Stats IV</span>"
    ]
  },
  {
    "objectID": "DescriptiveIV.html#portfolio-assesment-and-the-sharpe-ratio",
    "href": "DescriptiveIV.html#portfolio-assesment-and-the-sharpe-ratio",
    "title": "4  Descriptive Stats IV",
    "section": "4.6 Portfolio Assesment and The Sharpe Ratio",
    "text": "4.6 Portfolio Assesment and The Sharpe Ratio\nTo asses the performance of a portfolio calculate:\n\nThe mean return of the portfolio \\(\\alpha\\bar{R}_1+(1-\\alpha)\\bar{R}_2\\), where \\(\\alpha\\) is the weight of investment 1 in the portfolio and \\(\\bar{R}_i\\) is the average return of investment \\(i \\in\\){\\(1\\),\\(2\\)}.\nThe variance of the portfolio is given by \\(\\begin{bmatrix} \\alpha \\\\ 1-\\alpha \\end{bmatrix}^T \\begin{bmatrix} s_x^2 & s_{xy} \\\\ s_{xy} & s_y^2 \\end{bmatrix} \\begin{bmatrix} \\alpha \\\\ 1-\\alpha \\end{bmatrix}\\)\nThe Sharpe ratio quantifies the excess return of an investment over the risk free return. It is calculated by \\(\\frac{\\bar{R_p}-R_f}{s}\\), where \\(\\bar{R_p}\\) is the mean return of the portfolio, \\(R_f\\) is the risk free return, and \\(s\\) is the standard deviation.\n\n\nUseful R Functions\nThe range() function returns the maximum and minimum of a vector of values.\nThe diff() function finds the first difference of a vector.\nThe var() function calculates the sample variance for a vector of values. To calculate the population variance, adjust the result by a factor of \\((n-1)/n\\).\nThe sd() function calculates the sample standard deviation.\nThe matrix() function defines a matrix.\nWhen dealing with matrices, the t() function transposes a vector or matrix, and the operator %*% performs matrix multiplication.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Stats IV</span>"
    ]
  },
  {
    "objectID": "DescriptiveIV.html#the-sharpe-ratio",
    "href": "DescriptiveIV.html#the-sharpe-ratio",
    "title": "4  Descriptive Stats IV",
    "section": "4.6 The Sharpe Ratio",
    "text": "4.6 The Sharpe Ratio\nThe Sharpe Ratio measures how much excess return an investor receives for the extra volatility (risk) taken on beyond the risk-free rate. It essentially uses the same principle of normalization as the CV but adds the dimension of risk-adjusted performance. If the CV tells you the variability of returns in relation to their mean, the Sharpe Ratio tells you if that variability is worth it by comparing it against what you could safely earn without risk. In essence, while the CV highlights variability, the Sharpe Ratio motivates investment choices by rewarding higher returns per unit of risk taken.\nIn sum, the Sharpe ratio quantifies the excess return of an investment over the risk free return. It is calculated by \\(\\frac{\\bar{R_p}-R_f}{s}\\), where \\(\\bar{R_p}\\) is the mean return of the portfolio, \\(R_f\\) is the risk free return, and \\(s\\) is the standard deviation.\nExample: Consider the table below that includes a collection of investments.\n\n\n\n\n\n\n\n\n\nThe table shows four investments (Apple, Bitcoin, Shiba, and S&P). Just looking at the daily return, it is clear that Shiba provides the best returns. However, the cost for that high return is reflected in variability (22% for Shiba). The S&P is clearly the safest investment with a standard deviation of only 0.9%. The coefficient of variation, marries these two metrics showing the return per unit of risk taken over the free rate. If we assume a 0% risk free rate, the investment with the highest Sharpe Ratio is the S&P at 0.11%.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Stats IV</span>"
    ]
  },
  {
    "objectID": "DescriptiveIV.html#portfolio-assesment",
    "href": "DescriptiveIV.html#portfolio-assesment",
    "title": "4  Descriptive Stats IV",
    "section": "4.7 Portfolio Assesment",
    "text": "4.7 Portfolio Assesment\nTo asses the performance of a portfolio calculate:\n\nThe mean return of the portfolio \\(\\alpha\\bar{R}_1+(1-\\alpha)\\bar{R}_2\\), where \\(\\alpha\\) is the weight of investment 1 in the portfolio and \\(\\bar{R}_i\\) is the average return of investment \\(i \\in\\){\\(1\\),\\(2\\)}.\nThe variance of the portfolio is given by \\(\\begin{bmatrix} \\alpha \\\\ 1-\\alpha \\end{bmatrix}^T \\begin{bmatrix} s_x^2 & s_{xy} \\\\ s_{xy} & s_y^2 \\end{bmatrix} \\begin{bmatrix} \\alpha \\\\ 1-\\alpha \\end{bmatrix}\\)\n\n\nUseful R Functions\nThe range() function returns the maximum and minimum of a vector of values.\nThe diff() function finds the first difference of a vector.\nThe var() function calculates the sample variance for a vector of values. To calculate the population variance, adjust the result by a factor of \\((n-1)/n\\).\nThe sd() function calculates the sample standard deviation.\nThe matrix() function defines a matrix.\nWhen dealing with matrices, the t() function transposes a vector or matrix, and the operator %*% performs matrix multiplication.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Stats IV</span>"
    ]
  },
  {
    "objectID": "DescriptiveIV.html#measures-of-dispersion-in-r",
    "href": "DescriptiveIV.html#measures-of-dispersion-in-r",
    "title": "4  Descriptive Stats IV",
    "section": "4.7 Measures of Dispersion in R",
    "text": "4.7 Measures of Dispersion in R\nLet’s use some stock returns to explore R functions that allow us to calculate measures of dispersion. You can run the following command to get the data into R:\n\nlibrary(tidyverse)\nreturns&lt;-read_csv(\"https://jagelves.github.io/Data/returns.csv\")\n\nUse the glimpse() fucntion to view the data:\n\nglimpse(returns)\n\nRows: 1,182\nColumns: 3\n$ date   &lt;chr&gt; \"1/3/20\", \"1/6/20\", \"1/7/20\", \"1/8/20\", \"1/9/20\", \"1/10/20\", \"1…\n$ Stock  &lt;chr&gt; \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\",…\n$ Return &lt;dbl&gt; -0.009769540, 0.007936685, -0.004714194, 0.015958317, 0.0210183…\n\n\nIt seems like the data is stock returns for different companies. We can start by finding the average return of each stock. To do this, let’s use the group_by() and summarise() functions.\n\nreturns %&gt;% group_by(Stock) %&gt;% \n  summarise(Mean=mean(Return))\n\n# A tibble: 3 × 2\n  Stock     Mean\n  &lt;chr&gt;    &lt;dbl&gt;\n1 AAPL  0.00173 \n2 SPY   0.000828\n3 TSLA  0.00511 \n\n\nThe group_by() function makes a group out of every unique entry in the Stock variable. Since, there are three unique entries (AAPL, SPY and TSLA), it created three groups. The summarise() function allows us to combine (or use) all of the entries of a particular stock to find a summary statistic. Since we specified mean(), the command returns the mean of the particular stock returns. It seems like TSLA has the highest mean return at 0.5%.\nWe can keep adding to the table by specifying other measures. To calculate the variance we can use the var() function, for the standard deviation we can use the sd() function, and for the coefficient of variation we can find the ratio between the standard deviation and the mean.\n\nreturns %&gt;% group_by(Stock) %&gt;% \n  summarise(Mean=mean(Return),\n            Variance=var(Return),\n            SD=sd(Return),\n            CV=SD/Mean*100)\n\n# A tibble: 3 × 5\n  Stock     Mean Variance     SD    CV\n  &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 AAPL  0.00173  0.000659 0.0257 1483.\n2 SPY   0.000828 0.000313 0.0177 2138.\n3 TSLA  0.00511  0.00252  0.0502  983.\n\n\nTSLA seems to have the highest variation when following the standard deviation. However, if we look at the variation as a percentage of the mean, SPY seems to have the highest variation for the period considered. In the table below we once more use the group_by() and summarise() functions to calculate the other measures of dispersion.\n\nreturns %&gt;% group_by(Stock) %&gt;% \n  summarise(Range=diff(range(Return)),\n            MAD=mean(abs(Return-mean(Return))),\n            Sharpe=(mean(Return)-0.0001)/sd(Return))\n\n# A tibble: 3 × 4\n  Stock Range    MAD Sharpe\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 AAPL  0.251 0.0179 0.0635\n2 SPY   0.203 0.0107 0.0411\n3 TSLA  0.418 0.0354 0.0998\n\n\nIn this second table the range is calculated by finding the difference between the minimum and the maximum. We can retrieve the minimum and the maximum by using the range() function. The diff() function finds the difference (or range) between these two numbers. The MAD is calculated straight from the formula. Mainly, finding the absolute deviations from the mean and the averaging them out. Lastly, the Sharpe ratio assumes a risk free daily rate of 0.01%. Once again, TSLA seems like the investment that yields the highest return despite its higher variability.\nBelow is a list of the functions used:\n\nThe range() function returns the maximum and minimum of a vector of values.\nThe diff() function finds the first difference of a vector. Position 2 minus position 1, position 3 minus position 2, position 4 minus position 3, etc.\nThe var() function calculates the sample variance for a vector of values. To calculate the population variance, adjust the result by a factor of \\((n-1)/n\\).\nThe sd() function calculates the sample standard deviation.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Stats IV</span>"
    ]
  },
  {
    "objectID": "DescriptiveV.html#quantiles-and-percentiles",
    "href": "DescriptiveV.html#quantiles-and-percentiles",
    "title": "5  Descriptive Stats V",
    "section": "",
    "text": "Sort the data in ascending order.\nCompute the location of the percentile desired using \\(L_{p}=\\frac{(n+1)P}{100}\\) where \\(L_{p}\\) is the location of the \\(P_{th}\\) percentile, and \\(P\\) is the percentile desired.\nThe value at \\(L_p\\), is the the \\(P_{th}\\) percentile.\n\n\n\nWe sort the data: \\(IQ_{sorted}=\\{75,80,90,100,110,130\\}\\)\nFind the location of the 25th percentile: \\(L_{25}=7 \\times 0.25=1.75\\). The 25th percentile is in the position 1.75 of the sorted data.\nRetrieve the 25th percentile: Since position 1 is 75 and position 2 is 80, the 25th percentile lies 0.75 of the way between position 1 and 2. Hence, the 25th percentile is \\(P_{25}=75+0.75(5)=78.75\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Stats V</span>"
    ]
  },
  {
    "objectID": "DescriptiveV.html#chevyshevs-theorem-and-empirical-rule",
    "href": "DescriptiveV.html#chevyshevs-theorem-and-empirical-rule",
    "title": "5  Descriptive Stats V",
    "section": "Chevyshev’s Theorem and Empirical Rule",
    "text": "Chevyshev’s Theorem and Empirical Rule\nChevyshev’s Theorem states that at least \\(1-1/z^2\\)% of the data lies between \\(z\\) standard deviations from the mean. This result does not depend on the shape of the distribution.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Stats V</span>"
    ]
  },
  {
    "objectID": "DescriptiveV.html#the-empirical-rule",
    "href": "DescriptiveV.html#the-empirical-rule",
    "title": "5  Descriptive Stats V",
    "section": "5.3 The Empirical Rule",
    "text": "5.3 The Empirical Rule\nWhereas Chevyshev’s theorem holds for any data distribution, the empirical rule is a bit more precise when looking at “bell shaped” data. Formally, the Empirical Rule or (\\(68\\),\\(95\\),\\(99.7\\) rule) states that \\(68\\)%, \\(95\\)%, and \\(99.7\\)% of the data lies between \\(1\\), \\(2\\), and \\(3\\) standard deviations from the mean respectively. The rule requires that the data be bell shape (normally) distributed.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Stats V</span>"
    ]
  },
  {
    "objectID": "DescriptiveV.html#skew",
    "href": "DescriptiveV.html#skew",
    "title": "5  Descriptive Stats V",
    "section": "5.5 Skew",
    "text": "5.5 Skew\nA measurement of skew, identifies asymmetry in the distribution of data. If most of the data leans towards one side, it’s skewed. If it leans to the left, it’s left-skewed or negatively skewed, meaning the tail on the left side is longer. If it leans to the right, it’s right-skewed or positively skewed, with a longer tail on the right. If the data is evenly distributed, it’s not skewed at all, it’s symmetric. To determine if the data is skewed, calculate the Pearson’s Coefficient of Skew. \\(Sk=\\frac{3(\\bar {x}- Median)}{s_x}\\). The distribution is skewed to the left if \\(Sk&lt;0\\), skewed to the right is \\(Sk&gt;0\\), and symmetric if \\(Sk=0\\).\nThe image below shows the different types of skew:\n\n\n\n\n\n\n\n\n\nEx: Assume that for a variable the mean is 10, the median is 8, and the standard deviation is 3. The pearson coefficient of skew is equal to \\(Sk=3(10-8)/2=3\\). Since the skew is positive, we expect the distribution to be right skewed.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Stats V</span>"
    ]
  },
  {
    "objectID": "DescriptiveV.html#five-point-summary",
    "href": "DescriptiveV.html#five-point-summary",
    "title": "5  Descriptive Stats V",
    "section": "5.6 Five Point Summary",
    "text": "5.6 Five Point Summary\nA popular way to summarize data is by calculating the minimum, first quartile, median, third quartile and maximum (five point summary). This gives us a good idea of how data is distributed. We can additionally inquire how the middle 50% of the data varies. Recall, that we can use a range to assess dispersion. The interquartile range (IQR) quantifies the dispersion of the middle 50% of the data. Formally, the IQR is the difference between the third quartile (75th percentile) and the first quartile (25th percentile).\nExample: Let’s use the IQ scores for a group of students once more. Recall that the data is given by \\(IQ=\\{80,100,110,75,130,90\\}\\). The minimum and the maximum are easily identified as \\(Max=130\\) and \\(Min=75\\). The first quartile (\\(P_{25}\\)) was calculated in 1.1 as 78.75. Using the same steps the third quartile (\\(P_{75}\\)) is 115. The median is the average between the third and fourth numbers \\(Median=190/2=95\\). The five point summary is given in the table below:\n\n\n\n\\(Min\\)\n\\(P_{25}\\)\n\\(Median\\)\n\\(P_{75}\\)\n\\(Max\\)\n\n\n\n\n75\n78.75\n95\n115\n130\n\n\n\nTo calculate the interquartile range we find the difference between the 75th and 25th percentiles. \\(IQR=115-78.75=36.25\\) which means that the middle 50% of the data has a range of 36.25.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Stats V</span>"
    ]
  },
  {
    "objectID": "DescriptiveV.html#outliers",
    "href": "DescriptiveV.html#outliers",
    "title": "5  Descriptive Stats V",
    "section": "5.4 Outliers",
    "text": "5.4 Outliers\nGiven the boundaries set by both the empirical rule and Chevyshev’s theorem, we can classify points as being common or normal and not common or outliers. Specifically, outliers are extreme deviations from the mean. They are values that are not “common” or rarely occurring. Since both the empirical rule and Chevyshev’s theorem state that a large proportion of the data is between three standard deviation, it would be uncommon to have a data point that is more that three standard deviations from the mean.\nTo calculate outliers we use a z-score, which is a measure of distance from the mean in units of standard deviation. It can be calculated for any data point in your variable by using the formula \\(z_{i}=\\frac{x_i-\\bar{x}}{s_x}\\). \\(z\\)-scores above \\(3\\) are suspected to be outliers.\nEx: On Jan 22, 2006 Kobe Bryant scored 81 points against the Toronto Raptors. He had averaged 30 point per game with a standard deviation of 4 points. If we calculate the z-score we get: \\(z_{81}=\\frac{81-30}{4}=12.5\\). This mean that 81 is 12.5 standard deviations away from the mean, making this value extremely rare.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Stats V</span>"
    ]
  },
  {
    "objectID": "DescriptiveV.html#box-plots-in-r",
    "href": "DescriptiveV.html#box-plots-in-r",
    "title": "5  Descriptive Stats V",
    "section": "5.10 Box Plots in R",
    "text": "5.10 Box Plots in R\nA box plot is a graph that shows the five point summary, outliers (if any), and the distribution of data. It can be easily constructed using geom_boxplot() in R. Let’s use the eruptions variable once more.\n\nlibrary(ggthemes)\nfaithful %&gt;% \n  ggplot() +\n  geom_boxplot(aes(y=eruptions),\n               fill=\"lightgrey\", alpha=0.5,\n               col=\"black\", width=0.3) +\n  theme_clean() +\n  scale_x_continuous(breaks = NULL, limits=c(-1,1))+\n  ylim(limits=c(0,6))\n\n\n\n\n\n\n\n\nThe boxplot highlights the minimum just below 2, the maximum around 5, the first quartile just above 2, the median at 4, and the third quartile just above 4. Any outlier would be shown as a point beyond the whiskers of the box plot.\nHere is a summary of the functions used in this section:\n\nThe quantile() function returns the five point summary when no arguments are specified. For a specific quantile, specify the probs argument.\nThe scale() function calculates the z-scores for a vector of values.\nThe geom_boxplot() command returns a box plot for a vector of values.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Stats V</span>"
    ]
  },
  {
    "objectID": "DescriptiveV.html#useful-r-functions",
    "href": "DescriptiveV.html#useful-r-functions",
    "title": "5  Descriptive Stats V",
    "section": "5.9 Useful R Functions",
    "text": "5.9 Useful R Functions\nThe quantile() function returns the five point summary when no arguments are specified. For a specific quantile, specify the probs argument.\nThe boxplot() command returns a box plot for a vector of values.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Stats V</span>"
    ]
  },
  {
    "objectID": "DescriptiveV.html#chevyshevs-theorem",
    "href": "DescriptiveV.html#chevyshevs-theorem",
    "title": "5  Descriptive Stats V",
    "section": "5.2 Chevyshev’s Theorem",
    "text": "5.2 Chevyshev’s Theorem\nChebyshev’s Theorem is an important theorem, as it helps you form an expectation of the proportion of data that must lie between a given standard deviation from the mean. This offers a baseline to understanding the range and distribution of your data, and aids in detecting outliers. Formally, Chevyshev’s Theorem states that regardless of the shape of the distribution, at least (\\(1-1/z^2\\))% of the data lies between \\(z\\) standard deviations from the mean.\nEx: For a given data set, we want to know at least how much of the data is between two standard deviations. Substituting 2 into Chevyshev’s formula yields $1-1/4=0.75. Hence, 75% of the data lies between two standard deviations from the mean.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Stats V</span>"
    ]
  },
  {
    "objectID": "DescriptiveV.html#outliers-1",
    "href": "DescriptiveV.html#outliers-1",
    "title": "5  Descriptive Stats V",
    "section": "5.7 Outliers",
    "text": "5.7 Outliers\nOutliers are extreme deviations from the mean. They are values that are not “normal”. To calculate outliers:\n\nUse a z-score to measure the distance from the mean in units of standard deviation. \\(z_{i}=\\frac{x_i-\\bar{x}}{s_x}\\). \\(z\\)-scores above \\(3\\) are suspected outliers.\nCalculate \\(Q_1-1.5(IQR)\\) and \\(Q_3+1.5(IQR)\\), where \\(Q_1\\) is the first quartile, \\(Q_3\\) is the third quartile, and \\(IQR\\) is the interquartile range. If \\(x_i\\) is less than \\(Q_1-1.5(IQR)\\) or greater than \\(Q_3+1.5(IQR)\\), then it is considered an outlier.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Stats V</span>"
    ]
  },
  {
    "objectID": "DescriptiveV.html#.quartiles",
    "href": "DescriptiveV.html#.quartiles",
    "title": "5  Descriptive Stats V",
    "section": "",
    "text": "Sort the data in ascending order.\nCompute the location of the percentile desired using \\(L_{p}=\\frac{(n+1)P}{100}\\) where \\(L_{p}\\) is the location of the \\(P_{th}\\) percentile, and \\(P\\) is the percentile desired.\nThe value at \\(L_p\\), is the the \\(P_{th}\\) percentile.\n\n\n\nWe sort the data: \\(IQ_{sorted}=\\{75,80,90,100,110,130\\}\\)\nFind the location of the 25th percentile: \\(L_{25}=7 \\times 0.25=1.75\\). The 25th percentile is in the position 1.75 of the sorted data.\nRetrieve the 25th percentile: Since position 1 is 75 and position 2 is 80, the 25th percentile lies 0.75 of the way between position 1 and 2. Hence, the 25th percentile is \\(P_{25}=75+0.75(5)=78.75\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Stats V</span>"
    ]
  },
  {
    "objectID": "DescriptiveV.html#outliers-z-scores",
    "href": "DescriptiveV.html#outliers-z-scores",
    "title": "5  Descriptive Stats V",
    "section": "5.4 Outliers Z-Scores",
    "text": "5.4 Outliers Z-Scores\nGiven the boundaries set by both the empirical rule and Chevyshev’s theorem, we can classify points as being common (normal) and not common (outliers). Specifically, outliers are extreme deviations from the mean. They are values that are not “common” or rarely occurring. Since both the empirical rule and Chevyshev’s theorem state that a large proportion of the data is between three standard deviations, it would be uncommon to have a data point that is more that three standard deviations away from the mean.\nTo identify outliers we use a z-score, which is a measure of distance from the mean in units of standard deviations. It can be calculated for any data point in your variable by using the formula \\(z_{i}=\\frac{x_i-\\bar{x}}{s_x}\\). By definition, \\(z\\)-scores above \\(3\\) are suspected to be outliers.\nEx: On Jan 22, 2006 Kobe Bryant scored 81 points against the Toronto Raptors. He had averaged 30 point per game with a standard deviation of 4 points. If we calculate the z-score we get: \\(z_{81}=\\frac{81-30}{4}=12.5\\). This mean that 81 is 12.5 standard deviations away from the mean, making this value extremely rare.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Stats V</span>"
    ]
  },
  {
    "objectID": "DescriptiveV.html#outliers-iqr",
    "href": "DescriptiveV.html#outliers-iqr",
    "title": "5  Descriptive Stats V",
    "section": "5.7 Outliers IQR",
    "text": "5.7 Outliers IQR\nAn alternate way to identify outliers is by using the interquartile range. Specifically, we first calculate \\(Q_1-1.5(IQR)\\) and \\(Q_3+1.5(IQR)\\), where \\(Q_1\\) is the first quartile, \\(Q_3\\) is the third quartile, and \\(IQR\\) is the interquartile range. If the observation (\\(x_i\\)) is less than \\(Q_1-1.5(IQR)\\) or greater than \\(Q_3+1.5(IQR)\\), then it is considered an outlier.\nEx: Consider once more the IQ data. \\(IQ=\\{80,100,110,75,130,90\\}\\). The lower limit for on outlier is given by \\(LL=Q_1-1.5(IQR)=78.75-1.5(36.25)\\) or \\(LL=24.375\\). The upper limit is given by \\(UL=Q_3+1.5(IQR)=115+1.5(36.25)\\) or \\(UL=169.375\\). Any data point outside the range [24.375,169.375] is considered an outlier. In other words, 200 and 20 would be outliers, but 100 would not.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Stats V</span>"
    ]
  },
  {
    "objectID": "DescriptiveV.html#quantiles-and-quartiles-in-r",
    "href": "DescriptiveV.html#quantiles-and-quartiles-in-r",
    "title": "5  Descriptive Stats V",
    "section": "5.8 Quantiles and Quartiles in R",
    "text": "5.8 Quantiles and Quartiles in R\nR quickly calculates quantiles for a given variable (vector) using the quantile() function. Below we use the IQ example once more.\n\nIQ &lt;- c(80,100,110,75,130,90)\nquantile(IQ, type=6)\n\n    0%    25%    50%    75%   100% \n 75.00  78.75  95.00 115.00 130.00 \n\n\nYou will notice that an extra argument type has been been passed into the quantile function. Since, there are several ways to calculate quantiles, R allows you to identify which method you want to use. In sec 1.1 we explained method 6.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Stats V</span>"
    ]
  },
  {
    "objectID": "DescriptiveV.html#outliers-in-r",
    "href": "DescriptiveV.html#outliers-in-r",
    "title": "5  Descriptive Stats V",
    "section": "5.9 Outliers in R",
    "text": "5.9 Outliers in R\nTo identify outliers we can us the scale() function. We’ll consider the first five observations in the faithful data set.\n\nhead(scale(faithful),5)\n\n    eruptions    waiting\n1  0.09831763  0.5960248\n2 -1.47873278 -1.2428901\n3 -0.13561152  0.2282418\n4 -1.05555759 -0.6544374\n5  0.91575542  1.0373644\n\n\nThe data shown above are z-scores for the first five observations of the faithful data set. As you can see non of the observations are outliers, as they are all less that 3 standard deviations away from the mean.\nIf we want to filter all observations that are say 1.3 standard deviations away from the mean, we can use the following command from tidyverse:\n\nlibrary(tidyverse)\nfaithful %&gt;% mutate(z_eruptions=scale(eruptions)) %&gt;%\n                      filter(scale(eruptions)&gt;1.3)\n\n  eruptions waiting z_eruptions\n1     5.067      76    1.383614\n2     5.100      96    1.412526\n3     5.033      77    1.353825\n4     5.000      88    1.324912\n\n\nThis confirms that there are no outliers in the eruptions variable.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Stats V</span>"
    ]
  }
]