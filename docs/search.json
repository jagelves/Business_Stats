[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Business Statistics",
    "section": "",
    "text": "Introduction\n“Whatever you would make habitual, practice it; and if you would not make a thing habitual, do not practice it, but accustom yourself to something else.” Epictetus\nThis course companion is designed to help you build mastery in statistics and its applications using R. Through practice, you will develop the skills and confidence needed to apply statistical concepts effectively. Each chapter begins with a list of key concepts to guide your learning, and the problems are crafted to reinforce these ideas through hands-on experience. If you need additional support while learning R, I encourage you to explore Grolemund (2014). Take your time, enjoy the process, and make practice a habit!",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#why-r",
    "href": "index.html#why-r",
    "title": "Business Statistics",
    "section": "Why R?",
    "text": "Why R?\nWe will be using R to apply the lessons we learn in BUAD 231. R is a language and environment for statistical computing and graphics. There are several advantages to using the R software for statistical analysis and data science. Some of the main benefits include:\n\nR is a powerful and flexible programming language that allows users to manipulate and analyze data in many different ways.\nR has a large and active community of users, who have developed a wide range of packages and tools for data analysis and visualization.\nR is free and open-source, which makes it accessible to anyone who wants to use it.\nR is widely used in academia and industry, which means that there are many resources and tutorials available to help users learn how to use it.\nR is well-suited for working with large and complex datasets, and it can handle data from many different sources.\nR can be easily integrated with other tools and software, such as databases, visualization tools, and machine learning algorithms.\n\nOverall, R is a powerful and versatile tool for data analysis and data science, and it offers many benefits to users who want to work with data.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#installing-r.",
    "href": "index.html#installing-r.",
    "title": "Business Statistics",
    "section": "Installing R.",
    "text": "Installing R.\nTo install R, visit the R webpage at https://www.r-project.org/. Once in the website, click on the CRAN hyperlink.\n\n\n\n\n\n\n\n\n\nHere you can select the CRAN mirror. Scroll down until you see USA. You are free to choose any mirror you like, I recommend using the Duke University mirror.\n\n\n\n\n\n\n\n\n\nOnce you click on the hyperlink, you will be prompted to choose the download for your operating system. Depending on your operating system, choose either a Windows or Macintosh download.\n\n\n\n\n\n\n\n\n\nFollow all prompts and complete installation.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#installing-rstudio",
    "href": "index.html#installing-rstudio",
    "title": "Business Statistics",
    "section": "Installing RStudio",
    "text": "Installing RStudio\nVisit the Posit website at https://posit.co. Once on the website, hover to the top of the screen and select “Open Source” from the drop down menus.\n\n\n\n\n\n\n\n\n\nNext, choose “R Studio IDE”.\n\n\n\n\n\n\n\n\n\nScroll down until you see the products. You want to download “RStudio Desktop” and make sure it is the free version.\n\n\n\n\n\n\n\n\n\nFinally, select “Download RStudio” and follow the instructions for installation.\n\n\n\n\n\n\n\n\n\nIt is important to note that RStudio will not work if R is not installed. You can think of R as the engine and RStudio as the interface.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#posit-cloud",
    "href": "index.html#posit-cloud",
    "title": "Business Statistics",
    "section": "Posit Cloud",
    "text": "Posit Cloud\nIf you do not wish to install R, you can always use the cloud version. To do this, visit https://posit.cloud/. On the main page click on the “Get Started” button.\n\n\n\n\n\n\n\n\n\nChoose the “Cloud Free” option and log in using your Google credentials (if you have a Google account) or sign up if you want to create a new account.\n\n\n\n\nGrolemund, Garret. 2014. “Hands-on Programming with r.” https://jjallaire.github.io/hopr/.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "DescriptiveI.html",
    "href": "DescriptiveI.html",
    "title": "1  Descriptive Stats I",
    "section": "",
    "text": "1.1 Motivation\nUnderstanding the nature and classification of data is crucial for effective analysis and decision-making. Data are the building blocks of insights, providing a foundation for businesses, researchers, and policymakers to make informed choices. Whether capturing a snapshot of a specific moment, tracking changes over time, or organizing information in structured or unstructured formats, how data is collected and categorized significantly impacts how it is analyzed and interpreted. This overview highlights key types of data and their unique characteristics to help you better understand their application in various contexts.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Descriptive Stats I</span>"
    ]
  },
  {
    "objectID": "DescriptiveI.html#data-and-types-of-data",
    "href": "DescriptiveI.html#data-and-types-of-data",
    "title": "1  Descriptive Stats I",
    "section": "1.2 Data and Types of Data",
    "text": "1.2 Data and Types of Data\nData are facts and figures collected, analyzed and summarized for presentation and interpretation. Data can be classified as:\n\nCross Sectional Data refers to data collected at the same (or approximately the same) point in time. Ex: NFL standings in 1980 or Country GDP in 2015.\nTime Series Data refers to data collected over several time periods. Ex: U.S. inflation rate from 2000-2010 or Tesla deliveries from 2016-2022.\nStructured Data resides in a predefined row-column format (tidy). Ex: spreadsheet data.\nUnstructured Data do not conform to a pre-defined row-column format. Ex: Text, video, and other multimedia.\n\nExample: Consider a retail store analyzing its sales performance. If the store collects data on the total revenue generated by each location on Black Friday, it is cross-sectional data. On the other hand, if the store tracks weekly sales for the past year to observe trends, it is time series data. Structured data, like sales figures stored in spreadsheets, allows for easy comparison and analysis. Meanwhile, customer feedback gathered from social media posts and video reviews represents unstructured data, requiring advanced tools to extract meaningful insights.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Descriptive Stats I</span>"
    ]
  },
  {
    "objectID": "DescriptiveI.html#data-sets",
    "href": "DescriptiveI.html#data-sets",
    "title": "1  Descriptive Stats I",
    "section": "1.3 Data Sets",
    "text": "1.3 Data Sets\nA data set contains all data collected for a particular study. Data sets are composed of:\n\nElements are the entities on which data are collected. Ex: Football teams, countries, and individuals.\nVariables are a set of characteristics collected for each element. Ex: Goals scored, GDP, weight.\nObservations are the set of measurements obtained for a particular element. Ex: Salah, 20 (goals), 15 (assists). US, 2.3 (inflation), 4.5% (federal interest rate).\n\n\n\n\nElements\nVariable 1\nVariable 2\n\n\n\n\nElement 1\n#\n#\n\n\nElement 2\n#\n#\n\n\nElement 3\n#\n#\n\n\n…\n…\n…\n\n\n\nExample: Consider the dataset on electric vehicles (EV’s) displayed below:\n\n\n\n\n\n\n\n\n\nIn this dataset, each row represents an electric vehicle model, making the elements the specific EV models rather than the manufacturers. The variables collected for each model include:\n\nMake: The manufacturer of the EV.\nModel: The specific name of the EV model.\nRange_km: Driving range in kilometers on a full charge.\nTopSpeed_kmh: Maximum speed in km/h.\nPrice_pounds: Price in pounds (£).\nCharge_kmh: Charging speed in kilometers per hour.\n\nAn example observation is “Tesla Model 3,” with the following data: Make: Tesla, Model: Model 3, Range_km: 415, TopSpeed_kmh: 201, Price_pounds: 39,990, Charge_kmh: 690.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Descriptive Stats I</span>"
    ]
  },
  {
    "objectID": "DescriptiveI.html#scales-of-measurement",
    "href": "DescriptiveI.html#scales-of-measurement",
    "title": "1  Descriptive Stats I",
    "section": "1.4 Scales of Measurement",
    "text": "1.4 Scales of Measurement\nUnderstanding scales of measurement is crucial for analyzing and interpreting data effectively in business. By distinguishing between categorical (e.g., marital status, satisfaction ratings) and numerical data (e.g., profits, prices), you’ll know what methods to use for analysis. Knowing whether data is nominal, ordinal, interval, or ratio ensures your analysis and conclusions are accurate and relevant.\nThe scales of measurements determine the amount and type of information contained in each variable. In general, variables can be classified as categorical or numerical.\n\nCategorical (qualitative) data includes labels or names to identify an attribute of each element. Categorical data can be nominal or ordinal.\n\nWith nominal data, the order of the categories is arbitrary. Ex: Marital Status, Race/Ethnicity, or NFL division.\nWith ordinal data, the order or rank of the categories is meaningful. Ex: Rating, Difficulty Level, or Spice Level.\n\nNumerical (quantitative) include numerical values that indicate how many (discrete) or how much (continuous). The data can be either interval or ratio.\n\nWith interval data, the distance between values is expressed in terms of a fixed unit of measure. The zero value is arbitrary and does not represent the absence of the characteristic. Ratios are not meaningful. Ex: Temperature or Dates.\nWith ratio data, the ratio between values is meaningful. The zero value is not arbitrary and represents the absence of the characteristic. Ex: Prices, Profits, Wins.\n\n\nExample: Let’s keep using the EV example. Consider the new data set below:\n\n\n\n\n\n\n\n\n\nThe variables can be classified as follows: Car (Categorical - Nominal), consists of names of cars, which are labels used to identify each row. The order of these names does not matter, making it nominal data. Brand (Categorical - Nominal) represents the manufacturer of the car (e.g., Ford, Audi). These are labels with no inherent order, making it nominal data. Range (Numerical - Ratio), refers to the car’s driving range in miles. It is numerical and ratio because it has a meaningful zero (a car with zero range cannot move), and ratios are meaningful (e.g., a car with 250 miles range has double the range of one with 125 miles). Rating (Categorical - Ordinal) represents a rank or score (e.g., 4, 3, 2). The order matters, as higher ratings indicate better performance. However, the intervals between ratings are not consistent, so it is ordinal data. Year (Numerical - Interval) represents a point in time. While numerical, it is interval data because the zero point is arbitrary (e.g., year 0 does not indicate the “absence” of time), and ratios are not meaningful (e.g., 2020 is not “twice as late” as 1010).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Descriptive Stats I</span>"
    ]
  },
  {
    "objectID": "DescriptiveI.html#useful-base-r-functions",
    "href": "DescriptiveI.html#useful-base-r-functions",
    "title": "1  Descriptive Stats I",
    "section": "1.5 Useful Base R Functions",
    "text": "1.5 Useful Base R Functions\nUnderstanding and using Base R functions is essential for efficiently managing and analyzing data. Functions like na.omit() help clean datasets by removing rows with missing values, ensuring your analyses are accurate and complete. nrow() and ncol() quickly provide insights into the size of your dataset, while is.na() allows you to identify and address missing data. The summary() function is a powerful way to generate descriptive statistics and assess the overall structure of your data at a glance. Additionally, coercion functions like as.integer(), as.factor(), and as.double() enable you to convert variables to appropriate data types, ensuring compatibility with different analysis methods.\n\nThe na.omit() function removes any observations that have a missing value (NA). The resulting data frame has only complete cases. Input: A data frame (tibble) or vector.\nThe nrow() and ncol() functions return the number of rows and columns respectively from a data frame. Input: A data frame (tibble).\nThe is.na() function returns a vector of True and False that specify if an entry is missing (NA) or not. Input: A data frame (tibble) or vector.\nThe summary() function returns a collection of descriptive statistics from a data frame (or vector). The function also returns whether there are any missing values (NA) in a variable. Input: A data frame (tibble) or vector.\nThe as.integer(), as.factor(), as.double(), are functions used to coerce your data into a different scale of measurement. Input: A vector or column of a data frame (tibble).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Descriptive Stats I</span>"
    ]
  },
  {
    "objectID": "DescriptiveI.html#useful-dplyr-functions",
    "href": "DescriptiveI.html#useful-dplyr-functions",
    "title": "1  Descriptive Stats I",
    "section": "1.6 Useful DPLYR Functions",
    "text": "1.6 Useful DPLYR Functions\nThe dplyr package has a collection of functions that are useful for data manipulation and transformation. If you are interested in this package you can refer to Wickham (2017). To install, run the following command in the console install.packages(\"dplyr\").\n\nThe arrange() function allows you to sort data frames in ascending order. Pair with the desc() function to sort the data in descending order.\nThe filter() function allows you to subset the rows of your data based on a condition.\nThe select() function allows you to select a subset of variables from your data frame.\nThe mutate() function allows you to create a new variable.\nThe group_by() function allows you to group your data frame by categories present in a given variable.\nThe summarise() function allows you to summarise your data, based on groupings generated by the goup_by() function.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Descriptive Stats I</span>"
    ]
  },
  {
    "objectID": "DescriptiveI.html#exercises",
    "href": "DescriptiveI.html#exercises",
    "title": "1  Descriptive Stats I",
    "section": "1.7 Exercises",
    "text": "1.7 Exercises\nThe following exercises will help you test your knowledge on the Scales of Measurement. They will also allow you to practice some basic data “wrangling” in R. In these exercises you will:\n\nIdentify numerical and categorical data.\nClassify data according to their scale of measurement.\nSort and filter data in R.\nHandle missing values (NA’s) in R.\n\nAnswers are provided below. Try not to peak until you have a formulated your own answer and double checked your work for any mistakes.\n\nExercise 1\nA bookstore has compiled data set on their current inventory. A portion of the data is shown below:\n\n\n\nTitle\nPrice\nYear Published\nRating\n\n\n\n\nFrankenstein\n5.49\n1818\n4.2\n\n\nDracula\n7.60\n1897\n4.0\n\n\n…\n…\n…\n…\n\n\nSleepy Hollow\n6.95\n1820\n3.8\n\n\n\n\nWhich of the above variables are categorical and which are numerical?\n\n\n\nAnswer\n\nThe “Title” variable represents the names of books. Therefore, this is a categorical variable. “Price” represents the cost of each book in a numeric format, making it a numerical variable. “Year Published” indicates the publication year of each book. It is numerical. If “Rating” represents a numerical score based on a continuous scale (e.g., average user ratings on a platform like Goodreads), it is numerical because arithmetic operations like averaging or comparing differences are meaningful. If “Rating” represents predefined categories (e.g., “Excellent,” “Good,” “Fair,” “Poor”) or is interpreted as ranks without meaningful differences between values, it would be categorical.\n\n\nWhat is the measurement scale of each of the above variable?\n\n\n\nAnswer\n\nThe measurement scale is nominal for Title since these are labels used to identify each book and do not have a numerical meaning or order. If Rating represents a score (e.g., 4.2, 4.0) given to each book, it is numerical and could be considered interval data because the scale represents a meaningful difference, but it may not have an absolute zero or meaningful ratios (e.g., a book rated 4.0 is not “twice as good” as one rated 2.0). Price is a measurable quantity with a meaningful zero (e.g., a book priced at $0 means it is free), making it ratio data. Year is interval data because the zero point is arbitrary (year 0 does not represent the absence of time) and differences between years are meaningful (e.g., 1897 - 1818 = 79 years).\n\n\n\nExercise 2\nA car company tracks the number of deliveries every quarter. A portion of the data is shown below:\n\n\n\nYear\nQuarter\nDeliveries\n\n\n\n\n2016\n1\n14800\n\n\n2016\n2\n14400\n\n\n…\n…\n…\n\n\n2022\n3\n343840\n\n\n\n\nWhat is the measurement scale of the Year variable? What are the strengths and weaknesses of this type of measurement scale?\n\n\n\nAnswer\n\nThe variable Year is measured on the interval scale because the observations can be ranked, categorized and measured when using this kind of scale. However, there is no true zero point so we cannot calculate meaningful ratios between years.\n\n\nWhat is the measurement scale for the Quarter variable? What is the weakness of this type of measurement scale?\n\n\n\nAnswer\n\nThe variable Quarter is measured on the ordinal scale, even though it contains numbers. It is the least sophisticated level of measurement because if we are presented with nominal data, all we can do is categorize or group the data.\n\n\nWhat is the measurement scale for the Deliveries variable? What are the strengths of this type of measurement scale?\n\n\n\nAnswer\n\nThe variable Deliveries is measured on the ratio scale. It is the strongest level of measurement because it allows us to categorize and rank the data as well as find meaningful differences between observations. Also, with a true zero point, we can interpret the ratios between observations.\n\n\n\nExercise 3\nUse the airquality data set included in R for this problem.\n\nSort the data by Temp in descending order. What is the day and month of the first observation on the sorted data?\n\n\n\nAnswer\n\nThe day and month of the first observation is August 28th.\nThe easiest way to sort in R is by using the dplyr package. Specifically, the arrange() function within the package. Let’s also use the desc() function to make sure that the data is sorted in descending order. We can use indexing to retrieve the first row of the sorted data set.\n\nlibrary(dplyr)\nSortedAQ&lt;-arrange(airquality,desc(Temp))\nSortedAQ[1,]\n\n  Ozone Solar.R Wind Temp Month Day\n1    76     203  9.7   97     8  28\n\n\n\n\nSort the data only by Temp in descending order. Of the \\(10\\) hottest days, how many of them were in July?\n\n\n\nAnswer\n\nWe can use the arrange() function one more time for this question. Then we can use indexing to retrieve the top \\(10\\) observations.\n\nSortedAQ2&lt;-arrange(airquality,desc(Temp))\nSortedAQ2[1:10,]\n\n   Ozone Solar.R Wind Temp Month Day\n1     76     203  9.7   97     8  28\n2     84     237  6.3   96     8  30\n3    118     225  2.3   94     8  29\n4     85     188  6.3   94     8  31\n5     NA     259 10.9   93     6  11\n6     73     183  2.8   93     9   3\n7     91     189  4.6   93     9   4\n8     NA     250  9.2   92     6  12\n9     97     267  6.3   92     7   8\n10    97     272  5.7   92     7   9\n\n\n\n\nHow many missing values are there in the data set? What rows have missing values for Solar.R?\n\n\n\nAnswer\n\nThere are a total of \\(44\\) missing values. Ozone has \\(37\\) and Solar.R has \\(7\\). Rows \\(5\\), \\(6\\), \\(11\\), \\(27\\), \\(96\\), \\(97\\), \\(98\\) are missing for Solar.R.\nWe can easily identify missing values with the summary() function.\n\nsummary(airquality)\n\n     Ozone           Solar.R           Wind             Temp      \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n NA's   :37       NA's   :7                                       \n     Month            Day      \n Min.   :5.000   Min.   : 1.0  \n 1st Qu.:6.000   1st Qu.: 8.0  \n Median :7.000   Median :16.0  \n Mean   :6.993   Mean   :15.8  \n 3rd Qu.:8.000   3rd Qu.:23.0  \n Max.   :9.000   Max.   :31.0  \n                               \n\n\nTo view the rows that have NA’s in them, we can use the is.na() function and indexing. Below we see that \\(7\\) values are missing for the Solar.R variable in the months \\(5\\) and \\(8\\) combined.\n\nairquality[is.na(airquality$Solar.R),]\n\n   Ozone Solar.R Wind Temp Month Day\n5     NA      NA 14.3   56     5   5\n6     28      NA 14.9   66     5   6\n11     7      NA  6.9   74     5  11\n27    NA      NA  8.0   57     5  27\n96    78      NA  6.9   86     8   4\n97    35      NA  7.4   85     8   5\n98    66      NA  4.6   87     8   6\n\n\n\n\nRemove all observations that have a missing values. Create a new object called CompleteAG.\n\n\n\nAnswer\n\nTo create the new object of complete observations we can use the na.omit() function.\n\nCompleteAQ&lt;-na.omit(airquality)\n\n\n\nWhen using CompleteAG, how many days was the temperature at least \\(60\\) degrees?\n\n\n\nAnswer\n\nThere were \\(107\\) days where the temperature was at least \\(60\\).\nUsing base R we have:\n\nnrow(CompleteAQ[CompleteAQ$Temp&gt;=60,])\n\n[1] 107\n\n\nWe can also use dplyr for this question. Specifically, using the filter() and nrow() functions we get:\n\nnrow(filter(CompleteAQ,Temp&gt;=60))\n\n[1] 107\n\n\n\n\nWhen using CompleteAG, how many days was the temperature within [\\(55\\),\\(75\\)] degrees and an Ozone below \\(20\\)?\n\n\n\nAnswer\n\nThere were \\(24\\) days where the temperature was between \\(55\\) and \\(75\\) and the ozone level was below \\(20\\).\nUsing base R we have:\n\nnrow(CompleteAQ[CompleteAQ$Temp&gt;55 & CompleteAQ$Temp&lt;75 & CompleteAQ$Ozone&lt;20,])\n\n[1] 24\n\n\nUsing the filter() function once more we get:\n\nnrow(filter(CompleteAQ,Temp&gt;55,Temp&lt;75,Ozone&lt;20))\n\n[1] 24\n\n\n\n\n\nExercise 4\nUse the Packers data set for this problem. You can find the data set at https://jagelves.github.io/Data/Packers.csv\n\nRemove the any observation that has a missing value with the na.omit() function. How many observations are left in the data set?\n\n\n\nAnswer\n\nThere are \\(84\\) observations in the complete cases data set.\nLet’s import the data to R by using the read.csv() function.\n\nPackers&lt;-read.csv(\"https://jagelves.github.io/Data/Packers.csv\")\n\nWe can remove any missing observation by using the na.omit() function. We can name this new object Packers2.\n\nPackers2&lt;-na.omit(Packers)\n\nTo find the number of observations we can use the dim() function. It returns the number of observations and variables.\n\ndim(Packers2)\n\n[1] 84  8\n\n\n\n\nDetermine the type of the Experience variable by using the typeof() function. What type is the variable?\n\n\n\nAnswer\n\nThe type is character.\nUse the typeof() function on the Experience variable.\n\ntypeof(Packers2$Experience)\n\n[1] \"character\"\n\n\n\n\nRemove observations that have an “R” and coerce the Experience variable to an integer using the as.integer() function. What is the total sum of years of experience?\n\n\n\nAnswer\n\nThe total sum of experience is \\(288\\).\nFirst, remove any observation with an R by using indexing and logicals.\n\nPackers2&lt;-Packers2[Packers2$Experience!=\"R\",]\n\nNow we can coerce the variable to an integer by using the as.integer() function.\n\nPackers2$Experience&lt;-as.integer(Packers2$Experience)\n\nLastly, calculate the sum using the sum() function.\n\nsum(Packers2$Experience)\n\n[1] 288\n\n\n\n\n\n\n\nWickham, Hadley. 2017. “R for Data Science.” https://r4ds.hadley.nz.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Descriptive Stats I</span>"
    ]
  },
  {
    "objectID": "DescriptiveII.html",
    "href": "DescriptiveII.html",
    "title": "2  Descriptive Stats II",
    "section": "",
    "text": "2.1 Frequency Distributions (Categorical)\nUnderstanding and visualizing data distribution is a fundamental step in data analysis. A frequency distribution organizes a structured data summary into non-overlapping classes, allowing for insights into patterns and trends. Complementary to this, relative frequency, cumulative frequency, and cumulative relative frequency offer deeper perspectives on the proportions and accumulation of data within these classes. Visualization techniques, including bar plots and histograms, play a crucial role in representing these distributions, with bar plots suited for qualitative data and histograms tailored for quantitative data. The R package ggplot2, has functions like geom_bar() and geom_hist() to plot distributions efficiently. By leveraging these methods, data can be transformed into clear and meaningful insights.\nA frequency distribution is perhaps the most valuable tool for summarizing categorical data. It illustrates with a table the number of items within distinct, non-overlapping categories. Presenting the data in a tabular format makes it easier to identify patterns and trends within the categories. A key component of this analysis is the relative frequency, which quantifies the proportion of items in each category relative to the total number of observations. You can calculate it by taking the frequency of a particular class (\\(f_{i}/n\\)), and dividing it by the total frequency \\(n\\). Relative frequency helps contextualize the data by highlighting the significance of each category compared to the whole.\nExample: Consider data on students’ answers to the question, what is your favorite food? You can see the data below:\nSimply observing raw data can make identifying the most and least popular items challenging. A frequency distribution organizes this information into a clear table, showcasing the popularity of each item. The frequency distribution of the table is displayed below:\nFood\nFrequency\nRelative\n\n\n\n\nChicken\n5\n0.20\n\n\nPasta\n4\n0.16\n\n\nPizza\n6\n0.24\n\n\nSushi\n10\n0.40\nEach food item is tallied up, and the result is shown in the Frequency column. We can also show the tally result as a ratio of the total food items recorded in the data (i.e., 25). For example, five students liked chicken; out of the 25 students surveyed, this represents 0.2 or 20%. This approach makes it much easier to pinpoint the most popular and the least popular items. Below, you can see the bar graph showing the frequency distribution of the food items data. Note that the visualization is constructed by showing each food item as a bar with a height equal to the frequency.\nIn sum, the bar plot illustrates the frequency distribution of categorical data. It includes the classes in the horizontal axis and frequencies or relative frequencies in the vertical axis and has gaps between each bar.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Stats II</span>"
    ]
  },
  {
    "objectID": "DescriptiveII.html#frequency-distributions-numerical",
    "href": "DescriptiveII.html#frequency-distributions-numerical",
    "title": "2  Descriptive Stats II",
    "section": "2.2 Frequency Distributions (Numerical)",
    "text": "2.2 Frequency Distributions (Numerical)\n\nThe cumulative frequency shows the number of data items with values less than or equal to the upper class limit of each class.\nThe cumulative relative frequency is given by \\(cf_{i}/n\\), where \\(cf_{i}\\) is the cumulative frequency of class \\(i\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Stats II</span>"
    ]
  },
  {
    "objectID": "DescriptiveII.html#plots-using-ggplot2",
    "href": "DescriptiveII.html#plots-using-ggplot2",
    "title": "2  Descriptive Stats II",
    "section": "2.3 Plots Using ggplot2",
    "text": "2.3 Plots Using ggplot2\nA bar plot illustrates the frequency distribution of qualitative data.\n\nIs an illustration for qualitative data.\nIncludes the classes in the horizontal axis and frequencies or relative frequencies in the vertical axis.\nHas gaps between each bar.\n\nA histogram illustrates the frequency distribution of quantitative data.\n\nIs an illustration for quantitative data.\nThere are no gaps between the bars.\nThe number, width and limits of each class must be determined.\n\nThe number of classes can be determined by the \\(2^k\\) rule: select \\(k\\) such that \\(2^k\\) is greater than the number of observations by the smallest amount.\nThe width of the class is approximately range/(# of Classes). The value should be rounded up.\nThe limits should be chosen so that each point belongs to only one class.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Stats II</span>"
    ]
  },
  {
    "objectID": "DescriptiveII.html#useful-r-functions",
    "href": "DescriptiveII.html#useful-r-functions",
    "title": "2  Descriptive Stats II",
    "section": "Useful R Functions",
    "text": "Useful R Functions\nThe table() command generates frequency distributions or contingency tables if two variables are used.\nThe prop.table() command generates relative frequency distributions from an object that contains a table.\nThe cut() function generates class limits and bins used in frequency distributions (and histograms) for quantitative data.\nBase R has the barplot() function for categorical variable, histogram() function for numerical data, and the plot() function for line charts or scatter plots. Below are some arguments that are helpful when plotting.\n\nmain: used to set the plot’s title. The title should be entered as a character.\ncol: used to set the color of the plot. Hex and RGB values are allowed as inputs. The color should be entered as a character.\nxlab and ylab: are used to set the labels for the \\(x\\) and \\(y\\) axis respectively. The labels should be entered as characters.\nlegend() is a function to customize the legend of a graph. This argument may be used with the plot(), barplot() or histogram() functions.\n\nx: used to set the location of the legend in the plotting area. Ex: “bottomleft”.\nlegend: a vector specifying the legend names to be included.\ncol: a vector specifying the color of each item in the legend.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Stats II</span>"
    ]
  },
  {
    "objectID": "DescriptiveII.html#exercises",
    "href": "DescriptiveII.html#exercises",
    "title": "2  Descriptive Stats II",
    "section": "2.4 Exercises",
    "text": "2.4 Exercises\nThe following exercises will help you practice summarizing data with tables and simple graphs. In particular, the exercises work on:\n\nDeveloping frequency distributions for both categorical and numerical data.\nConstructing bar charts, histograms, and line charts.\nCreating contingency tables.\n\nAnswers are provided below. Try not to peak until you have a formulated your own answer and double checked your work for any mistakes.\n\nExercise 1\nInstall the ISLR2 package in R. You will need the BrainCancer data set to answer this question.\n\nConstruct a frequency and relative frequency table of the Diagnosis variable. What was the most common diagnosis? What percentage of the sample had this diagnosis?\n\n\n\nAnswer\n\nThe most common diagnosis is Meningioma, a slow-growing tumor that forms from the membranous layers surrounding the brain and spinal cord. The diagnosis represents about \\(48.28\\)% of the sample.\nStart by loading the ISLR2 package. To construct the frequency distribution table, use the table() function.\n\nlibrary(ISLR2)\ntable(BrainCancer$diagnosis)\n\n\nMeningioma  LG glioma  HG glioma      Other \n        42          9         22         14 \n\n\nThe relative frequency distribution can be easily retrieved by saving the frequency table in an object and then using the prop.table() function.\n\nfreq&lt;-table(BrainCancer$diagnosis)\nprop.table(freq)\n\n\nMeningioma  LG glioma  HG glioma      Other \n 0.4827586  0.1034483  0.2528736  0.1609195 \n\n\n\n\nConstruct a bar chart. Summarize the findings.\n\n\n\nAnswer\n\nThe majority of diagnosis are Meningioma. Low grade glioma is the least common of diagnosis. High grade glioma and other diagnosis have about the same frequency.\nTo construct the bar chart use the geom_bar() function from tidyverse.\n\nlibrary(tidyverse)\nlibrary(ggthemes)\nggplot(data=BrainCancer) + \n  geom_bar(aes(diagnosis), alpha=0.5, col=\"black\") + \n  theme_clean()\n\n\n\n\n\n\n\n\n\n\nConstruct a contingency table that shows the Diagnosis along with the Status. Which diagnosis had the highest number of non-survivals (0)? What was the survival rate of this diagnosis?\n\n\n\nAnswer\n\n\\(33\\) people did not survive Meningioma. The survival rate of Meningioma is only \\(21.43\\)%.\nUse the table() function one more time to create the contingency table for the two variables.\n\n(freq2&lt;-table(BrainCancer$status,BrainCancer$diagnosis))\n\n   \n    Meningioma LG glioma HG glioma Other\n  0         33         5         5     9\n  1          9         4        17     5\n\n\nTo get the survival rates, we can use the prop.table() function once again.\n\nprop.table(freq2,margin = 2)\n\n   \n    Meningioma LG glioma HG glioma     Other\n  0  0.7857143 0.5555556 0.2272727 0.6428571\n  1  0.2142857 0.4444444 0.7727273 0.3571429\n\n\n\n\n\nExercise 2\nYou will need the airquality data set (in base R) to answer this question.\n\nConstruct a frequency distribution for Temp. Use five intervals with widths of \\(50&lt;x\\le60\\); \\(60&lt;x\\le70\\); etc. Which interval had the highest frequency? How many times was the temperature between \\(50\\) and \\(60\\) degrees?\nConstruct a relative frequency, cumulative frequency and the relative cumulative frequency distributions. What proportion of the time was Temp between \\(50\\) and \\(60\\) degrees? How many times was the Temp \\(70\\) degrees or less? What proportion of the time was Temp more than \\(70\\) degrees?\nConstruct the histogram. Is the distribution symmetric? If not, is it skewed to the left or right?\n\n\n\nExercise 3\nYou will need the Portfolio data set from the ISLR2 package to answer this question.\n\nConstruct a line chart that shows the returns over time for each portfolio (X and Y) by using two lines each with a unique color. Assume the data is for the period \\(1901\\) to \\(2000\\). Include also a legend that matches colors to portfolios.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Stats II</span>"
    ]
  },
  {
    "objectID": "DescriptiveII.html#answers",
    "href": "DescriptiveII.html#answers",
    "title": "2  Descriptive Stats II",
    "section": "2.5 Answers",
    "text": "2.5 Answers\n\nExercise 1\n\nThe most common diagnosis is Meningioma, a slow-growing tumor that forms from the membranous layers surrounding the brain and spinal cord. The diagnosis represents about \\(48.28\\)% of the sample.\n\nStart by loading the ISLR2 package. To construct the frequency distribution table, use the table() function.\n\nlibrary(ISLR2)\ntable(BrainCancer$diagnosis)\n\n\nMeningioma  LG glioma  HG glioma      Other \n        42          9         22         14 \n\n\nThe relative frequency distribution can be easily retrieved by saving the frequency table in an object and then using the prop.table() function.\n\nfreq&lt;-table(BrainCancer$diagnosis)\nprop.table(freq)\n\n\nMeningioma  LG glioma  HG glioma      Other \n 0.4827586  0.1034483  0.2528736  0.1609195 \n\n\n\nThe majority of diagnosis are Meningioma. Low grade glioma is the least common of diagnosis. High grade glioma and other diagnosis have about the same frequency.\n\nTo construct the bar chart use the barplot() function in R.\n\nbarplot(freq, col = \"#F5F5F5\", ylim=c(0,50))\n\n\n\n\n\n\n\n\n\n\\(33\\) people did not survive Meningioma. The survival rate of Meningioma is only \\(21.43\\)%.\n\nUse the table() function one more time to create the contingency table for the two variables.\n\n(freq2&lt;-table(BrainCancer$status,BrainCancer$diagnosis))\n\n   \n    Meningioma LG glioma HG glioma Other\n  0         33         5         5     9\n  1          9         4        17     5\n\n\nTo get the survival rates, we can use the prop.table() function once again.\n\nprop.table(freq2,margin = 2)\n\n   \n    Meningioma LG glioma HG glioma     Other\n  0  0.7857143 0.5555556 0.2272727 0.6428571\n  1  0.2142857 0.4444444 0.7727273 0.3571429\n\n\n\nMeningioma and not surviving is the most common with \\(33\\) occurrences. High grade glioma and surviving is the the second most common.\n\nUse the barplot() function one more time to construct the stacked column chart.\n\nbarplot(table(BrainCancer$status,BrainCancer$diagnosis),\n        legend.text = c(\"Not Survived\",\"Survived\"), ylim=c(0,50))\n\n\n\n\n\n\n\n\n\n\nExercise 2\n\nThe highest frequency is in the \\(80 &lt; x ≤ 90\\) bin. \\(8\\) temperatures were between \\(50 &lt; x ≤ 60\\) degrees.\n\nCreate a vector containing the intervals desired by using the seq() function.\n\nintervals &lt;- seq(50, 100, by=10)\n\nNext use the cut() function to create the cuts for the histogram.\n\nintervals.cut &lt;- cut(airquality$Temp, intervals, left=FALSE, right=TRUE)\n\nThe frequency distribution can be obtained by using the table() function on the interval.cut object created above.\n\ntable(intervals.cut)\n\nintervals.cut\n (50,60]  (60,70]  (70,80]  (80,90] (90,100] \n       8       25       52       54       14 \n\n\n\nThe temperature was \\(5.22\\)% of the time between \\(50\\) and \\(60\\); The temperature was \\(70\\) or less \\(33\\) times; The temperature was above \\(70\\), \\(78.43\\)% of the time.\n\nTo get the relative frequency table, start by saving the proportion table into an object.Then you can use the prop.table() function.\n\nfreq&lt;-table(intervals.cut) \nprop.table(freq)\n\nintervals.cut\n   (50,60]    (60,70]    (70,80]    (80,90]   (90,100] \n0.05228758 0.16339869 0.33986928 0.35294118 0.09150327 \n\n\nFor the cumulative distribution you can use the cumsum() function on the frequency distribution.\n\ncumulfreq&lt;-cumsum(freq)\ncumulfreq\n\n (50,60]  (60,70]  (70,80]  (80,90] (90,100] \n       8       33       85      139      153 \n\n\nLastly, for the relative cumulative distribution table, you can use the cumsum() function on the relative frequency table.\n\ncumsum(prop.table(freq))\n\n   (50,60]    (60,70]    (70,80]    (80,90]   (90,100] \n0.05228758 0.21568627 0.55555556 0.90849673 1.00000000 \n\n\n\nThe distribution is not perfectly symmetric. It is skewed slightly to the left (see histogram.)\n\nUse the hist() function to create the histogram.\n\nhist(airquality$Temp, breaks=intervals, \n     right=TRUE,col=\"#F5F5F5\", main=\"Temperature in NY\", xlab=\"\")\n\n\n\n\n\n\n\n\n\n\nExercise 3\n\nFrom \\(1901\\) through \\(2000\\), both portfolios have behaved very similarly. Returns are between \\(-3\\)% and \\(3\\)%, there is no trend, and positive (negative) returns for X seem to match with positive (negative) returns of Y.\n\nYou can use the plot() function to create a plot of Portfolio Y. The line for Portfolio X can be added with the lines() function.\n\nplot(Portfolio$Y, \n     x=seq(1901,2000), type=\"l\", \n     col=\"black\", xlab=\"\", ylab=\"% Return\", ylim=c(-3,3), \n     xlim=c(1901,2000), lwd=2, axes = F)\naxis(side=1, labels=TRUE, font=1,las=1)\naxis(side=2, labels=TRUE, font=1,las=1)\nlines(Portfolio$X, x=seq(1901,2000), type=\"l\", \n      col=\"darkgrey\", lwd=2)\nlegend(x = \"bottomleft\",          \n       legend = c(\"Port Y\", \"Port X\"),  \n       lty = c(1, 1),           \n       col = c(\"black\", \"darkgrey\"),         \n       lwd = 2,\n       bty=\"n\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Stats II</span>"
    ]
  },
  {
    "objectID": "DescriptiveIII.html",
    "href": "DescriptiveIII.html",
    "title": "3  Descriptive Statistics III",
    "section": "",
    "text": "3.1 Concepts",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics III</span>"
    ]
  },
  {
    "objectID": "DescriptiveIII.html#concepts",
    "href": "DescriptiveIII.html#concepts",
    "title": "3  Descriptive Statistics III",
    "section": "",
    "text": "Measures of Central Location\nMeasures of Central Location determine where the center of a distribution lies.\n\nThe mean is the average value for a numerical variable. The sample statistic is estimated by \\(\\bar{x}=\\sum x_{i}/n\\), where \\(x_i\\) is observation \\(i\\), and \\(n\\) is the number of observations. The population parameter is defined as \\(\\mu=\\sum x_{i}/N\\).\nThe median is the value in the middle when data is organized in ascending order. When \\(n\\) is even, the median is the average between the two middle values.\nThe mode is the value with highest frequency from a set of observations.\nThe weighted mean uses weights to determine the importance of each data point of a variable. It is calculated by \\(\\frac{\\sum w_{i}x_{i}}{\\sum w_{i}}\\), where \\(w_{i}\\) are the weights associated to the values \\(x_{i}\\).\nThe geometric mean is a multiplicative average that is less sensitive to outliers. It is used to average growth rates or rated of return. It is calculated by \\(\\sqrt[n]{(1+r_1)*(1+r_2)...(1+r_n)}-1\\), where \\(\\sqrt[n]{}\\) is the \\(n_{th}\\) root, and \\(r_i\\) are the returns or growth rates.\n\n\n\nUseful R functions\nBase R has a collection of functions that calculate measures of central location.\n\nThe mean() function calculates the average of a vector of values.\nThe median() function returns the median of a vector of values.\nThe table() function provides us with a frequency distribution. We can then identify the mode(s) of the vector provided.\nThe summary() function returns a collection of descriptive statistics for a vector or data frame.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics III</span>"
    ]
  },
  {
    "objectID": "DescriptiveIII.html#exercises",
    "href": "DescriptiveIII.html#exercises",
    "title": "3  Descriptive Statistics III",
    "section": "3.2 Exercises",
    "text": "3.2 Exercises\nThe following exercises will help you practice the measures of central location. In particular, the exercises work on:\n\nCalculating the mean, median, and the mode.\nCalculating the weighted average.\nApplying the geometric mean for growth rates and returns.\n\nAnswers are provided below. Try not to peak until you have a formulated your own answer and double checked your work for any mistakes.\n\nExercise 1\nFor the following exercises, make your calculations by hand and verify results using R functions when possible.\n\nUse the following observations to calculate the mean, the median, and the mode.\n\n\n\n8\n10\n9\n12\n12\n\n\n\nUse following observations to calculate the mean, the median, and the mode.\n\n\n\n-4\n0\n-6\n1\n-3\n-4\n\n\n\nUse the following observations, calculate the mean, the median, and the mode.\n\n\n\n20\n15\n25\n20\n10\n15\n25\n20\n15\n\n\n\n\n\n\nExercise 2\nDownload the ISLR2 package. You will need the OJ data set to answer this question.\n\nFind the mean price for Country Hill (PriceCH) and Minute Maid (PriceMM).\nFind the mean price of Country Hill (PriceCH) in store 1 and store 2 (StoreID). Which store had the better price?\nFind the mean price paid by Country Hill (PriceCH) purchasers (Purchase) in store 1 (StoreID)? How about store 2? Which store had the better price?\n\n\n\nExercise 3\n\nOver the past year an investor bought TSLA. She made these purchases on three occasions at the prices shown in the table below. Calculate the average price per share.\n\n\n\n\nDate\nPrice Per Share\nNumber of Shares\n\n\n\n\nFebruary\n250.34\n80\n\n\nApril\n234.59\n120\n\n\nAug\n270.45\n50\n\n\n\n\nWhat would have been the average price per share if the investor would have bought equal amounts of shares each month?\n\n\n\nExercise 4\n\nConsider the following observations for the consumer price index (CPI). Calculate the inflation rate (Growth Rate of the CPI) for each period.\n\n\n\n1.0\n1.3\n1.6\n1.8\n2.1\n\n\n\nSuppose that you want to invest $1000 dollars in a stock that is predicted to yield the following returns in the next four years. Calculate both the arithmetic mean and the geometric mean. Use the geometric mean to estimate how much money you would have by the end of year 4.\n\n\n\nYear\nAnnual Return\n\n\n\n\n1\n17.3\n\n\n2\n19.6\n\n\n3\n6.8\n\n\n4\n8.2",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics III</span>"
    ]
  },
  {
    "objectID": "DescriptiveIII.html#answers",
    "href": "DescriptiveIII.html#answers",
    "title": "3  Descriptive Statistics III",
    "section": "3.3 Answers",
    "text": "3.3 Answers\n\nExercise 1\n\nTo find the mean we will use the following formula \\(( \\frac{1}{n} \\sum_{i=i}^{n} x_{i})\\). The summation of the values is \\(51\\) and the number of observations is \\(5\\). The mean is \\(51/5=10.2\\).\nThe median is found by locating the middle value when data is sorted in ascending order. The median in this example is \\(10\\).\nThe mode is the value with the highest frequency. In this example the mode is \\(12\\) since it is repeated twice and all other numbers appear only once.\n\nThe mean can be easily verified in R by using the mean() function:\n\nmean(c(8,10,9,12,12))\n\n[1] 10.2\n\n\nSimilarly, the median is easily verified by using the median() function:\n\nmedian(c(8,10,9,12,12))\n\n[1] 10\n\n\nWe can use the table() function to calculate frequencies and easily identify the mode.\n\ntable(c(8,10,9,12,12))\n\n\n 8  9 10 12 \n 1  1  1  2 \n\n\n\nThe mean is \\(-2.67\\), the median is \\(-3.5\\), the mode is \\(-4\\).\n\nThese mean is verified in R:\n\nmean(c(-4,0,-6,1,-3,-4))\n\n[1] -2.666667\n\n\nThe median in R:\n\nmedian(c(-4,0,-6,1,-3,-4))\n\n[1] -3.5\n\n\nFinally, the mode in R:\n\ntable(c(-4,0,-6,1,-3,-4))\n\n\n-6 -4 -3  0  1 \n 1  2  1  1  1 \n\n\n\nThe mean is \\(18.33\\), the median is \\(20\\), the data is bimodal with both \\(15\\) and \\(20\\) being modes.\n\nThese mean is verified in R:\n\nmean(c(20,15,25,20,10,15,25,20,15))\n\n[1] 18.33333\n\n\nThe median in R:\n\nmedian(c(20,15,25,20,10,15,25,20,15))\n\n[1] 20\n\n\nThe frequency distribution identifies the modes:\n\ntable(c(20,15,25,20,10,15,25,20,15))\n\n\n10 15 20 25 \n 1  3  3  2 \n\n\n\n\nExercise 2\n\nThe mean price for Country Hill is \\(1.87\\). The mean price for Minute Maid is \\(2.09\\).\n\nThe means can be easily found with the mean() function:\n\nlibrary(ISLR2)\nmean(OJ$PriceCH)\n\n[1] 1.867421\n\nmean(OJ$PriceMM)\n\n[1] 2.085411\n\n\n\nThe mean price at store 1 for Country Hill is \\(1.80\\) vs. \\(1.84\\) for store 2. The juice is cheaper at store 1.\n\nThe means for each store can be found by using indexing and a logical statement. The Country Hill mean price at store 1 is given by:\n\nmean(OJ$PriceCH[OJ$StoreID==1])\n\n[1] 1.803758\n\n\nThe Country Hill mean price at store 2 is given by:\n\nmean(OJ$PriceCH[OJ$StoreID==2])\n\n[1] 1.841216\n\n\n\nPurchasers of Country Hill at store 1 paid and average of \\(1.80\\) for Country Hill juice. At store 2 they paid \\(1.86\\). Once again the average price was lower at store 1.\n\nThe mean for Country Hill purchasers at store 1 is given by:\n\nmean(OJ$PriceCH[OJ$StoreID==1 & OJ$Purchase==\"CH\"])\n\n[1] 1.797176\n\n\nThe mean for Country Hill purchasers at store 2 is:\n\nmean(OJ$PriceCH[OJ$StoreID==2 & OJ$Purchase==\"CH\"])\n\n[1] 1.857383\n\n\n\n\nExercise 3\n\nThe average price of sale is found by using the weighted average formula. \\(\\frac{\\sum w_{i}x_{i}}{\\sum w_{i}}\\) The weights (\\(w_{i}\\)) are given by the number of shares bought and the values (\\(x_{i}\\)) are the prices. The weighted average is \\(246.802\\).\n\nIn R you can create two vectors. One holds the share price and the other one the number of shares bought.\n\nPricePerShare&lt;-c(250.34,234.59,270.45)\nNumberOfShares&lt;-c(80,120,50)\n\nNext, you can multiply the PricePerShare and NumberOfShares vectors to find the numerator and then use sum() function to find the denominator. The weighted average is:\n\n(WeightedAverage&lt;-\n  sum(PricePerShare*NumberOfShares)/sum(NumberOfShares))\n\n[1] 246.802\n\n\n\nThe average if equal shares were bought would be \\(251.7933\\).\n\nIn R you can use the mean() function on the PricePerShare vector.\n\n(Average&lt;-mean(PricePerShare))\n\n[1] 251.7933\n\n\n\n\nExercise 4\n\nThe inflation rate for each period is shown in the table below:\n\n\n\n\n30%\n23.08%\n12.5%\n16.67%\n\n\n\nIn R create an object to store the values of the CPI:\n\nCPI&lt;-c(1,1.3,1.6,1.8,2.1)\n\nNext use the diff() function to find the difference between the end value and start value. Divide the result by a vector of starting value and multiply times 100.\n\n(Inflation&lt;-100*diff(CPI)/CPI[1:4])\n\n[1] 30.00000 23.07692 12.50000 16.66667\n\n\n\nAt the end of 4 years it is predicted that you would have \\(1621.17\\) dollars. Each year you would have gained \\(12.84\\)% on average.\n\nIn R include the annual rates in a vector:\n\ngrowth&lt;-c(0.173,0.196,0.068,0.082)\n\nThe arithmetic mean is:\n\n100*mean(growth)\n\n[1] 12.975\n\n\nThe geometric mean is:\n\n(geom&lt;-((prod(1+growth))^(1/4)-1)*100)\n\n[1] 12.8384\n\n\nAt the end of the four years we would have:\n\n1000*(1+geom/100)^4\n\n[1] 1621.167",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics III</span>"
    ]
  },
  {
    "objectID": "DescriptiveIV.html",
    "href": "DescriptiveIV.html",
    "title": "4  Descriptive Stats IV",
    "section": "",
    "text": "4.1 Concepts",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Stats IV</span>"
    ]
  },
  {
    "objectID": "DescriptiveIV.html#concepts",
    "href": "DescriptiveIV.html#concepts",
    "title": "4  Descriptive Stats IV",
    "section": "",
    "text": "Measures of Dispersion\nMeasures of dispersion are used to determine the spread (variability) of the data.\n\nThe range is calculated by \\(largest-smallest\\). It ignores the variability of the data between the largest and smallest values.\nThe variance calculates the dispersion around the mean. It uses squared deviations. The population parameter is given by \\(\\sigma^2= \\frac{\\sum (x_i-\\mu)^2}{N}\\), while the sample statistic is \\(s^2=\\frac{\\sum (x_i-\\bar{x})^2}{n-1}\\).\nThe standard deviation measures the average deviation around the mean. It is calculated as the square root of the variance. For the population parameter use \\(\\sigma=\\sqrt{\\sigma^2}\\) and \\(s=\\sqrt{s^2}\\) for the sample statistic.\nThe Mean Absolute Deviation (\\(MAD\\)) measures the average deviation from the mean. This measure uses absolute deviations. It is calculated by \\(MAD=\\frac{\\sum |x_i-\\mu|}{N}\\) for the population and \\(mad=\\frac{\\sum |x_i-\\bar{x}|}{n}\\) for the sample.\nThe coefficient of variation \\(CV=s/\\bar{x}\\) adjusts the standard deviation for differences in units of measure or scale.\n\n\n\nPortfolio Assesment\nTo asses the performance of a portfolio calculate:\n\nThe mean return of the portfolio \\(\\alpha\\bar{R}_1+(1-\\alpha)\\bar{R}_2\\), where \\(\\alpha\\) is the weight of investment 1 in the portfolio and \\(\\bar{R}_i\\) is the average return of investment \\(i \\in\\){\\(1\\),\\(2\\)}.\nThe variance of the portfolio is given by \\(\\begin{bmatrix} \\alpha \\\\ 1-\\alpha \\end{bmatrix}^T \\begin{bmatrix} s_x^2 & s_{xy} \\\\ s_{xy} & s_y^2 \\end{bmatrix} \\begin{bmatrix} \\alpha \\\\ 1-\\alpha \\end{bmatrix}\\)\nThe Sharpe ratio quantifies the excess return of an investment over the risk free return. It is calculated by \\(\\frac{\\bar{R_p}-R_f}{s}\\), where \\(\\bar{R_p}\\) is the mean return of the portfolio, \\(R_f\\) is the risk free return, and \\(s\\) is the standard deviation.\n\n\n\nUseful R Functions\nThe range() function returns the maximum and minimum of a vector of values.\nThe diff() function finds the first difference of a vector.\nThe var() function calculates the sample variance for a vector of values. To calculate the population variance, adjust the result by a factor of \\((n-1)/n\\).\nThe sd() function calculates the sample standard deviation.\nThe matrix() function defines a matrix.\nWhen dealing with matrices, the t() function transposes a vector or matrix, and the operator %*% performs matrix multiplication.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Stats IV</span>"
    ]
  },
  {
    "objectID": "DescriptiveIV.html#exercises",
    "href": "DescriptiveIV.html#exercises",
    "title": "4  Descriptive Stats IV",
    "section": "4.2 Exercises",
    "text": "4.2 Exercises\nThe following exercises will help you practice the measures of dispersion. In particular, the exercises work on:\n\nCalculating the range, MAD, variance, and the standard deviation.\nUsing R to calculate measures of dispersion.\nCalculating and using the Sharpe ratio to select investments.\n\nAnswers are provided below. Try not to peak until you have a formulated your own answer and double checked your work for any mistakes.\n\nExercise 1\nFor the following exercises, make your calculations by hand and verify results using R functions when possible. Make sure to calculate the deviations from the mean.\n\nUse the following observations to calculate the Range, MAD, Variance and Standard Deviation. Assume that the data below is the entire population.\n\n\n\n70\n68\n4\n98\n\n\n\nUse the following observations to calculate the Range, MAD, Variance and Standard Deviation. Assume that the data below is a sample from the population.\n\n\n\n-4\n0\n-6\n1\n-3\n0\n\n\n\n\n\n\nExercise 2\nYou will need the Stocks data set to answer this question. You can find this data at https://jagelves.github.io/Data/Stocks.csv The data is a sample of daily stock prices for ticker symbols TSLA (Tesla), VTI (S&P 500) and GBTC (Bitcoin).\n\nCalculate the standard deviations for each stock. Which stock had the lowest standard deviation?\nCalculate the MAD. Does your answer in 1. remain the same?\nFinally, calculate the coefficient of variation. Any changes to your conclusions?\n\n\n\nExercise 3\nInstall the ISLR2 package. You will need the Portfolio data set to answer this question. The data has 100 records of the returns of two stocks.\n\nCalculate the mean and standard deviation for each stock. Which investment has higher returns on average? Which investment is safest as measured by the standard deviation?\nUse a Risk Free rate of return of 3.5% to calculate the Sharpe ratio for each stock. Which stock would you recommend?\nCalculate the average return for a portfolio that has 30% of stock X and 70% of stock Y. What is the standard deviation of the portfolio?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Stats IV</span>"
    ]
  },
  {
    "objectID": "DescriptiveIV.html#answers",
    "href": "DescriptiveIV.html#answers",
    "title": "4  Descriptive Stats IV",
    "section": "4.3 Answers",
    "text": "4.3 Answers\n\nExercise 1\n\nThe mean is \\(60\\), the Range is \\(94\\), the MAD is \\(28\\), the variance is \\(1186\\) and the variance is \\(34.44\\).\n\nStart by crating a vector to hold the values:\n\nEx1&lt;-c(70,68,4,98)\n\nThe range can be calculated by using the range() and diff() functions in R.\n\n(Range&lt;-diff(range(Ex1)))\n\n[1] 94\n\n\nNext, we can create a table by hand that captures the deviations from the mean. Let’s calculate the mean first:\n\n(Average1&lt;-mean(Ex1))\n\n[1] 60\n\n\nNow we can use the mean to fill out a table of deviations:\n\n\n\n\\(x_{i}\\)\n\\(x_{i}-\\bar{x}\\)\n\\((x_{i}-\\bar{x})^2\\)\n\\(|x_{i}-\\bar{x}|\\)\n\n\n\n\n70\n10\n100\n10\n\n\n68\n8\n64\n8\n\n\n4\n-56\n3136\n56\n\n\n98\n38\n1444\n38\n\n\n\nThe variance averages out the squared deviations \\((x_{i}-\\bar{x})^2\\), the MAD averages out the absolute deviations \\(|x_{i}-\\bar{x}|\\), and the standard deviation is the square root of the variance.\nLet’s verify the variance in R:\n\nSquaredDeviations1&lt;-(Ex1-Average1)^2\nAverageDeviations1&lt;-mean(SquaredDeviations1)\nvar(Ex1)*3/4\n\n[1] 1186\n\n\nNote that R calculates the sample variance. Hence, we must multiply the result by \\(3/4\\) to get the population variance. The standard deviation is just the square root of the variance:\n\nsqrt(AverageDeviations1)\n\n[1] 34.43835\n\n\nLastly, the MAD is calculated by averaging the absolute deviations \\(|x_{i}-\\bar{x}|\\).\n\nAbsoluteDeviations1&lt;-abs(Ex1-Average1)\nmean(AbsoluteDeviations1)\n\n[1] 28\n\n\n\nThe mean is \\(-2\\), Range is \\(7\\), the MAD is \\(2.33\\), the variance is \\(7.6\\) and the standard deviation is \\(2.76\\).\n\nHere is the table of deviations from the mean:\n\n\n\n\\(x_{i}\\)\n\\(x_{i}-\\bar{x}\\)\n\\((x_{i}-\\bar{x})^2\\)\n\\(|x_{i}-\\bar{x}|\\)\n\n\n\n\n-4\n-2\n4\n2\n\n\n0\n2\n4\n2\n\n\n-6\n-4\n16\n4\n\n\n1\n3\n9\n3\n\n\n-3\n-1\n1\n1\n\n\n0\n2\n4\n2\n\n\n\nWe can check the results in R. Let’s start with the variance:\n\nEx2&lt;-c(-4,0,-6,1,-3,0)\nvar(Ex2)\n\n[1] 7.6\n\n\nThe standard deviation can be found with the sd() function:\n\nsd(Ex2)\n\n[1] 2.75681\n\n\nThe MAD is given by:\n\n(MAD&lt;-mean(abs(Ex2-mean(Ex2))))\n\n[1] 2.333333\n\n\nLastly, the range:\n\ndiff(range(Ex2))\n\n[1] 7\n\n\n\n\nExercise 2\n\nFor the sample taken, GBTC has the less variation. The standard deviation of GBTC is \\(9.43\\), which is less than \\(16.57\\) for VTI or \\(50.38\\) for TSLA.\n\nStart by loading the data set from the website. Since the file is in csv format, we will use the read.csv() function.\n\nStockPrices&lt;-read.csv(\"https://jagelves.github.io/Data/Stocks.csv\")\n\nLet’s start with the standard deviation of the Tesla stock. The standard deviation is given by:\n\nsd(StockPrices$TSLA)\n\n[1] 50.38092\n\n\nNext, let’s find the standard deviation for the S&P 500 or VTI. The standard deviation is given by:\n\nsd(StockPrices$VTI)\n\n[1] 16.5731\n\n\nFinally, let’s calculate the standard deviation for GBTC or Bitcoin.\n\nsd(StockPrices$GBTC)\n\n[1] 9.434213\n\n\n\nThe answer is the same, since the MAD for GBTC is \\(8.46\\) which is lower than \\(14.27\\) for VTI or \\(41.67\\) for TSLA.\n\nTo calculate the MAD for TSLA we can use the following command:\n\n(MADTSLA&lt;-mean(abs(StockPrices$TSLA-mean(StockPrices$TSLA))))\n\n[1] 41.67163\n\n\nThe MAD for VTI is:\n\n(MADVTI&lt;-mean(abs(StockPrices$VTI-mean(StockPrices$VTI))))\n\n[1] 14.27169\n\n\nThe MAD for GBTC is:\n\n(MADGBTC&lt;-mean(abs(StockPrices$GBTC-mean(StockPrices$GBTC))))\n\n[1] 8.458029\n\n\n\nBy considering the magnitudes of the stock prices, it seems like VTI is the less volatile stock. VTI has a CV of \\(0.08\\) which is lower than \\(0.44\\) for GBTC or \\(0.18\\) for TSLA. In fact, by CV Bitcoin seems to be the most risky asset.\n\nThe coefficients of variations are as follows. For TSLA the CV is:\n\n(CVTSLA&lt;-sd(StockPrices$TSLA)/mean(StockPrices$TSLA))\n\n[1] 0.1793755\n\n\nFor VTI the CV is:\n\n(CVVTI&lt;-sd(StockPrices$VTI)/mean(StockPrices$VTI))\n\n[1] 0.07970004\n\n\nFor GBTC we get:\n\n(CVGBTC&lt;-sd(StockPrices$GBTC)/mean(StockPrices$GBTC))\n\n[1] 0.4442497\n\n\n\n\nExercise 3\n\nThe best performing stock on average is stock X. It has an average return of \\(-0.078\\)% vs. \\(0.097\\)% for stock Y. The safest stock is stock X as well, since the standard deviation is \\(1.062\\) percentage points vs. \\(1.14\\) percentage points for stock Y.\n\nStart by loading the ISLR2 package:\n\nlibrary(ISLR2)\n\nNext, calculate the mean for stock X:\n\nmean(Portfolio$X)\n\n[1] -0.07713211\n\n\nand stock Y.\n\nmean(Portfolio$Y)\n\n[1] -0.09694472\n\n\nThen, calculate the standard deviation for stock X\n\nsd(Portfolio$X)\n\n[1] 1.062376\n\n\nand stock Y.\n\nsd(Portfolio$Y)\n\n[1] 1.143782\n\n\n\nThe Sharpe Ratio measures the excess return per unit of risk taken. Stock X has the better Sharpe Ratio. \\(-0.106\\) vs. \\(-0.115\\). Stock X is recommended since it provides a higher excess return per unit of risk taken.\n\nTo calculate Sharpe Ratios use both the average return, and the standard deviation. For stock X, the Sharpe Ratio is:\n\n(mean(Portfolio$X)-0.035)/sd(Portfolio$X)\n\n[1] -0.1055484\n\n\nThe Sharpe Ratio for stock Y:\n\n(mean(Portfolio$Y)-0.035)/sd(Portfolio$Y)\n\n[1] -0.1153583\n\n\n\nThe portfolio has an average return of \\(-0.091\\) which is worse than stock X but better than stock Y. The standard deviation is \\(1.00\\). This is better than stock X and Y separately. The Sharpe ratio of \\(-0.091\\) is also better for the portfolio than for each stock individually.\n\nThe mean of the portfolio is given by:\n\n(mean_return=0.3*mean(Portfolio$X)+0.7*mean(Portfolio$Y))\n\n[1] -0.09100094\n\n\nThe covariance matrix is given by:\n\n(risk&lt;-cov(Portfolio))\n\n          X         Y\nX 1.1286424 0.6263583\nY 0.6263583 1.3082375\n\n\nUsing the matrix we can now calculate the standard deviation:\n\n(standard&lt;-sqrt(t(c(0.3,0.7)) %*% (risk %*% c(0.3,0.7))))\n\n         [,1]\n[1,] 1.002838\n\n\nFinally, the Sharpe ration for the portfolio is:\n\nmean_return/standard[1]\n\n[1] -0.09074338",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Stats IV</span>"
    ]
  },
  {
    "objectID": "DescriptiveV.html",
    "href": "DescriptiveV.html",
    "title": "5  Descriptive Stats V",
    "section": "",
    "text": "5.1 Concepts",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Stats V</span>"
    ]
  },
  {
    "objectID": "DescriptiveV.html#concepts",
    "href": "DescriptiveV.html#concepts",
    "title": "5  Descriptive Stats V",
    "section": "",
    "text": "Quantiles and Percentiles\nA quantile is a location within a set of ranked numbers (or distribution), below which a certain proportion, \\(q\\), of that set lie. Ex: 0.25 of the data lies below the 0.25 quantile.\nPercentiles express quantiles in percentage form. Ex: 25% of the data lies below the 25th percentile. To calculate a percentile:\n\nSort the data in ascending order.\nCompute the location of the percentile desired using \\(L_{p}=\\frac{(n+1)P}{100}\\) where \\(L_{p}\\) is the location of the \\(P_{th}\\) percentile, and \\(P\\) is the percentile desired.\nThe value at \\(L_p\\), is the the \\(P_{th}\\) percentile.\n\n\n\nChevyshev’s Theorem and Empirical Rule\nChevyshev’s Theorem states that at least \\(1-1/z^2\\)% of the data lies between \\(z\\) standard deviations from the mean. This result does not depend on the shape of the distribution.\nThe Empirical Rule or (\\(68\\),\\(95\\),\\(99.7\\) rule) states that \\(68\\)%, \\(95\\)%, and \\(99.7\\)% of the data lies between \\(1\\), \\(2\\), and \\(3\\) standard deviations from the mean respectively. The rule depends on the data being normally distributed.\n\n\nFive Point Summary and Outliers\nA popular way to summarize data is by calculating the minimum, first quartile, median, third quartile and maximum (five point summary).\nThe interquartile range (IQR) is the difference between the third quartile and the first quartile.\nOutliers are extreme deviations from the mean. They are values that are not “normal”. To calculate outliers:\n\nUse a z-score to measure the distance from the mean in units of standard deviation. \\(z_{i}=\\frac{x_i-\\bar{x}}{s_x}\\). \\(z\\)-scores above \\(3\\) are suspected outliers.\nCalculate \\(Q_1-1.5(IQR)\\) and \\(Q_3+1.5(IQR)\\), where \\(Q_1\\) is the first quartile, \\(Q_3\\) is the third quartile, and \\(IQR\\) is the interquartile range. If \\(x_i\\) is less than \\(Q_1-1.5(IQR)\\) or greater than \\(Q_3+1.5(IQR)\\), then it is considered an outlier.\n\nA box plot is a graph that shows the five point summary, outliers (if any), and the distribution of data.\nTo determine if the data is skewed, calculate the Pearson’s Coefficient of Skew. \\(Sk=\\frac{3(\\bar {x}- Median)}{s_x}\\). The distribution is skewed to the left if \\(Sk&lt;0\\), skewed to the right is \\(Sk&gt;0\\), and symmetric if \\(Sk=0\\).\n\n\nUseful R Functions\nThe quantile() function returns the five point summary when no arguments are specified. For a specific quantile, specify the probs argument.\nThe boxplot() command returns a box plot for a vector of values.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Stats V</span>"
    ]
  },
  {
    "objectID": "DescriptiveV.html#exercises",
    "href": "DescriptiveV.html#exercises",
    "title": "5  Descriptive Stats V",
    "section": "5.2 Exercises",
    "text": "5.2 Exercises\nThe following exercises will help you practice other statistical measures. In particular, the exercises work on:\n\nConstructing a five point summary and a boxplot.\nApplying Chebyshev’s Theorem.\nIdentifying skewness.\nIdentifying outliers.\n\nAnswers are provided below. Try not to peak until you have a formulated your own answer and double checked your work for any mistakes.\n\nExercise 1\nFor the following exercises, make your calculations by hand and verify results using R functions when possible.\n\nUse the following observations to calculate the minimum, the first, second and third quartiles, and the maximum. Are there any outliers? Find the IQR to answer the question.\n\n\n\n3\n10\n4\n1\n0\n30\n6\n\n\n\nConfirm your finding of an outlier by calculating the \\(z\\)-score. Is \\(30\\) an outlier when using a \\(z\\)-Score?\nUse Chebyshev’s theorem to determine what percent of the data falls between the \\(z\\)-score found in \\(2\\).\n\n\n\nExercise 2\nYou will need the Stocks data set to answer this question. You can find this data at https://jagelves.github.io/Data/Stocks.csv The data is a sample of daily stock prices for ticker symbols TSLA (Tesla), VTI (S&P 500) and GBTC (Bitcoin).\n\nConstruct a boxplot for Stock A. Is the data skewed or symmetric?\nCreate a histogram of the data. Include a vertical line for the mean and median. Explain how the mean and median indicates a skew in the data. Calculate the skewness statistic to confirm your result.\nUse a line chart to plot your data. Can you explain why the data has a skew?\n\n\n\nExercise 3\nYou will need the mtcars data set to answer this question. This data set is part of R. You don’t need to download any files to access it.\n\nConstruct a boxplot for the hp variable. Write a command in R that retrieves the outlier. Which car is the outlier?\nCreate a histogram of the data. Is the data skewed? Include a vertical line for the mean and median. Calculate the skewness statistic to confirm your result.\nTransform the data by taking a natural logarithm. Specifically, create a new variable called Loghp. Repeat the procedure in 2. Is the skew still there?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Stats V</span>"
    ]
  },
  {
    "objectID": "DescriptiveV.html#answers",
    "href": "DescriptiveV.html#answers",
    "title": "5  Descriptive Stats V",
    "section": "5.3 Answers",
    "text": "5.3 Answers\n\nExercise 1\n\nThe minimum is \\(0\\), the first quartile is \\(2\\), second quartile is \\(4\\), third quartile is \\(8\\), and maximum is \\(30\\). \\(30\\) is an outlier since it is beyond \\(Q_{3}+1.5*IQR\\).\n\nQuartiles are calculated using the percentile formula \\((n+1)P/100\\). The data set has seven numbers. The first quartile’s location is \\(8/4=2\\), the second quartile’s location is \\(8/2=4\\) and the third quartile’s location is \\(24/4=6\\). The values at these location, when data is organized in ascending order, are \\(1\\), \\(4\\), and \\(10\\).\nIn R we can get the five number summary by using the quantile() function. Since there are various rules that can be used to calculate percentiles, we specify type \\(6\\) to match our rules.\n\nEx1&lt;-c(3,10,4,1,0,30,6)\nquantile(Ex1,type = 6)\n\n  0%  25%  50%  75% 100% \n   0    1    4   10   30 \n\n\nThe interquartile range is needed to determine if there are any outliers. The \\(IQR\\) for this data set is \\(Q_{3}-Q_{1}=9\\). This reveals that \\(30\\) is and outlier, since \\(10+1.5*9=23.5\\). Everything beyond \\(23.5\\) is an outlier.\n\nIf we use the \\(z\\)-score instead we find that \\(30\\) is not an outlier since the \\(z\\)-score is \\(Z_{30}=2.15\\). This observation is only \\(2.15\\) standard deviations away from the mean.\n\nIn R we can make a quick calculation of the \\(z\\)-Score to confirm our results. The \\(z\\)-score is given by \\(Z_{i}=\\frac{x_{30}-\\mu}{\\sigma}\\).\n\n(Z30&lt;-(30-mean(Ex1))/sd(Ex1))\n\n[1] 2.148711\n\n\n\nChebyshev’s theorem states that \\(1-\\frac{1}{z_{2}}\\) of the data lies between \\(z\\) standard deviation from the mean.\n\nSubstituting the \\(z\\)-score found in 2. we get \\(78.34\\)%. In R:\n\n1-1/(Z30)^2\n\n[1] 0.7834073\n\n\n\n\nExercise 2\n\nThe data is skewed to the right.\n\nStart by loading the data set:\n\nStockPrices&lt;-read.csv(\"https://jagelves.github.io/Data/Stocks.csv\")\n\nTo construct the boxplot in R, use the boxplot() command.\n\nboxplot(as.numeric(StockPrices$VTI),axes=F, ylim=c(120,260),\n        cex=1.5, col=\"#F5F5F5\",pch=21,bg=\"red\")\naxis(side=1, labels=c(\"VTI\"), at=seq(1))\naxis(side=2, labels=TRUE, at=seq(140,260,20),font=1,las=1)\n\n\n\n\n\n\n\n\nThe boxplot shows that there are no outliers. The data also looks like it has a slight skew to the right.\n\nThe mean is more sensitive to outliers than the median. Hence, when the data is skewed to the right we expect that the mean is larger than the median.\n\nLet’s construct a histogram in R to search for skewness.\n\nhist(StockPrices$VTI,main=\"\", ylim=c(0,40), \n     xlab=\"Prices\", col=\"#F5F5F5\")\nabline(v=mean(StockPrices$VTI),col=\"black\",lwd=2)\nabline(v=median(StockPrices$VTI),col=\"darkgrey\",lwd=2)\nlegend(x = \"topright\",          \n       legend = c(\"Mean\", \"Median\"),  \n       lty = c(1, 1),           \n       col = c(\"black\", \"darkgrey\"),         \n       lwd = 2,\n       bty=\"n\")    \n\n\n\n\n\n\n\n\nThe lines are close to each other but the mean is slighlty larger than the median. Let’s confirm with the skewness statistic \\(3(mean-median)/sd\\).\n\n(skew&lt;-3*(mean(StockPrices$VTI-median(StockPrices$VTI))/sd(StockPrices$VTI)))\n\n[1] 0.2856304\n\n\nThis indicates that there is a slight skew to the right of the data.\n\nThe line chart indicates that the data has a downward trend in the early periods. This creates a few points that are large. In later periods the stock price stabilizes to lower levels.\n\n\nplot(y=StockPrices$VTI,x=seq(1,length(StockPrices$VTI)),\n     type=\"l\", ylab=\"Prices\", xlab=\"Period\", axes=F)\naxis(side=1, labels=TRUE, at=seq(0,250,50),font=1,las=1)\naxis(side=2, labels=TRUE, at=seq(0,300,20),font=1,las=1)\n\n\n\n\n\n\n\n\n\n\nExercise 3\n\nThe outlier is the Masserati Bora. The horse power is \\(335\\).\n\nIn R we can construct a boxplot with the following command:\n\nboxplot(mtcars$hp,axes=F, ylim=c(0,400),\n        cex=1.5, col=\"#F5F5F5\",pch=21,bg=\"red\")\naxis(side=1, labels=c(\"HP\"), at=seq(1))\naxis(side=2, labels=TRUE, at=seq(0,400,50),font=1,las=1)\n\n\n\n\n\n\n\n\nFrom the graph it seems like the outlier is beyond a horsepower of 275. Let’s write an R command to retrieve the car.\n\nmtcars[mtcars$hp&gt;275,]\n\n              mpg cyl disp  hp drat   wt qsec vs am gear carb\nMaserati Bora  15   8  301 335 3.54 3.57 14.6  0  1    5    8\n\n\nIt’s the Masserati Bora!\n\nThe histogram looks skewed to the right. This is confirmed by the estimation of a Pearson coefficient fo skewness of \\(1.04\\).\n\nIn R we can construct a histogram with vertical lines for the mean and median wit the following code:\n\nhist(mtcars$hp,main=\"\", ylim=c(0,12), xlab=\"Horse Power\",\n     col=\"#F5F5F5\")\nabline(v=mean(mtcars$hp),col=\"black\",lwd=2)\nabline(v=median(mtcars$hp),col=\"darkgrey\",lwd=2)\nlegend(x = \"topright\",          \n       legend = c(\"Mean\", \"Median\"),  \n       lty = c(1, 1),           \n       col = c(\"black\", \"darkgrey\"),         \n       lwd = 2,\n       bty=\"n\")    \n\n\n\n\n\n\n\n\nThe histogram looks skewed to the right. Pearson’s Coefficient of Skewness is:\n\n(SkewHP&lt;-3*(mean(mtcars$hp)-median(mtcars$hp))/sd(mtcars$hp))\n\n[1] 1.036458\n\n\n\nThe skew is still there, but the distribution now look more symmetrical and the Skew coefficient has decreased to \\(0.44\\).\n\nIn R we can create an new variable that captures the log transformation. The log() function takes the natural logarithm of a number or vector.\n\nLogHP&lt;-log(mtcars$hp)\n\nLet’s use this new variable to create our histogram:\n\nhist(LogHP,main=\"\", ylim=c(0,12), xlab=\"Horse Power\", \n     col=\"#F5F5F5\")\nabline(v=mean(LogHP),col=\"black\",lwd=2)\nabline(v=median(LogHP),col=\"darkgrey\",lwd=2)\nlegend(x = \"topright\",          \n       legend = c(\"Mean\", \"Median\"),  \n       lty = c(1, 1),           \n       col = c(\"black\", \"darkgrey\"),         \n       lwd = 2,\n       bty=\"n\")    \n\n\n\n\n\n\n\n\nThe mean and the variance now look closer together. The tail of the distribution (skew) now also looks diminished. The Skewness coefficient has decreased significantly:\n\n(SkewLogHP&lt;-3*(mean(LogHP)-median(LogHP))/sd(LogHP))\n\n[1] 0.4402212",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Stats V</span>"
    ]
  },
  {
    "objectID": "RegressionI.html",
    "href": "RegressionI.html",
    "title": "6  Regression I",
    "section": "",
    "text": "6.1 Concepts",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regression I</span>"
    ]
  },
  {
    "objectID": "RegressionI.html#concepts",
    "href": "RegressionI.html#concepts",
    "title": "6  Regression I",
    "section": "",
    "text": "Measures of Association\nMeasures of association determine whether there is a linear relationship between two variables. They also determine the strength of the relationship.\n\nThe covariance is a measure that determines the direction of the relationship between two variables. It is calculated by \\(s_{xy}=\\frac {\\sum(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum (x_i-\\bar{x})^2}\\). If \\(s_{xy}&gt;0\\) there is a direct relationship, if \\(s_{xy}&lt;0\\) there is an inverse relationship, and if \\(s_{xy}=0\\) there is no relationship.\nThe correlation measures the strength of the linear relationship. It is calculated by \\(r= \\frac {s_{xy}}{s_x s_y}\\). The correlation coefficient is between \\([-1,1]\\). When the correlation coefficient is \\(1\\) (\\(-1\\)), there is a perfect direct (inverse) relationship between the two variables.\nThe coefficient of determination or \\(R^2\\), measures the percent of variation in \\(y\\) explained by variations in \\(x\\). It is calculated by \\(R^2=r^2\\). The next chapter expands on this measure.\nA scatter plot displays pairs of [\\(x\\),\\(y\\)] as points on the Cartesian plane. The plot provides a visual aid to determine the relationship between two variables.\n\n\n\nUseful R Functions\nTo calculate the covariance use the cov() function.\nThe correlation coefficient can be calculated using the cor() function.\nThe plot() function will create scatter plots.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regression I</span>"
    ]
  },
  {
    "objectID": "RegressionI.html#exercises",
    "href": "RegressionI.html#exercises",
    "title": "6  Regression I",
    "section": "6.2 Exercises",
    "text": "6.2 Exercises\nThe following exercises will help you understand statistical measures that establish the relationship between two variables. In particular, the exercises work on:\n\nCalculating covariance and correlation.\nUsing R to plot scatter diagrams.\nCalculating the coefficient of determination.\n\nAnswers are provided below. Try not to peak until you have a formulated your own answer and double checked your work for any mistakes.\n\nExercise 1\nFor the following exercises, make your calculations by hand and verify results using R functions when possible.\n\nConsider the data below. Calculate the covariance and correlation coefficient by finding deviations from the mean. Use R to verify your result. Is there a direct or inverse relationship between the two variables? How strong is the relationship?\n\n\n\n\nx\n20\n21\n15\n18\n25\n\n\n\n\ny\n17\n19\n12\n13\n22\n\n\n\n\nConsider the data below. Calculate the covariance and correlation coefficient by finding deviations from the mean. Use R to verify your result. Is there a direct or inverse relationship between the two variables? How strong is the relationship?\n\n\n\n\nw\n19\n16\n14\n11\n18\n\n\n\n\nz\n17\n20\n20\n16\n18\n\n\n\n\n\nExercise 2\nYou will need the mtcars data set to answer this question. This data set is part of R. You don’t need to download any files to access it.\n\nCalculate the correlation coefficient between hp and mpg. Explain the results. Specifically, the direction of the relationship and the strength given the context of the problem.\nCreate a scatter diagram of the two variables. Is the scatter diagram what you expected after you calculated the correlation coefficient?\nCalculate the coefficient of determination. How close is it to one? What else could be explaining the variation in the mpg? Let your dependent variable be mpg.\n\n\n\nExercise 3\nYou will need the College data set to answer this question. You can find this data set here: https://jagelves.github.io/Data/College.csv\n\nCreate a scatter diagram between GRAD_DEBT_MDN (Median Debt) and MD_EARN_WNE_P10 (Median Earnings). What type of relationship do you observe between the variables?\nCalculate the correlation coefficient and the coefficient of determination. According to the data, are higher debts correlated with higher earnings?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regression I</span>"
    ]
  },
  {
    "objectID": "RegressionI.html#answers",
    "href": "RegressionI.html#answers",
    "title": "6  Regression I",
    "section": "6.3 Answers",
    "text": "6.3 Answers\n\nExercise 1\n\nThe covariance is \\(14.9\\) and the correlation is \\(0.96\\). The results indicate that there is a strong direct relationship between the two variables.\n\nLet’s start by finding the deviations from the mean for the x variable in R.\n\nx&lt;-c(20,21,15,18,25)\n(devx&lt;-x-mean(x))\n\n[1]  0.2  1.2 -4.8 -1.8  5.2\n\n\nWe will do the same with y:\n\ny&lt;-c(17,19,12,13,22)\n(devy&lt;-y-mean(y))\n\n[1]  0.4  2.4 -4.6 -3.6  5.4\n\n\nNote that when the deviations in x are negative (positive), they are also negative (positive) in y. This is indicative of a direct relationship between the two variables. The covariance is given by:\n\n(Ex1Cov&lt;-sum(devx*devy)/(length(devx)-1))\n\n[1] 14.9\n\n\nWe can verify this by using cov() function in R.\n\ncov(x,y)\n\n[1] 14.9\n\n\nThe correlation coefficient is found by dividing the covariance over the product of standard deviations. In R:\n\n(Ex1Cor&lt;-Ex1Cov/(sd(x)*sd(y)))\n\n[1] 0.9678386\n\n\nWe can once more verify the result in R with the built in function cor().\n\ncor(x,y)\n\n[1] 0.9678386\n\n\n\nThe covariance is \\(0.85\\) and the correlation is \\(0.148\\). The results indicate that there is a very weak direct relationship between the two variables. They might be unrelated.\n\nLet’s start with w and finding the deviations from the mean in R.\n\nw&lt;-c(19,16,14,11,18)\n(devw&lt;-w-mean(w))\n\n[1]  3.4  0.4 -1.6 -4.6  2.4\n\n\nWe will do the same with z:\n\nz&lt;-c(17,20,20,16,18)\n(devz&lt;-z-mean(z))\n\n[1] -1.2  1.8  1.8 -2.2 -0.2\n\n\nThe covariance is given by:\n\n(Ex2Cov&lt;-sum(devw*devz)/(length(devz)-1))\n\n[1] 0.85\n\n\nWe can verify this with the cov() function in R.\n\ncov(w,z)\n\n[1] 0.85\n\n\nThe correlation coefficient is found by dividing the covariance over the product of standard deviations. In R:\n\n(Ex2Cor&lt;-Ex2Cov/(sd(z)*sd(w)))\n\n[1] 0.1480558\n\n\nWe can once more verify the result in R with the built in function cor().\n\ncor(w,z)\n\n[1] 0.1480558\n\n\n\n\nExercise 2\n\nThe correlation coefficient is \\(-0.78\\). This is indicative of a moderately strong inverse relationship between mpg and mp.\n\nIn R we can easily calculate the correlation coefficient with the cor() function.\n\ncor(mtcars$mpg,mtcars$hp)\n\n[1] -0.7761684\n\n\n\nThe scatter diagram is downward sloping. Most points are close to the trend line. It is what was expected from a correlation coefficient of \\(-0.78\\).\n\n\nplot(y=mtcars$mpg,x=mtcars$hp, main=\"\", \n     axes=F,pch=21, bg=\"blue\",\n     xlab=\"Horse Power\",\n     ylab=\"Miles Per Gallon\", ylim=c(10,40),xlim=c(50,400))\naxis(side=1, labels=TRUE, font=1,las=1)\naxis(side=2, labels=TRUE, font=1,las=1)\nabline(lm(mtcars$mpg~mtcars$hp),\n       col=\"darkgray\",lwd=2)\n\n\n\n\n\n\n\n\n\nThe coefficient of determination is \\(0.6\\). This value is not very close to one. This is expected since miles per gallon can also vary because of the cars weight, and fuel efficiency. It makes sense that the hp only explains \\(60\\)% of the total variation.\n\nIn R we can calculate the coefficient of determination by squaring the correlation coefficient.\n\ncor(mtcars$mpg,mtcars$hp)^2\n\n[1] 0.6024373\n\n\n\n\nExercise 3\n\nIt seems like there is a direct relationship between both variables. The more debt you take, the higher the salary.\n\nStart by loading the data. We’ll use the read.csv() function:\n\nCollege&lt;-read.csv(\"https://jagelves.github.io/Data/College.csv\")\n\nThe two variables of interest are GRAD_DEBT_MDN and MD_EARN_WNE_P10. The following code creates the scatter plot:\n\nThe correlation coefficient shows a moderate direct relationship between earnings and debt \\(0.43\\). The coefficient of determination indicates that only \\(19\\)% of the variation in earnings can be explained by debt.\n\nIn R we can start with the correlation coefficient:\nThe coefficient of determination is:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regression I</span>"
    ]
  },
  {
    "objectID": "RegressionII.html",
    "href": "RegressionII.html",
    "title": "7  Regression II",
    "section": "",
    "text": "7.1 Concepts",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regression II</span>"
    ]
  },
  {
    "objectID": "RegressionII.html#concepts",
    "href": "RegressionII.html#concepts",
    "title": "7  Regression II",
    "section": "",
    "text": "The Regression Line\nThe regression line is fitted so that the average distance between the line and the sample points is as small as possible. The line is defined by a slope (\\(\\beta\\)) and an intercept (\\(\\alpha\\)). Mathematically, the regression line is expressed as \\(\\hat{y_i}=\\hat{\\alpha}+\\hat{\\beta}x_i\\), where \\(\\hat{y_i}\\) are the predicted values of \\(y\\) given the \\(x\\)’s.\n\nThe slope determines the steepness of the line. The estimate quantifies how much a unit increase in \\(x\\) changes \\(y\\). The estimate is given by \\(\\hat{\\beta}= \\frac {s_{xy}}{s_{x}^2}\\).\nThe intercept determines where the line crosses the \\(y\\) axis. It returns the value of \\(y\\) when \\(x\\) is zero. The estimate is given by \\(\\hat{\\alpha}=\\bar{y}-\\hat{\\beta}\\bar{x}\\).\n\n\n\nGoodness of Fit\nThere are a couple of popular measures that determine the goodness of fit of the regression line.\n\nThe coefficient of determination or \\(R^2\\) is the percent of the variation in \\(y\\) that is explained by changes in \\(x\\). The higher the \\(R^2\\) the better the explanatory power of the model. The \\(R^2\\) is always between [0,1]. To calculate use \\(R^2=SSR/SST\\).\n\n\\(SSR\\) (Sum of Squares due to Regression) is the part of the variation in \\(y\\) explained by the model. Mathematically, \\(SSR=\\sum{(\\hat{y_i}-\\bar{y})^2}\\).\n\\(SSE\\) (Sum of Squares due to Error) is the part of the variation in \\(y\\) that is unexplained by the model. Mathematically, \\(SSE=\\sum{(y_i-\\hat{y_i})^2}\\).\nSST (Sum of Squares Total) is the total variation of \\(y\\) with respect to the mean. Mathematically, \\(SST=\\sum{(y_i-\\bar{y})^2}\\).\nNote that \\(SST=SSR+SSE\\).\n\nThe adjusted \\(R^2\\) recognizes that the \\(R^2\\) is a non-decreasing function of the number of explanatory variables in the model. This metric penalizes a model with more explanatory variables relative to a simpler model. It is calculated by \\(1-(1-R^2) \\frac {n-1}{n-k-1}\\), where \\(k\\) is the number of explanatory variables used in the model and \\(n\\) is the sample size.\nThe Residual Standard Error estimates the average dispersion of the data points around the regression line. It is calculated by \\(s_e =\\sqrt{\\frac{SSE}{n-k-1}}\\).\n\n\n\nUseful R Functions\nThe lm() function to estimates the linear regression model.\nThe predict() function uses the linear model object to predict values. New data is entered as a data frame.\nThe coef() function returns the model’s coefficients.\nThe summary() function returns the model’s coefficients, and goodness of fit measures.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regression II</span>"
    ]
  },
  {
    "objectID": "RegressionII.html#exercises",
    "href": "RegressionII.html#exercises",
    "title": "7  Regression II",
    "section": "7.2 Exercises",
    "text": "7.2 Exercises\nThe following exercises will help you get practice on Regression Line estimation and interpretation. In particular, the exercises work on:\n\nEstimating the slope and intercept.\nCalculating measures of goodness of fit.\nPrediction using the regression line.\n\nAnswers are provided below. Try not to peak until you have a formulated your own answer and double checked your work for any mistakes.\n\nExercise 1\nFor the following exercises, make your calculations by hand and verify results using R functions when possible.\n\nConsider the data below. Calculate the deviations from the mean for each variable and use the results to estimate the regression line. Use R to verify your result. On average by how much does y increase per unit increase of x?\n\n\n\n\nx\n20\n21\n15\n18\n25\n\n\n\n\ny\n17\n19\n12\n13\n22\n\n\n\n\nCalculate SST, SSR, and SSE. Confirm your results in R. What is the \\(R^2\\)? What is the Standard Error estimate? Is the regression line a good fit for the data?\nAssume that x is observed to be 32, what is your prediction of y? How confident are you in this prediction?\n\n\n\nExercise 2\nYou will need the Education data set to answer this question. You can find the data set at https://jagelves.github.io/Data/Education.csv . The data shows the years of education (Education), and annual salary in thousands (Salary) for a sample of \\(100\\) people.\n\nEstimate the regression line using R. By how much does an extra year of education increase the annual salary on average? What is the salary of someone without any education?\nConfirm that the regression line is a good fit for the data. What is the estimated salary of a person with \\(16\\) years of education?\n\n\n\nExercise 3\nYou will need the FoodSpend data set to answer this question. You can find this data set at https://jagelves.github.io/Data/FoodSpend.csv .\n\nOmit any NA’s that the data has. Create a dummy variable that is equal to \\(1\\) if an individual owns a home and \\(0\\) if the individual doesn’t. Find the mean of your dummy variable. What proportion of the sample owns a home?\nRun a regression with Food being the dependent variable and your dummy variable as the independent variable. What is the interpretation of the intercept and slope?\nNow run a regression with Food being the independent variable and your dummy variable as the dependent variable. What is the interpretation of the intercept and slope? Hint: you might want to plot the scatter diagram and the regression line.\n\n\n\nExercise 4\nYou will need the Population data set to answer this question. You can find this data set at https://jagelves.github.io/Data/Population.csv .\n\nRun a regression of Population on Year. How well does the regression line fit the data?\nCreate a prediction for Japan’s population in 2030. What is your prediction?\nCreate a scatter diagram and include the regression line. How confident are you of your prediction after looking at the diagram?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regression II</span>"
    ]
  },
  {
    "objectID": "RegressionII.html#answers",
    "href": "RegressionII.html#answers",
    "title": "7  Regression II",
    "section": "7.3 Answers",
    "text": "7.3 Answers\n\nExercise 1\n\nThe regression lines is \\(\\hat{y}=-4.93+1.09x\\). For each unit increase in x, y increases on average \\(1.09\\).\n\nStart by generating the deviations from the mean for each variable. For x the deviations are:\n\nx&lt;-c(20,21,15,18,25)\n(devx&lt;-x-mean(x))\n\n[1]  0.2  1.2 -4.8 -1.8  5.2\n\n\nNext, find the deviations for y:\n\ny&lt;-c(17,19,12,13,22)\n(devy&lt;-y-mean(y))\n\n[1]  0.4  2.4 -4.6 -3.6  5.4\n\n\nFor the slope we need to find the deviation squared of the x’s. This can easily be done in R:\n\n(devx2&lt;-devx^2)\n\n[1]  0.04  1.44 23.04  3.24 27.04\n\n\nThe slope is calculated by \\(\\frac{\\sum_{i=i}^{n} (x_{i}-\\bar{x})(y_{i}-\\bar{y})}{\\sum_{i=i}^{n} (x_{i}-\\bar{x})^2}\\). In R we can just find the ratio between the summations of (devx)(devy) and devx2.\n\n(slope&lt;-sum(devx*devy)/sum(devx2))\n\n[1] 1.087591\n\n\nThe intercept is given by \\(\\bar{y}-\\beta(\\bar{x})\\). In R we find that the intercept is equal to:\n\n(intercept&lt;-mean(y)-slope*mean(x))\n\n[1] -4.934307\n\n\nOur results can be easily verified by using the lm() and coef() functions in R.\n\nfitEx1&lt;-lm(y~x)\ncoef(fitEx1)\n\n(Intercept)           x \n  -4.934307    1.087591 \n\n\n\nSST is \\(69.2\\), SSR is \\(64.82\\) and SSE is \\(4.38\\) (note that \\(SSR+SSE=SST\\)). The \\(R^2\\) is just \\(\\frac{SSR}{SST}=0.94\\) and the Standard Error estimate is \\(1.21\\). They both indicate a great fit of the regression line to the data.\n\nLet’s start by calculating the SST. This is just \\(\\sum{(y_{i}-\\bar{y})^2}\\).\n\n(SST&lt;-sum((y-mean(y))^2))\n\n[1] 69.2\n\n\nNext, we can calculate SSR. This is calculated by the following formula \\(\\sum{(\\hat{y_{i}}-\\bar{y})^2}\\). To obtain the predicted values in R, we can use the output of the lm() function. Recall our fitEx1 object created in Exercise 1. It has fitted.values included:\n\n(SSR&lt;-sum((fitEx1$fitted.values-mean(y))^2))\n\n[1] 64.82044\n\n\nThe ratio of SSR to SST is the \\(R^2\\):\n\n(R2&lt;-SSR/SST)\n\n[1] 0.9367115\n\n\nFinally, let’s calculate SSE \\(\\sum{(y_{i}-\\hat{y_{i}})^2}\\):\n\n(SSE&lt;-sum((y-fitEx1$fitted.values)^2))\n\n[1] 4.379562\n\n\nWith the SSE we can calculate the Standard Error estimate:\n\nsqrt(SSE/3)\n\n[1] 1.208244\n\n\nWe can confirm these results using the summary() function.\n\nsummary(fitEx1)\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n      1       2       3       4       5 \n 0.1825  1.0949  0.6204 -1.6423 -0.2555 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  -4.9343     3.2766  -1.506  0.22916   \nx             1.0876     0.1632   6.663  0.00689 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.208 on 3 degrees of freedom\nMultiple R-squared:  0.9367,    Adjusted R-squared:  0.9156 \nF-statistic:  44.4 on 1 and 3 DF,  p-value: 0.00689\n\n\n\nIf \\(x=32\\) then \\(\\hat{y}=29.87\\). The regression is a good fit, so we can feel good about our prediction. However, we would be concerned about the sample size of the data.\n\nIn R we can obtain a prediction by using the predict() function. This function requires a data frame as an input for new data.\n\npredict(fitEx1, newdata = data.frame(x=c(32)))\n\n       1 \n29.86861 \n\n\n\n\nExercise 2\n\nAn extra year of education increases the annual salary about \\(5,300\\) dollars (slope). A person that has no education would be expected to earn \\(17,2582\\) dollars (intercept).\n\nStart by loading the data in R:\n\nEducation&lt;-read.csv(\"https://jagelves.github.io/Data/Education.csv\")\n\nNext, let’s use the lm() function to estimate the regression line and obtain the coefficients:\n\nfitEducation&lt;-lm(Salary~Education, data = Education)\ncoefficients(fitEducation)\n\n(Intercept)   Education \n  17.258190    5.301149 \n\n\n\nThe \\(R^2\\) is \\(0.668\\) and the standard error is \\(21\\). The line is a moderately good fit. If someone has \\(16\\) years of experience, the regression line would predict a salary of \\(102,000\\) dollars.\n\nLet’s get the \\(R^2\\) and the Standard Error estimate by using the summary() function and fitEx1 object.\n\nsummary(fitEducation)\n\n\nCall:\nlm(formula = Salary ~ Education, data = Education)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-62.177  -9.548   1.988  15.330  45.444 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  17.2582     4.0768   4.233  5.2e-05 ***\nEducation     5.3011     0.3751  14.134  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.98 on 98 degrees of freedom\nMultiple R-squared:  0.6709,    Adjusted R-squared:  0.6675 \nF-statistic: 199.8 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nLastly, let’s use the regression line to predict the salary for someone who has \\(16\\) years of education.\n\npredict(fitEducation, newdata = data.frame(Education=c(16)))\n\n       1 \n102.0766 \n\n\n\n\nExercise 3\n\nApproximately, \\(36\\)% of the sample owns a home.\n\nStart by loading the data into R and removing all NA’s:\n\nSpend&lt;-read.csv(\"https://jagelves.github.io/Data/FoodSpend.csv\")\nSpend&lt;-na.omit(Spend)\n\nTo create a dummy variable for OwnHome we can use the ifelse() function:\n\nSpend$dummyOH&lt;-ifelse(Spend$OwnHome==\"Yes\",1,0)\n\nThe average of the dummy variable is given by:\n\nmean(Spend$dummyOH)\n\n[1] 0.3625\n\n\n\nThe intercept is the average food expenditure of individuals without homes (\\(6417\\)). The slope, is the difference in food expenditures between individuals that do have homes minus those who don’t. We then conclude that individuals that do have a home spend about \\(-2516\\) less on food than those who don’t have homes.\n\nTo run the regression use the lm() function:\n\nlm(Food~dummyOH,data=Spend)\n\n\nCall:\nlm(formula = Food ~ dummyOH, data = Spend)\n\nCoefficients:\n(Intercept)      dummyOH  \n       6473        -3418  \n\n\n\nThe scatter plot shows that most of the points for home owners are below \\(6000\\). For non-home owners they are mainly above \\(6000\\). The line can be used to predict the likelihood of owning a home given someones food expenditure. The intercept is above one, but still it gives us the indication that it is likely that low food expenditures are highly predictive of owning a home. The slope tells us how that likelihood changes as the food expenditures increase by 1. In general, the likelihood of owning a home decreases as the food expenditure increases.\n\nRun the lm() function once again:\n\nfitFood&lt;-lm(dummyOH~Food,data=Spend)\ncoefficients(fitFood)\n\n  (Intercept)          Food \n 1.4320766616 -0.0002043632 \n\n\nFor the scatter plot use the following code:\n\nplot(y=Spend$dummyOH,x=Spend$Food, \n     main=\"\", axes=F, pch=21, bg=\"blue\",\n     xlab=\"Food\",ylab=\"Dummy\")\naxis(side=1, labels=TRUE, font=1,las=1)\naxis(side=2, labels=TRUE, font=1,las=1)\nabline(fitFood,\n       col=\"gray\",lwd=2)\n\n\n\n\n\n\n\n\n\n\nExercise 4\n\nIf we follow the \\(R^2=0.81\\) the model fits the data very well.\n\nLet’s load the data from the web:\n\nPopulation&lt;-read.csv(\"https://jagelves.github.io/Data/Population.csv\")\n\nNow let’s filter the data so that we can focus on the population for Japan.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nJapan&lt;-filter(Population,Country.Name==\"Japan\")\n\nNext, we can run the regression of Population against the Year. Let’s also run the summary() function to obtain the fit and the coefficients.\n\nfit&lt;-lm(Population~Year,data=Japan)\nsummary(fit)\n\n\nCall:\nlm(formula = Population ~ Year, data = Japan)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-9583497 -4625571  1214644  4376784  5706004 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -988297581   68811582  -14.36   &lt;2e-16 ***\nYear            555944      34569   16.08   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4871000 on 60 degrees of freedom\nMultiple R-squared:  0.8117,    Adjusted R-squared:  0.8086 \nF-statistic: 258.6 on 1 and 60 DF,  p-value: &lt; 2.2e-16\n\n\n\nThe prediction for \\(2030\\) is about \\(140\\) million people.\n\nLet’s use the predict() function:\n\npredict(fit,newdata=data.frame(Year=c(2030)))\n\n        1 \n140268585 \n\n\n\nAfter looking at the scatter plot, it seems unlikely that the population in Japan will hit \\(140\\) million. Population has been decreasing in Japan!\n\nUse the plot() and abline() functions to create the figure.\n\nplot(y=Japan$Population,x=Japan$Year, main=\"\", \n     axes=F,pch=21, bg=\"#A7C7E7\",\n     xlab=\"Year\",\n     ylab=\"Population\")\naxis(side=1, labels=TRUE, font=1,las=1)\naxis(side=2, labels=TRUE, font=1,las=1)\nabline(fit,\n       col=\"darkgray\",lwd=2)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regression II</span>"
    ]
  },
  {
    "objectID": "ProbabilityI.html",
    "href": "ProbabilityI.html",
    "title": "8  Probability I",
    "section": "",
    "text": "8.1 Concepts",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability I</span>"
    ]
  },
  {
    "objectID": "ProbabilityI.html#concepts",
    "href": "ProbabilityI.html#concepts",
    "title": "8  Probability I",
    "section": "",
    "text": "Frequentist Vs. Bayesian\nThe frequentist interpretation assumes that probabilities represent proportions of specific events occurring over infinitely identical trials.\nThe Bayesian interpretation assumes that probabilities are subjective beliefs about the relative likelihood of events.\n\n\nExperiments and Sets\nAn experiment is a process that leads to one of several outcomes. Ex: Tossing a Die, Tossing a Coin, Drawing a Card, etc.\nAn outcome is the result of an experiment. Ex: A coin landing on heads, drawing the ace of spades.\nThe sample space \\((S)\\) of an experiment contains all possible outcomes of the experiment. Ex: \\(S\\)={\\(1\\),\\(2\\),\\(3\\),\\(4\\),\\(5\\),\\(6\\)} is the sample space for tossing a die.\nAn event is a subset of the sample space. \\(A\\)={\\(2\\),\\(4\\),\\(6\\)} is the event of tossing an even number when rolling a die.\n\n\nBasic Probability Concepts\nA probability is a numerical value that measures the likelihood that an event occurs.\nTo calculate probabilities, find the ratio between favorable outcomes and total outcomes. \\(p=favorable/total\\).\n\nThe probability of any event \\(A\\) is a value between \\(0\\) and \\(1\\) inclusive. Formally, \\(0\\leq P(A) \\leq1\\).\nWhen the probability of the event is \\(0\\) then the event is impossible. When the probability is \\(1\\) then the event is certain.\nThe sum of the probabilities of a list of mutually exclusive and exhaustive events equals \\(1\\). Formally, \\(\\sum P(x_i)=1\\).\n\nMutually exclusive events do not share any common outcomes. The occurrence of one event precludes the occurrence of others.\nExhaustive events include all outcomes in the sample space.\n\n\nTo assign probabilities you can use the Empirical, Classical, or Subjective Methods.\n\nEmpirical: calculated as a relative frequency of occurrence.\nClassical: based on logical analysis.\nSubjective: calculated by drawing on personal and subjective judgement.\n\n\n\nProbability Rules\nThe Complement Rule: \\(P(A^c)=1-P(A)\\), where \\(A^c\\) is the complement of \\(A\\).\nThe Addition Rule: \\(P(A \\cup B)=P(A)+P(B)-P(A \\cap B)\\), where \\(\\cap\\) is intersection and \\(\\cup\\) is union.\nThe Multiplication Rule:\n\nif events are dependent \\(P(A \\cap B)= P(A|B)P(B)\\), where \\(P(A|B)\\) is the conditional probability.\nif events are independent \\(P(A \\cap B)= P(A)P(B)\\).\n\nThe Law of Total Probability: \\(P(A)=P(A|B)P(B)+P(A|B^c)P(B^c)\\).\nBayes’ Theorem: \\(P(A|B)=P(B|A)P(A)/P(B)\\).\n\n\nCounting Rules\nThe Combination function counts the number of ways to choose \\(x\\) objects from a total of \\(n\\) objects. The order in which the \\(x\\) objects are listed does not matter.\n\nIf repetition is not allowed use \\(C_n^x= \\frac{n!}{(n-x)!x!}\\).\nIf repetition is allowed use \\(\\frac{(x+n-1)!}{(n-1)!x!}\\).\n\nThe Permutation function also counts the number of ways to choose \\(x\\) objects from a total of \\(n\\) objects. However, the order in which the \\(x\\) objects are listed does matter.\n\nIf repetition is not allowed use \\(P_n^x= \\frac{n!}{(n-x)!}\\).\nIf repetition is allowed use \\(n^x\\).\n\n\n\nUseful R Functions\nThe table() function can be used to construct frequency distributions.\nThe factorial() function returns the factorial of a number.\nThe gtools package contains the combinations() and permutations() functions used to calculate combinations and permutations. Use the repeats.allowed argument to specify counting with repetition or no repetition. The v argument allows you to specify a vector of elements.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability I</span>"
    ]
  },
  {
    "objectID": "ProbabilityI.html#exercises",
    "href": "ProbabilityI.html#exercises",
    "title": "8  Probability I",
    "section": "8.2 Exercises",
    "text": "8.2 Exercises\nThe following exercises will help you practice some probability concepts and formulas. In particular, the exercises work on:\n\nCalculating simple probabilities.\nApplying probability rules.\nUsing counting rules.\n\nAnswers are provided below. Try not to peak until you have a formulated your own answer and double checked your work for any mistakes.\n\nExercise 1\nFor the following exercises, make your calculations by hand and verify results with a calculator or R.\n\nA sample space \\(S\\) yields five equally likely events, \\(A\\), \\(B\\), \\(C\\), \\(D\\), and \\(E\\). Find \\(P(D)\\), \\(P(B^c)\\), and \\(P(A \\cup C \\cup E)\\).\nConsider the roll of a die. Define \\(A\\) as {1,2,3}, \\(B\\) as {1,2,3,5,6}, \\(C\\) as {4,6}, and \\(D\\) as {4,5,6}. Are the events \\(A\\) and \\(B\\) mutually exclusive, exhaustive, both or none? What about events \\(A\\) and \\(D\\)?\nA recent study suggests that \\(33.1\\)% of the adult U.S. population is overweight and \\(35.7\\)% obese. What is the probability that a randomly selected adult in the U.S. is either obese or overweight? What is the probability that their weight is normal? Are the events mutually exclusive and exhaustive?\n\n\n\nExercise 2\nFor the following exercises, make your calculations by hand and verify results with a calculator or R.\n\nLet \\(P(A)=0.65\\), \\(P(B)=0.3\\), and \\(P(A|B)=0.45\\). Calculate \\(P(A \\cap B)\\), \\(P(A \\cup B)\\), and \\(P(B|A)\\).\nLet \\(P(A)=0.4\\), \\(P(B)=0.5\\), and \\(P(A^c \\cap B^c)=0.24\\). Calculate \\(P(A^c|B^c)\\), \\(P(A^c \\cup B^c)\\), and \\(P(A \\cup B)\\).\nStock \\(A\\) will rise in price with a probability of \\(0.4\\), stock \\(B\\) will rise with a probability of \\(0.6\\). If stock \\(B\\) rises in price, then \\(A\\) will also rise with a probability of \\(0.5\\). What is the probability that at least one of the stocks will rise in price? Prove that events \\(A\\) and \\(B\\) are (are not) mutually exclusive (independent).\n\n\n\nExercise 3\n\nCreate a joint probability table from the contingency table below. Find \\(P(A)\\), \\(P(A \\cap B)\\), \\(P(A|B)\\), and \\(P(B|A^c)\\). Determine whether the events are independent or mutually exclusive.\n\n\n\n\n\n\\(B\\)\n\\(B^c\\)\n\n\n\\(A\\)\n26\n34\n\n\n\\(A^c\\)\n14\n26\n\n\n\n\n\nExercise 4\nYou will need the Crash data set and R to answer this question. The data shows information on several car crashes. Specifically, if the crash was Head-On or Not Head-On and whether there was Daylight or No Daylight. You can find the data here: https://jagelves.github.io/Data/Crash.csv\n\nCreate a contingency table.\nFind the probability that a) a car crash is Head-On, b) a car crash is in daylight c) a car crash is Head-On given that there is daylight.\nShow that Crashes and Light are dependent.\n\n\n\nExercise 5\n\nUse Bayes’ Theorem in the following question. Let \\(P(A)=0.7\\), \\(P(B|A)=0.55\\), and \\(P(B|A^c)=0.10\\). Find \\(P(A^c)\\), \\(P(A \\cap B)\\), \\(P(A^c \\cap B)\\), \\(P(B)\\), and \\(P(A|B)\\).\nSome find tutors helpful when taking a course. Julia has a 40% chance to fail a course if she does not have a tutor. With a tutor, the probability of failing is only 10%. There is a 50% chance that Julia finds an available tutor. What is the probability that Julia will fail the course? If she ends up failing the course, what is the probability that she had a tutor?\n\n\n\nExercise 6\n\nCalculate the following values and verify your results using R. a) 3!, b) 4!, c) \\(C_6^8\\), d) \\(P_6^8\\).\nThere are 10 players in a local basketball team. If we chose 5 players to randomly start a game, in how many ways can we select the five players if order doesn’t matter? What if order matters?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability I</span>"
    ]
  },
  {
    "objectID": "ProbabilityI.html#answers",
    "href": "ProbabilityI.html#answers",
    "title": "8  Probability I",
    "section": "8.3 Answers",
    "text": "8.3 Answers\n\nExercise 1\n\n\\(P(D)=1/5=0.2\\) since all events are equally likely. \\(P(B^c)=4/5=0.8\\), and \\(P(A \\cup C \\cup E)=P(A + C + E)=3/5=0.6\\).\nEvents \\(A\\) and \\(B\\) are not mutually exclusive since they share some of the same elements. They are not exhaustive since the union of both doesn’t create the sample space.\nThe probability is \\(68.8\\)%. The events are mutually exclusive. If someone is classified as obese, the person is not classified again as overweight. The events are not exhaustive since there are people in the U.S. that have a normal weight. The probability that the person drawn has normal weight is \\(31.2\\)%.\n\n\n\nExercise 2\n\nFrom the multiplication rule, \\(P(A|B)*P(B)=P(A \\cap B)\\).\nSubstituting values yields, \\(P(A \\cap B)=0.45*0.3=0.135\\).\nFrom the addition rule, \\(P(A \\cup B)=P(A)+P(B)-P(A \\cap B)\\).\nSubstituting yields, \\(P(A \\cup B)=0.65+0.3-0.135=0.815\\).\nFrom the multiplication rule once again, \\(P(B|A)=\\frac{P(A \\cap B)}{P(A)}\\). Substituting yields, \\(P(B|A)=0.135/0.65=0.2076923\\).\nFrom the complement rule we have that \\(P(A^c)=0.6\\) and \\(P(B^c)=0.5\\).\nUsing the multiplication rule, \\(P(A^c|B^c)=\\frac{P(A^c \\cap B^c)}{P(B^c)}\\). Substituting yields \\(P(A^c|B^c)=0.24/0.5=0.48\\).\nFrom the addition rule \\(P(A^c \\cup B^c)=P(A^c)+P(B^c)-P(A^c \\cap B^c)\\).\nSubstituting yields \\(P(A^c \\cup B^c)=0.6+0.5-0.24=0.86\\).\nThe event that has no elements of \\(A\\) or \\(B\\) is given by \\(P(A^c \\cap B^c)\\). Therefore \\(P(A \\cup B)=1-0.24=0.76\\) has all the elements of A and B.\nIn short the problem states \\(P(A)=0.4\\), \\(P(B)=0.6\\), and \\(P(A|B)=0.5\\). Where \\(A\\) and \\(B\\) are events of stocks rising in price. The question asks for \\(P(A \\cup B)=P(A)+P(B)-P(A \\cap B)\\).\nUsing the multiplication rule \\(P(A \\cap B)=0.5*0.6=0.3\\).\nHence, \\(P(A \\cup B)=0.4+0.6-0.3=0.7\\).\nThe events are not mutually exclusive since \\(P(A \\cap B)=0.3 \\neq 0\\).\nThe events are also not independent since \\(P(A|B)=0.5 \\neq 0.4=P(A)\\).\n\n\n\nExercise 3\n\nBelow is the joint probability table. The \\(P(A)=0.26+0.34=0.6\\), \\(P(A \\cap B)=0.26\\), \\(P(A|B)=0.26/0.4=0.65\\), and \\(P(B|A^c)=0.14/0.4=0.35\\). Events \\(A\\) and \\(B\\) are not independent since \\(P(A) \\neq P(A|B)\\). The events are not mutually exclusive since \\(P(A \\cap B)=0.26 \\neq 0\\).\n\n\n\n\n\n\\(B\\)\n\\(B^c\\)\nTotal\n\n\n\\(A\\)\n0.26\n0.34\n0.6\n\n\n\\(A^c\\)\n0.14\n0.26\n0.4\n\n\nTotal\n0.4\n0.6\n1\n\n\n\n\n\nExercise 4\n\nThe probability of a Head-On crash is \\((166+108)/4858=0.056\\). The probability of a daylight crash is \\((166+3258)/4858=0.70\\). The probability that the car crash is Head-On given daylight is \\(166/(166+3258)=0.048\\).\n\nStart by loading the data into R.\n\nCrash&lt;-read.csv(\"https://jagelves.github.io/Data/Crash.csv\")\n\nTo create a contingency table use the table() command in R.\n\n(freq&lt;-table(Crash$Crash.Type,Crash$Light.Condition))\n\n             \n              Daylight Not Daylight\n  Head-on          166          108\n  Not Head-On     3258         1326\n\n\nThis table is used to calculate probabilities. We can pass it through the prop.table() function to get the contingency table.\n\nround(prop.table(freq),2)\n\n             \n              Daylight Not Daylight\n  Head-on         0.03         0.02\n  Not Head-On     0.67         0.27\n\n\n\nThe two variables are dependent since \\(P(Head-On|Daylight) \\neq P(Head-On)\\), that is \\(0.048 \\neq 0.56\\).\n\n\n\nExercise 5\n\n\\(P(A^c)=1-P(A)=1-0.7=0.3\\), \\(P(A \\cap B)=𝑃(𝐵|𝐴)𝑃(𝐴) = 0.55(0.70) = 0.385\\), \\(P(A^c \\cap B)=𝑃(B|A^c)𝑃(A^c) = 0.10(0.30) = 0.03\\), \\(P(B)= 𝑃(A \\cap B) + 𝑃(𝐴^c \\cap 𝐵) = 0.385 + 0.03 = 0.415\\), and \\(P(A|B)= \\frac{𝑃(A \\cap B)}{P(B)}=0.385/0.415=0.9277\\).\nLet the event of failing be \\(F\\), the event of not failing be \\(NF\\), the event of having a tutor be \\(T\\), and the event of not having a tutor be \\(NT\\). The probability of failing the course is \\(0.25\\). \\((𝐹) = 𝑃(𝐹 \\cap 𝑇) + 𝑃(𝐹 \\cap 𝑇^c) = 𝑃(𝐹|𝑇)𝑃(𝑇) + 𝑃(𝐹|𝑇^c)𝑃(𝑇^c) = 0.10(0.50) + 0.40(0.50) = 0.05 + 0.20 = 0.25\\) The probability of not having a tutor, given that she failed the course is \\(0.2\\). \\(P(𝑇|𝐹) = \\frac{𝑃(𝐹\\cap𝑇)}{𝑃(𝐹\\cap𝑇)+𝑃(𝐹 \\cap𝑇^c)}= 0.05/0.25 = 0.20\\)\n\n\n\nExercise 6\n\n\\(3!=3 \\times 2 \\times 1=6\\), \\(4!=6 \\times 4=24\\), \\(C_6^8=28\\), and \\(P_6^8=20,160\\)\n\nIn R we can just use the factorial command. So \\(3!\\) is:\n\nfactorial(3)\n\n[1] 6\n\n\nand \\(4!\\) is:\n\nfactorial(4)\n\n[1] 24\n\n\nFor combinations and permutations we can use the gtools package:\n\nlibrary(gtools)\nC&lt;-combinations(8,6)\nnrow(C)\n\n[1] 28\n\n\n\nIf order doesn’t matter, there are \\(252\\) ways. If order matters, then there are \\(30,240\\) ways.\n\nIn R we can once more use the combination and permutation functions:\n\nB1&lt;-combinations(10,5)\nnrow(B1)\n\n[1] 252\n\n\n\nB2&lt;-permutations(10,5)\nnrow(B2)\n\n[1] 30240",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability I</span>"
    ]
  },
  {
    "objectID": "ProbabilityII.html",
    "href": "ProbabilityII.html",
    "title": "9  Probability II",
    "section": "",
    "text": "9.1 Concepts",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability II</span>"
    ]
  },
  {
    "objectID": "ProbabilityII.html#concepts",
    "href": "ProbabilityII.html#concepts",
    "title": "9  Probability II",
    "section": "",
    "text": "Random Variables\nA random variable associates a numerical value with each possible experimental outcome. Specifically, the random variable takes on a value with some probability.\nA random variable is fully characterized by its probability density function (PDF) if continuous or the probability mass function (PMF) if discrete.\n\n\nExpected Value and Variance\nWhen summarizing a random variable, we are mostly interested in the variable’s central tendency (Expected Value) and dispersion (Variance).\nThe expected value (mean) is a measure of central location. For a discrete random variable it is given by \\(E(x)=\\mu=\\sum xf(x)\\), where \\(f(x)\\) is the probability mass function. For a continuous random variable it is given by \\(E(x)= \\int_{-\\infty}^{\\infty} x f(x) dx\\), where \\(f(x)\\) is the probability density function.\nThe variance summarizes the deviation of the values of the random variable from the mean. It is calculated by \\(var(x)=E[(x-E(x))^2]=E[x^2]-E[x]^2\\). Note that this formula can be used for both discrete and continuous random variables.\n\n\nDiscrete Uniform Distribution\nThe discrete uniform distribution is a probability distribution that assigns equal probability to each outcome in a finite set of possible outcomes. In other words, each outcome in the set is equally likely to occur.\nThe probability mass function is given by \\(f(x)=1/n\\), where \\(n\\) is the number of elements in the sample space (all possible outcomes).\nThe expected value is given by \\(E(x)=\\frac {\\sum x_i}{n}\\), where \\(x_i\\) are the possible values, and \\(n\\) is the number of possible values.\nThe variance is given by \\(var(x)=\\frac {\\sum (x_i-E(x))^2}{n-1}\\).\n\n\nBinomial Distribution\nThe binomial distribution is a probability distribution that describes the outcome of a sequence of \\(n\\) independent Bernoulli trials. In a Bernoulli trial, there are only two possible outcomes: “success” and “failure”. The probability of success is denoted by \\(p\\), and the probability of failure is denoted by \\(q = 1 - p\\). In a sequence of \\(n\\) independent Bernoulli trials, the number of successes (\\(x\\)) is a random variable that follows a binomial distribution.\nThe probability mass function is given by \\(f(x)=C_x^n (p^x)(1-p)^{n-x}\\), where \\(n\\) is the number of trials, \\(x\\) is the number of successes, \\(p\\) is the probability of success, and \\(C_x^n\\) is the number of ways there can be \\(x\\) successes in \\(n\\) trials.\nThe expected value of the binomial distribution is \\(E(x)=np\\).\nThe variance of the binomial distribution is \\(var(x)=np(1-p)\\).\n\n\nThe Hypergeometric Distribution\nThe hypergeometric distribution is a probability distribution that describes the outcome of drawing a sample from a population without replacement. It is used to calculate the probability of drawing a certain number of successes (\\(x\\)) in a sample of a given size (\\(n\\)), where the success or failure of each individual draw is not dependent on the success or failure of other draws.\nThe hypergeometric experiment differs from the binomial since:\n\ntrials are not independent.\nthe probability of success changes from trial to trial.\n\nThe probability mass function is given by \\(f(x)=\\frac {C_x^r C_{n-x}^{N-r}}{C_n^N}\\), where \\(n\\) is the number of trials, \\(x\\) is the number of successes, \\(r\\) is the number of elements in the population labeled as success, and \\(N\\) is the number of elements in the population.\nThe expected value of the hypergeometric distribution is \\(E(x)=n \\frac {r}{N}\\).\nThe variance of the hypergeometric distribution is \\(var(x)= n \\frac {r}{N} (1- \\frac {r}{N}) (\\frac {N-n}{N-1})\\).\n\n\nPoisson Distribution\nThe Poisson distribution estimates the number of successes (\\(x\\)) over a specified interval of time or space.\nThe probability mass function is given by \\(f(x)= \\frac {\\mu e^{-x}}{x!}\\), where \\(\\mu\\) is the expected number of successes in any given interval and also the variance, and \\(e\\) is Euler’s number (2.71828…).\nAn experiment satisfies a Poisson process if:\n\nThe number of successes with a specified time or space interval equals any integer between zero and infinity.\nThe number of successes counted in non-overlapping intervals are independent.\nThe probability of success in any interval is the same for all intervals of equal size and is proportional to the size of the interval.\n\n\n\nUseful R Functions\nTo calculate probabilities based on discrete random variables use the pbinom(), phyper(), and ppois() functions. For the uniform distribution use the extraDistr package and the pdunif() function.\nTo calculate cumulative probabilities use the dbinom(), dhyper(), dpois(), and ddunif() functions.\nTo calculate quantiles use the qbinom(), qhyper(), qpois(), and qdunif() functions.\nTo generate random numbers use the rbinom(), rhyper(), rpois(), and rdunif() functions.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability II</span>"
    ]
  },
  {
    "objectID": "ProbabilityII.html#exercises",
    "href": "ProbabilityII.html#exercises",
    "title": "9  Probability II",
    "section": "9.2 Exercises",
    "text": "9.2 Exercises\nThe following exercises will help you practice some probability concepts and formulas. In particular, the exercises work on:\n\nCalculating probabilities for discrete random variables.\nCalculating the expected value and standard deviation.\nApplying the binomial, Poisson and hypergeometric probability distributions.\n\nAnswers are provided below. Try not to peak until you have a formulated your own answer and double checked your work for any mistakes.\n\nExercise 1\nFor the following exercises, make your calculations by hand and verify results with a calculator or R.\n\nConsider the table below. Calculate the mean and standard deviation. What is the probability that \\(x&lt;15\\)?\n\n\n\n\n\\(x\\)\n5\n10\n15\n20\n\n\n\\(P(X=x)\\)\n0.35\n0.3\n0.2\n0.15\n\n\n\n\nConsider the table below. Calculate the mean and standard deviation. What is the probability that \\(x\\geq-9\\)?\n\n\n\n\n\\(y\\)\n-23\n-17\n-9\n-3\n\n\n\\(P(Y=y)\\)\n0.5\n0.25\n0.15\n0.1\n\n\n\n\nThe returns on a couple of funds depends on the state of the economy. The economy is expected to be Good with a probability of 20%, Fair with probability of 50% and Poor with probability of 30%. Which fund would you choose if you want to maximize your return? What would you choose if you really dislike risk?\n\n\n\n\nState of Economy\nFund 1\nFund 2\n\n\n\n\nGood\n20\n40\n\n\nFair\n10\n20\n\n\nPoor\n-10\n-40\n\n\n\n\n\nExercise 2\n\nUse the table below. A portfolio has 200,000 dollars invested in Asset \\(X\\) and 300,000 dollars in asset \\(Y\\). If the correlation coefficient between the two investments is \\(0.4\\), what is the expected return and standard deviation of the portfolio?\n\n\n\n\nMeasure\nX\nY\n\n\n\n\nExpected Return (%)\n8\n12\n\n\nStandard Deviation (%)\n12\n20\n\n\n\n\n\nExercise 3\n\nLet \\(Z\\) be a binomial random variable with \\(n=5\\) and \\(p=0.35\\) use the binomial formula to find \\(P(Z=1)\\), \\(P(Z \\geq 2)\\). What is the expected value and standard deviation of \\(Z\\)?\nLet \\(W\\) be a binomial random variable with \\(n=200\\) and \\(p=0.77\\) use the binomial formula to find \\(P(W&gt;160)\\), \\(P(155 \\leq W \\leq 165)\\). What is the expected value and standard deviation of \\(W\\)?\nSixty percent of a firm’s employees are men. Suppose four of the firm’s employees are randomly selected. What is more likely, finding three men and one woman, or two men and one woman? Does your answer change if the proportion falls to \\(50\\)%?\n\n\n\nExercise 4\n\nAssume that \\(S\\) is a Poisson process with mean of \\(\\mu=1.5\\). Calculate \\(P(S=2)\\) and \\(P(S \\geq 2)\\). What is the mean and standard deviation of \\(S\\)?\nAssume that \\(T\\) is a Poisson process with mean of \\(\\mu=20\\). Calculate \\(P(T=14)\\) and \\(P(18 \\leq T \\leq 23)\\).\nA local pharmacy administers on average \\(84\\) Covid-19 vaccines per week. The vaccines shots are evenly administered across all days. Find the probability that the number of vaccine shots administered on a Wednesday is more than eight but less than \\(12\\).\n\n\n\nExercise 5\n\nAssume that \\(X\\) is a hypergeometric random variable with \\(N=25\\), \\(S=3\\), and \\(n=4\\). Calculate \\(P(X=0)\\), \\(P(X=1)\\), and \\(P(X \\leq 1)\\).\nCompute the probability of at least eight successes in a random sample of \\(20\\) items obtained from a population of \\(100\\) items that contains \\(25\\) successes. What are the expected value and standard deviation of the number of successes?\nFor \\(1\\) dollar a player gets to select six numbers for the base game of Powerball. In the game, five balls are randomly drawn from 59 consecutively numbered white balls. One ball, called the Powerball, is randomly drawn from \\(39\\) consecutively numbered red balls. What is the probability that a player is able to match two out of five randomly drawn white balls? What is the probability of winning the jackpot?",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability II</span>"
    ]
  },
  {
    "objectID": "ProbabilityII.html#answers",
    "href": "ProbabilityII.html#answers",
    "title": "9  Probability II",
    "section": "9.3 Answers",
    "text": "9.3 Answers\n\nExercise 1\n\nThe expected value is \\(10.75\\) and the standard deviation is \\(5.31\\). The probability of \\(x&lt;15\\) is \\(0.65\\).\n\nIn R we can create vectors for both \\(x\\) and the probabilities \\(P(X=x)\\).\n\nx&lt;-c(5,10,15,20)\npx&lt;-c(0.35,0.3,0.2,0.15)\n\nThe expected value is the sum product of probabilities and values. Formally, \\(\\sum_{i=1}^{n}x_{i}p_{i}\\) and in R:\n\n(ex&lt;-sum(x*px))\n\n[1] 10.75\n\n\nThe standard deviation is given by \\(\\sqrt{\\sum_{i=1}^{n}(x_{i}-\\mu)^2p_{i}}\\). We can calculate it in R with the following code:\n\n(sd&lt;-sqrt(sum((x-ex)^2*px)))\n\n[1] 5.30919\n\n\n\nThe expected value is \\(-17.4\\) and the standard deviation is \\(6.86\\). The probability of is \\(0.25\\).\n\nLet’s create the vectors once more in R.\n\ny&lt;-c(-23,-17,-9,-3)\npy&lt;-c(0.5,0.25,0.15,0.1)\n\nThe expected value is given by:\n\n(ey&lt;-sum(y*py))\n\n[1] -17.4\n\n\nThe standard deviation is given by:\n\n(sdy&lt;-sqrt(sum((y-ey)^2*py)))\n\n[1] 6.858571\n\n\n\nBoth funds have the same expected return of \\(6\\). The safest return comes from fund 1 since the standard deviation is only \\(11.14\\) vs. \\(31.05\\) for fund 2.\n\nIn R we can create a data frame with probabilities and the performance of the funds.\n\nfunds&lt;-data.frame(probs=c(0.2,0.5,0.3),fund1=c(20,10,-10), fund2=c(40,20,-40))\n\nLet’s create a function for the expected value and standard deviation. For the expected value:\n\nExpected_Value&lt;-function(x,p){\n  sum(x*p)\n}\n\nNow we can use the formula to calculate the expected value of fund1:\n\nExpected_Value(funds$fund1,funds$probs)\n\n[1] 6\n\n\nand fund 2:\n\nExpected_Value(funds$fund2,funds$probs)\n\n[1] 6\n\n\nFor the standard deviation we can create another function:\n\nStandard_Deviation&lt;-function(x,p){\n  sqrt(sum((x-Expected_Value(x,p))^2*p))\n}\n\nUsing the function to get the standard deviation of fund 1 we get:\n\nStandard_Deviation(funds$fund1,funds$probs)\n\n[1] 11.13553\n\n\nand for fund 2:\n\nStandard_Deviation(funds$fund2,funds$probs)\n\n[1] 31.04835\n\n\n\n\nExercise 2\n\nThe expected return of the portfolio is \\(10.4\\) and the standard deviation is \\(14.60\\).\n\nIn R we can start by calculating the expected return. This is given by the formula \\(\\alpha R_{1} +\\beta R_{2}\\):\n\n(ER&lt;-(2/5)*8+(3/5)*12)\n\n[1] 10.4\n\n\nNext we can find the standard deviation with the formula \\(\\sqrt{\\alpha^2 \\sigma_{1}^2+\\beta^2 \\sigma_{2}+\\alpha \\beta \\rho \\sigma_{1} \\sigma_{2}}\\):\n\n(Risk&lt;-sqrt(0.4^2*12^2 + 0.6^2*20^2+2*0.4*0.6*0.4*12*20))\n\n[1] 14.59863\n\n\n\n\nExercise 3\n\n\\(P(Z=1)=0.31\\), and \\(P(Z \\geq 2)=0.57\\). The expected value is \\(np=1.75\\) and the standard deviation is \\(\\sqrt{np(1-p)}=1.067\\).\n\nLet’s use R and the dbinom() function to find \\(P(Z=1)\\).\n\ndbinom(1,5,0.35)\n\n[1] 0.3123859\n\n\nWe can now use pbinom() to find the cumulative distribution. Since we want the right tale of the distribution, we will specify this with an argument.\n\npbinom(1,5,0.35, lower.tail=F)\n\n[1] 0.571585\n\n\n\n\\(P(W&gt;160)=0.14\\), and \\(P(155 \\leq W \\leq 165 )=0.45\\). The expected value is \\(np=154\\) and the standard deviation is \\(\\sqrt{np(1-p)}=5.95\\).\n\nUsing the pbinom() function we find that \\(P(W&gt;160)\\).\n\npbinom(160,200,0.77, lower.tail = F)\n\n[1] 0.136611\n\n\nWe make two calculations to find the probability. First, \\(P(W \\leq 165)\\) and then \\(P(W \\geq 154)\\). The difference between these two, gives us the desired outcome.\n\npbinom(165,200,0.77, lower.tail=T)-pbinom(154,200,0.77, lower.tail=T)\n\n[1] 0.4487104\n\n\n\nThe probabilities are the same. Each event has a probability of \\(0.3456\\). If the probability changes to \\(0.5\\) now the event of two women and two men is more likely.\n\nLet’s calculate the probabilities in R. First, the probability of three men and one woman.\n\ndbinom(3,4,0.6)\n\n[1] 0.3456\n\n\nNow the probability of two men and two women.\n\ndbinom(2,4,0.6)\n\n[1] 0.3456\n\n\nChanging the probabilities reveals that:\n\ndbinom(3,4,0.5)\n\n[1] 0.25\n\n\n\ndbinom(2,4,0.5)\n\n[1] 0.375\n\n\nHaving two of each is the most likely outcome.\n\n\nExercise 4\n\nThe \\(P(S=2)=0.25\\) and \\(P(S \\geq 2)=0.44\\). The expected value and the variance is \\(1.5\\).\n\nIn R we will make use of the dpois() function:\n\ndpois(2,1.5)\n\n[1] 0.2510214\n\n\nFor the second probability we will use ppois():\n\nppois(1,1.5, lower.tail=F)\n\n[1] 0.4421746\n\n\n\nThe \\(P(T=14)=0.039\\) and \\(P(18 \\leq T \\leq 23)=0.49\\).\n\nUsing the dpois() function once more:\n\ndpois(14,20)\n\n[1] 0.03873664\n\n\nFor the second probability we will find the difference between two probabilities:\n\nppois(23,20, lower.tail=T)-ppois(17,20, lower.tail=T)\n\n[1] 0.4904644\n\n\n\nThe probability of administering more than \\(8\\) but less than \\(12\\) shots is \\(0.3\\).\n\nLet’s first note that if \\(84\\) shots are administered on average weekly, then \\(12\\) are administered daily. Now we can use this average and the ppois() function to find the probability:\n\nppois(11,12)-ppois(8,12)\n\n[1] 0.3065696\n\n\n\n\nExercise 5\n\n\\(P(X=0)=0.58\\), \\(P(X=1)=0.37\\), and \\(P(X \\leq 1)=0.94\\).\n\nIn R we can use the dhyper() function\n\ndhyper(0,3,22,4)\n\n[1] 0.5782609\n\n\nonce more for the second probability:\n\ndhyper(1, 3, 22, 4)\n\n[1] 0.3652174\n\n\nFor the last probability we can add the previous probabilities or use the phyper() function:\n\nphyper(1, 3, 22, 4)\n\n[1] 0.9434783\n\n\n\nThe probability is \\(0.545\\).\n\nIn R we use the dhyper() function once more:\n\ndhyper(0, 2, 10, 3)\n\n[1] 0.5454545\n\n\n\nThe probability of matching two white balls is \\(5%\\). Winning the jackpot is extremely unlikely! A probability of \\(0.00000000512\\). It is more likely to be struck by lightning according to the CDC.\n\nIn R use the dhyper() function:\n\ndhyper(2, 5, 54, 5)\n\n[1] 0.04954472\n\n\nFor the jackpot we first calculate the probability of getting all of the white balls.\n\noptions(digits = 5,scipen=999)\ndhyper(5, 5, 54, 5)\n\n[1] 0.00000019974\n\n\nNow the probability of getting the Powerball.\n\ndhyper(1, 1, 38, 1)\n\n[1] 0.025641\n\n\nSince the two events are independent, we can multiply them to find the probability of a jackpot.\n\ndhyper(5, 5, 54, 5)*dhyper(1, 1, 38, 1)\n\n[1] 0.0000000051217",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability II</span>"
    ]
  },
  {
    "objectID": "ProbabilityIII.html",
    "href": "ProbabilityIII.html",
    "title": "10  Probability III",
    "section": "",
    "text": "10.1 Concepts",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Probability III</span>"
    ]
  },
  {
    "objectID": "ProbabilityIII.html#concepts",
    "href": "ProbabilityIII.html#concepts",
    "title": "10  Probability III",
    "section": "",
    "text": "Continuous Random Variables\nContinuous random variables are characterized by their probability density function \\(f(x)\\). The probability density function does not directly provide probabilities!\nThe probability of a continuous random variable assuming a single value is zero. Instead, probabilities are defined for intervals. These are calculated by areas under the PDF curve (integral).\n\n\nUniform Distribution\nThe uniform probability density function is given by \\(f(x)= \\frac {1}{b-a}\\) when \\(a \\leq x \\leq b\\) and \\(0\\) otherwise.\nThe expected value of the uniform distribution is \\(E(x)= \\frac {a+b}{2}\\).\nThe variance of the uniform distribution is \\(var(x)= \\frac {(b-a)^2} {12}\\)\n\n\nNormal Distribution\nThe normal PDF is given by \\(f(x)= \\frac {1}{\\sigma \\sqrt{2\\pi}} e^{\\frac {-1}{2} (\\frac {x-\\mu}{\\sigma})}\\), where \\(\\mu\\) is the mean, \\(\\sigma\\) is the standard deviation, \\(\\pi\\) is 3.1415… , and \\(e\\) is 2.7282… . The normal distribution has the following properties:\n\nThe normal curve is symmetrical about the mean \\(\\mu\\).\nThe mean is at the middle and divides the area of the distribution into halves.\nThe total area under the curve is equal to 1.\nThe distribution is completely determined by its mean and standard deviation.\n\nThe standard normal distribution has a mean of \\(0\\) and a standard deviation of \\(1\\).\n\n\nExponential Distribution\nThe exponential distribution is useful in computing probabilities for the time it takes to complete a task. It describes the time between events in a Poisson process.\nThe probability density function is given by \\(f(x)=\\frac {1}{\\mu}e^{ \\frac {-x}{\\mu}}\\).\n\n\nTriangular Distribution\nThe triangular distribution is characterized by a single mode (the peak of the distribution) and two boundaries. It is often used in situations where the lower and upper bounds of a potential outcome are known, but the exact likelihood of the outcome is uncertain.\nThe probability density function is given by \\(f(x)=\\frac {2(x-a)}{(b-a)(c-a)}\\) for \\(a \\leq x &lt; c\\); \\(f(x)=\\frac {2}{(b-a)}\\) for \\(x=c\\); \\(f(x)=\\frac {2(b-x)}{(b-a)(b-c)}\\) for \\(c &lt; x \\leq b\\), and \\(f(x)=0\\) otherwise.\nThe expected value of the distribution is \\(E(x)= \\frac {a+b+c}{3}\\).\nThe variance of the triangular distribution is \\(var(x) = \\frac {a^2+b^2+c^2-ab-ac-bc}{18}\\).\n\n\nUseful R Functions\nTo calculate the density of continuous random variables use the dunif(), dnorm(), and dexp() functions. For the triangular distribution use the extraDistr package and the dtriang() function.\nTo calculate probabilities of continuous random variables use the punif(), pnorm(), pexp(), and ptriang() functions.\nTo calculate quartiles of continuous random variables use the qunif(), qnorm(),qexp(), and qtriang() functions.\nTo calculate generate random variables based on continuous random variables use the runif(), rnorm(), rexp(), and rtriang() functions.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Probability III</span>"
    ]
  },
  {
    "objectID": "ProbabilityIII.html#exercises",
    "href": "ProbabilityIII.html#exercises",
    "title": "10  Probability III",
    "section": "10.2 Exercises",
    "text": "10.2 Exercises\nThe following exercises will help you practice some probability concepts and formulas. In particular, the exercises work on:\n\nCalculating probabilities for continuous random variables.\nCalculating the expected value and standard deviation.\nApplying the uniform, normal, and exponential distributions.\n\nAnswers are provided below. Try not to peak until you have a formulated your own answer and double checked your work for any mistakes.\n\nExercise 1\nFor the following exercises, make your calculations by hand and verify results with a calculator or R.\n\nA random variable \\(X\\) follows a continuous uniform distribution with minimum of \\(-2\\) and maximum of \\(4\\). Determine the height of the density function \\(f(x)\\), the mean, the standard deviation, and calculate \\(P(X \\leq -1)\\).\nYour internet provider will arrive sometime between 10:00 am and 12:00 pm. Suppose you have to run a quick errand at 10:00 am. If it takes \\(15\\) minutes to run the errand, what is the probability that you will be back before the internet provider arrives? What if you take \\(30\\) minutes?\n\n\n\nExercise 2\n\nA random variable \\(Z\\) follows a standard normal distribution. Find \\(P(-0.67 \\leq Z \\leq -0.23)\\), \\(P(0 \\leq Z \\leq 1.96)\\), \\(P(-1.28 \\leq Z \\leq 0)\\) and \\(P(Z &gt; 4.2)\\).\nLet \\(Y\\) be normally distributed with \\(\\mu=2.5\\) and \\(\\sigma=2\\). Find \\(P(Y&gt;7.6)\\), \\(P(7.4 \\leq Y \\leq 10.6)\\), a \\(y\\) such that \\(P(Y&gt;y)=0.025\\), and a \\(y\\) such that \\(P(y \\leq Y \\leq 2.5)=0.4943\\).\nAssume that football game times are normally distributed with a mean of \\(3\\) hours and a standard deviation of \\(0.4\\) hour. What is the probability that the game lasts at most \\(2.5\\) hours? Find the maximum value for a game to be in the bottom \\(1\\)% of the distribution.\n\n\n\nExercise 3\n\nRandom variable \\(S\\) is exponentially distributed with mean of \\(0.1\\). What is the standard deviation of \\(S\\)? What is \\(P(0.10 \\leq S \\leq 0.2)\\)?\nA tollbooth operator has observed that cars arrive randomly at a rate of \\(360\\) cars per hour. What is the mean time between car arrivals? What is the probability that the next car will arrive within ten seconds?",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Probability III</span>"
    ]
  },
  {
    "objectID": "ProbabilityIII.html#answers",
    "href": "ProbabilityIII.html#answers",
    "title": "10  Probability III",
    "section": "10.3 Answers",
    "text": "10.3 Answers\n\nExercise 1\n\nThe height of the density function \\(f(x)=0.1667\\), the mean is \\(1\\), standard deviation is \\(1.73\\), and \\(P(X \\leq -1)=0.1667\\).\n\n\\(f(x)\\) can be easily estimated by using the formula of the continuous uniform random variable. \\(f(x)=\\frac{1}{b-a}\\). Using R as a calculator we find:\n\n1/(4-(-2))\n\n[1] 0.1666667\n\n\nThe mean is given by \\(\\mu = \\frac{a+b}{2}\\). In R we determine that the mean is:\n\n(-2+4)/2\n\n[1] 1\n\n\nThe standard deviation is \\(\\sigma = \\sqrt {\\frac{(b-a)^2}{12}}\\). Using R we find:\n\nsqrt((4-(-2))^2/12)\n\n[1] 1.732051\n\n\nFinally, we can find the probability of \\(Z\\) being less than \\(-1\\) by using the punif() function:\n\npunif(-1,-2,4)\n\n[1] 0.1666667\n\n\n\nThe probability that you will arrive on time is \\(0.875\\). If the time of the errand is 30 minutes, then the probability goes down to \\(0.75\\).\n\nThere is a \\(120\\) minute interval in which the IP can arrive. The density function is given by \\(f(x)=1/120\\). Using R we can find \\(P(X&gt;15)\\):\n\npunif(15,0,120,lower.tail=F)\n\n[1] 0.875\n\n\nOnce more we can find \\(P(X&gt;30)\\):\n\npunif(30,0,120,lower.tail=F)\n\n[1] 0.75\n\n\n\n\nExercise 2\n\n\\(P(-0.67 \\leq Z \\leq -0.23)=0.158\\), \\(P(0 \\leq Z \\leq 1.96)=0.475\\), \\(P(-1.28 \\leq Z \\leq 0)=0.4\\) and \\(P(Z &gt; 4.2) \\approx 0\\).\n\nUse the pnorm() function to find the probabilities. \\(P(-0.67 \\leq Z \\leq -0.23)\\):\n\npnorm(-0.23)-pnorm(-0.67)\n\n[1] 0.157617\n\n\n\\(P(0 \\leq Z \\leq 1.96)\\)\n\npnorm(1.96)-pnorm(0)\n\n[1] 0.4750021\n\n\n\\(P(-1.28 \\leq Z \\leq 0)\\)\n\npnorm(0)-pnorm(-1.28)\n\n[1] 0.3997274\n\n\n\\(P(Z &gt; 4.2)\\)\n\noptions(scipen=999)\npnorm(4.2,lower.tail = F)\n\n[1] 0.00001334575\n\n\n\n\\(P(Y&gt;7.6)=0.005386\\), \\(P(7.4 \\leq Y \\leq 10.6)=0.0071\\), a \\(y\\) such that \\(P(Y&gt;y)=0.025\\) is \\(6.42\\), and a \\(y\\) such that \\(P(y \\leq Y \\leq 2.5)\\) is \\(-2.56\\).\n\nLet’s use once more the pnorm() function in R.\n\\(P(Y&gt;7.6)\\)\n\npnorm(7.6,2.5,2,lower.tail = F)\n\n[1] 0.005386146\n\n\n\\(P(7.4 \\leq Y \\leq 10.6)\\)\n\npnorm(10.6,2.5,2)-pnorm(7.4,2.5,2)\n\n[1] 0.007117202\n\n\n\\(y\\) such that \\(P(Y&gt;y)=0.025\\)\n\nqnorm(0.025,2.5,2,lower.tail = F)\n\n[1] 6.419928\n\n\n\\(y\\) such that \\(P(y \\leq Y \\leq 2.5)=0.4943\\). Note that \\(2.5\\) is the mean. Hence we are looking for a \\(y\\) that has \\(0.5-0.4943=0.0057\\) on the left:\n\nqnorm(0.0057,2.5,2)\n\n[1] -2.560385\n\n\n\nThe probability is \\(10.56\\)%. A game lasting no more than \\(2.069\\) hours would be in the bottom \\(1\\)%.\n\nLet’s use pnorm() once more in R.\n\npnorm(2.5,3,0.4)\n\n[1] 0.1056498\n\n\nFor the threshold we can use qnorm()\n\nqnorm(0.01,3,0.4)\n\n[1] 2.069461\n\n\n\n\nExercise 3\n\nThe standard deviation is equal to the mean \\(0.1\\). \\(P(0.10 \\leq S \\leq 0.2)=0.2325\\)\n\nLet’s use pexp() in R:\n\npexp(0.2,rate = 10)-pexp(0.1,rate = 10)\n\n[1] 0.2325442\n\n\n\nThe mean time between car arrivals is \\(1/360=0.002778\\). The probability that the next car will arrive within the next 10 seconds is \\(0.6321\\).\n\nOnce more we use pexp() in R\n\npexp(1/360,360)\n\n[1] 0.6321206",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Probability III</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Grolemund, Garret. 2014. “Hands-on Programming with r.” https://jjallaire.github.io/hopr/.\n\n\nWickham, Hadley. 2017. “R for Data Science.” https://r4ds.hadley.nz.",
    "crumbs": [
      "References"
    ]
  }
]