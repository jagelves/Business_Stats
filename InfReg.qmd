```{=html}
<style>
details {
  background-color: #F8F9FA;
  padding: 5px;
  border: 1px solid #ddd; /* Light border */
  border-radius: 5px; /* Rounded corners */
  margin: 10px 0; /* Spacing */
}

details summary {
  cursor: pointer; /* Pointer cursor for interactivity */
}
</style>
```

# Regression and Inference
In this chapter, you will begin mastering the concepts of correlation significance, difference of means tests, and regression inference, empowering you with the tools to uncover meaningful relationships and differences in data while assessing their statistical significance. These skills will enable you to make informed business decisions, such as evaluating the strength of market trends, comparing performance across groups, or determining the impact of variables in predictive models, ensuring your conclusions are robust and actionable.


## Correlation Significance 

To determine the statistical significance of the correlation coefficient we test:

-   $H_o: \rho \geq 0$; $H_a: \rho <0$ left tail

-   $H_o: \rho \leq 0$; $H_a: \rho >0$ right tail

-   $H_o: \rho = 0$; $H_a: \rho \neq 0$ two tails

The test statistic for the correlation is given by $t_{df}= \frac{r_{xy}\sqrt{n-2}}{\sqrt{1-r_{xy}^2}}$, where $df=n-2$ and $r_{xy}$ is the sample correlation coefficient.

Run the `cor.test()` function to perform the test on two vectors. Here is a list of arguments to use:

-   *alternative*: is a choice between "two.sided", "less" and "greater".

-   *conf.level*: sets the confidence level. Enter as a decimal and not percentage.

## Difference of Means Tests

Tests for inference about the difference of two population means.

-   The test for unpaired mean differences (not equal variances) is given by $t_{df}= \frac {(\bar x_1 - \bar x_2)- \bar d_o}{\sqrt {\frac {s_1^2}{n_1} \frac{s_2^2}{n_2}}}$.

-   The test for unpaired mean difference (equal variances) is given by $t_{df}= \frac {(\bar x_1 - \bar x_2)- \bar d_o}{\sqrt {s_p^2 (\frac {1}{n_1} + \frac {1}{n_2})}}$.

-   The test for paired mean difference is given by $t_{df}= \frac {\bar d- d_o}{\frac {s}{\sqrt{n}}}$.

Run these test in R by using the `t.test()` function. Here is a list of arguments to use:

-   *paired*: use True for paired, False for independent. The default is False.

-   *var.equal*: use True for equal variances, False for unequal. The default is False.

-   *mu*: a value that indicate the hypothesized value of the mean or mean difference.

-   *alternative*: is a choice between "two.sided", "less" and "greater".

-   *conf.level*: sets the confidence level. Enter as a decimal and not percentage.

## Regression Inference

When running regression a couple of test can be performed on the coefficients to determine significance:

-   The first test competing hypothesis are $H_o: \beta_j = 0$; $H_a: \beta_j \ne 0$. The test statistic for the intercept (slope) coefficient is given by $t_{df}= \frac {b_j}{se(b_j)}$.

-   The second test competing hypothesis are $H_o: \beta_1=\beta_2=...\beta_k=0$; $H_a:$ *at least one* $\beta_i \neq 0$. The joint test of significance is given by $F_{df_1,df_2} = \frac {SSR/k}{SSE/(n-k-1)} = \frac {MSR}{MSE}$. The Anova table below shows more detail on this test.

| Anova | df | SS | MS | F | Significance |
|:----------:|:----------:|:----------:|:----------:|:-------------:|:----------:|
| Regression | $k$ | $SSR$ | $MSR=\frac{SSR}{k}$ | $F_{df_1,df_2} = \frac {MSR}{MSE}$ | $P(F) \geq \frac{MSR}{MSE}$ |
| Residual | $n-k-1$ | $SSE$ | $MSE=\frac {SSE}{n-k-1}$ |  |  |
| Total | $n-1$ | $SST$ |  |  |  |

To conduct these tests, save the `lm()` model into an object. The `summary()` function can then be used to retrieve the results of the tests on the model's parameters. Use the `anova()` function to obtain the Anova table.

## Exercises

The following exercises will help you test your knowledge on Regression and Inference. In particular, the exercises work on:

-   Determining the significance of correlations.
-   Conduct paired and unpaired test of means and proportions.
-   Determining the significance of the slope and intercept estimates both individually and jointly.
-   Developing prediction intervals.

Try not to peek at the answers until you have formulated your own answer and double-checked your work for any mistakes.

### Exercise 1 {.unnumbered}

1.  Consider the following competing hypothesis: $H_{o}: \rho=0$, $H_{a}: \rho \neq 0$. A sample of $25$ observations reveals that the correlation coefficient between two variables is $0.15$. At a $5$% confidence level, can we reject the null hypothesis?

    <details>
    <summary>Answer</summary>
    *At the 5% significance level, we cannot reject the null since the p-value is 0.47 > 0.05.*

    *Recall that the t-stat is calculated by \(\frac{r_{xy}\sqrt{n-2}}{\sqrt{1-r_{xy}^2}}\). We can use R as a calculator to calculate this value:*

    ```{r}
    rxy <- 0.15
    n <- 25
    (tstat <- (rxy * sqrt(n - 2)) / (sqrt(1 - rxy^2)))
    ```

    *Now, we can estimate the p-value using the `pt()` function:*

    ```{r}
    2 * pt(tstat, n - 2, lower.tail = FALSE)
    ```
    </details>

2.  Install the `ISLR2` package in R. Use the **Hitters** data set to look at the relationship between *Hits* and *Salary*. Specifically, calculate the correlation coefficient and test the competing hypothesis $H_{o}: \rho=0$, $H_{a}: \rho \neq 0$ at the $1$% significance level.

    <details>
    <summary>Answer</summary>
    *The estimated correlation of 0.44 and the t-value is 7.89. Since the p-value is approximately 0, we reject the null hypothesis \(H_{o}: \rho=0\).* 

    *Once the `ISLR2` package is downloaded, it can be loaded to R using the `library()` function. The `cor.test()` function conducts the appropriate test of significance.*

    ```{r}
    library(ISLR2)
    cor.test(Hitters$Salary, Hitters$Hits, conf.level = 0.95)
    ```
    </details>

### Exercise 2 {.unnumbered}

1.  Install the `ISLR2` package in R. Use the **Hitters** data set to investigate if the average hits were significantly different between the two divisions (American and National). Use the *NewLeague* and *Hits* variables to test the hypothesis at the $5$% significance level. Is there reason to believe that the population variances are different?

    <details>
    <summary>Answer</summary>
    *There is no reason to believe that the population variances are different. Players are recruited from what seems to be a common pool. At a 5% significance level, the difference of the two means is not significantly different from zero. We can't reject the null hypothesis.*

    *We will use the `t.test()` function in R to test the hypothesis. We note that the test is not paired, two-sided, and assumes equal variances in the population.*

    ```{r}
    t.test(Hitters$Hits[Hitters$NewLeague == "A"],
           Hitters$Hits[Hitters$NewLeague == "N"], paired = FALSE, 
           alternative = "two.sided", mu = 0, var.equal = TRUE,
           conf.level = 0.95)
    ```
    </details>

2.  Use the `ISLR2` package for this question. Particularly, use the **BrainCancer** data set to test whether males have a higher average survival time than women. Use the *sex* and *time* variables to test the hypothesis at the $5$% significance level. Is there reason to believe that the population variances are different?

    <details>
    <summary>Answer</summary>
    *There might be reason to believe that the population variances are different. Women and men are known to have medical differences. At a 5% significance level, the average survival time of men seems not to be larger than that of women. We can't reject the null hypothesis \(H_{o}: \bar{x}_{1} - \bar{x}_{2} \leq 0\).* 

    *Once more use the `t.test()` function in R to test the hypothesis. Note that the test is not paired, right-tailed, and assumes different variances in the population.*

    ```{r}
    t.test(BrainCancer$time[BrainCancer$sex == "Male"],
           BrainCancer$time[BrainCancer$sex == "Female"], paired = FALSE, 
           alternative = "greater", mu = 0, var.equal = FALSE,
           conf.level = 0.95)
    ```
    </details>

### Exercise 3 {.unnumbered}

1.  Use the **sleep** data set included in R. At the $1$% significance level, is there an effect of the drug on the $10$ patients? Assume that the *group* variable denotes before ($1$) the drug is administered and after ($2$) the drug is administered.

    <details>
    <summary>Answer</summary>
    *The drug seems to have an effect as we can reject the null hypothesis \(H_{o}: \bar{d} = 0\). The difference of means seems to be statistically different from zero.*

    *Use the `t.test()` function once more in R. Make sure to note that the test is paired and two-tailed.*

    ```{r}
    t.test(sleep$extra[sleep$group == 1],
           sleep$extra[sleep$group == 2], paired = TRUE,
           alternative = "two.sided", mu = 0, conf.level = 0.99)
    ```
    </details>

### Exercise 4 {.unnumbered}

1.  Install the `ISLR2` package in R. Use the **Hitters** data set to investigate the effect of *HmRun*, *RBI*, and *Years* on a player's *Salary*. Which variables are statistically different from zero? Are the variables jointly significant? Does the $R^2$ suggest a good fit of the data to the model?

    <details>
    <summary>Answer</summary>
    *Both RBI and Years are statistically significant, and the salary of a player increases as they gain more experience and have more RBIs. Home runs do not seem to have an impact on the salary of a player according to the data. The F-statistic reveals that the coefficients are jointly significant since the p-value is approximately zero. Both the Multiple and Adjusted \(R^2\) suggest that the model only accounts for 32% of the variation in Salary. We might have to include more variables in our model to better explain the salary of a player.*

    *We can run a linear regression in R by using the `lm()` function. We'll use the `summary()` function to get more details on the model's performance.*

    ```{r}
    fit <- lm(Salary ~ HmRun + RBI + Years, data = Hitters)
    summary(fit)
    ```
    </details>

2.  Jos√© Altuve had $28$ home runs, $57$ RBI's, and has been in the league for $12$ years. What is the model's predicted salary for him? What is the $95$% prediction interval? Note: The model predicts his salary if he played in $1987$.

    <details>
    <summary>Answer</summary>
    *The predicted salary is 619.93, and the 95% prediction interval is [-129.89, 1369.7].*

    ```{r}
    new <- data.frame(HmRun = 28, RBI = 57, Years = 12)
    predict(fit, newdata = new, level = 0.95, interval = "prediction")
    ```
    </details>